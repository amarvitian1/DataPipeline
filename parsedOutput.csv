link,title,description
http://arxiv.org/abs/1903.11593,What does AI see? Deep segmentation networks discover biomarkers for lung cancer survival. (arXiv:1903.11593v1 [eess.IV]),"<p>Non-small-cell lung cancer (NSCLC) represents approximately 80-85% of lung
cancer diagnoses and is the leading cause of cancer-related death worldwide.
Recent studies indicate that image-based radiomics features from positron
emission tomography-computed tomography (PET/CT) images have predictive power
on NSCLC outcomes. To this end, easily calculated functional features such as
the maximum and the mean of standard uptake value (SUV) and total lesion
glycolysis (TLG) are most commonly used for NSCLC prognostication, but their
prognostic value remains controversial. Meanwhile, convolutional neural
networks (CNN) are rapidly emerging as a new premise for cancer image analysis,
with significantly enhanced predictive power compared to other hand-crafted
radiomics features. Here we show that CNN trained to perform the tumor
segmentation task, with no other information than physician contours, identify
a rich set of survival-related image features with remarkable prognostic value.
In a retrospective study on 96 NSCLC patients before stereotactic-body
radiotherapy (SBRT), we found that the CNN segmentation algorithm (U-Net)
trained for tumor segmentation in PET/CT images, contained features having
strong correlation with 2- and 5-year overall and disease-specific survivals.
The U-net algorithm has not seen any other clinical information (e.g. survival,
age, smoking history) than the images and the corresponding tumor contours
provided by physicians. Furthermore, through visualization of the U-Net, we
also found convincing evidence that the regions of progression appear to match
with the regions where the U-Net features identified patterns that predicted
higher likelihood of death. We anticipate our findings will be a starting point
for more sophisticated non-intrusive patient specific cancer prognosis
determination.
</p>
"
http://arxiv.org/abs/1903.11596,Detecting service provider alliances. (arXiv:1903.11596v1 [cs.GT]),"<p>We present an algorithm for detecting service provider alliances. To perform
this, we modelize a cooperative game-theoretic model for competitor service
providers. A choreography (a peer-to-peer service composition model) needs a
set of services to fulfill its requirements. Users must choose, for each
requirement, which service providers will be used to enact the choreography at
lowest cost. Due to the lack of centralization, vendors can form alliances to
control the market. We propose a novel algorithm capable of detecting alliances
among service providers, based on our findings showing that this game has an
empty core, but a non-empty bargaining set.
</p>
"
http://arxiv.org/abs/1903.11598,A Simple Haploid-Diploid Evolutionary Algorithm. (arXiv:1903.11598v1 [cs.NE]),"<p>It has recently been suggested that evolution exploits a form of fitness
landscape smoothing within eukaryotic sex due to the haploid-diploid cycle.
This short paper presents a simple modification to the standard evolutionary
computing algorithm to similarly benefit from the process. Using the well-known
NK model of fitness landscapes it is shown that the benefit emerges as
ruggedness is increased.
</p>
"
http://arxiv.org/abs/1903.11621,Self-adaptive decision-making mechanisms to balance the execution of multiple tasks for a multi-robots team. (arXiv:1903.11621v1 [cs.RO]),"<p>This work addresses the coordination problem of multiple robots with the goal
of finding specific hazardous targets in an unknown area and dealing with them
cooperatively. The desired behaviour for the robotic system entails multiple
requirements, which may also be conflicting. The paper presents the problem as
a constrained bi-objective optimization problem in which mobile robots must
perform two specific tasks of exploration and at same time cooperation and
coordination for disarming the hazardous targets. These objectives are opposed
goals, in which one may be favored, but only at the expense of the other.
Therefore, a good trade-off must be found. For this purpose, a nature-inspired
approach and an analytical mathematical model to solve this problem considering
a single equivalent weighted objective function are presented. The results of
proposed coordination model, simulated in a two dimensional terrain, are showed
in order to assess the behaviour of the proposed solution to tackle this
problem. We have analyzed the performance of the approach and the influence of
the weights of the objective function under different conditions: static and
dynamic. In this latter situation, the robots may fail under the stringent
limited budget of energy or for hazardous events. The paper concludes with a
critical discussion of the experimental results.
</p>
"
http://arxiv.org/abs/1903.11626,Bridging Adversarial Robustness and Gradient Interpretability. (arXiv:1903.11626v1 [cs.LG]),"<p>Adversarial training is a training scheme designed to counter adversarial
attacks by augmenting the training dataset with adversarial examples.
Surprisingly, several studies have observed that loss gradients from
adversarially trained DNNs are visually more interpretable than those from
standard DNNs. Although this phenomenon is interesting, there are only few
works that have offered an explanation. In this paper, we attempted to bridge
this gap between adversarial robustness and gradient interpretability. To this
end, we identified that loss gradients from adversarially trained DNNs align
better with human perception because adversarial training restricts gradients
closer to the image manifold. We then demonstrated that adversarial training
causes loss gradients to be quantitatively meaningful. Finally, we showed that
under the adversarial training framework, there exists an empirical trade-off
between test accuracy and loss gradient interpretability and proposed two
potential approaches to resolving this trade-off.
</p>
"
http://arxiv.org/abs/1903.11633,Laplace Landmark Localization. (arXiv:1903.11633v1 [cs.CV]),"<p>Landmark localization in images and videos is a classic problem solved in
various ways. Nowadays, with deep networks prevailing throughout machine
learning, there are revamped interests in pushing facial landmark detection
technologies to handle more challenging data. Most efforts use network
objectives based on L1 or L2 norms, which have several disadvantages. First of
all, the locations of landmarks are determined from generated heatmaps (i.e.,
confidence maps) from which predicted landmark locations (i.e., the means) get
penalized without accounting for the spread: a high scatter corresponds to low
confidence and vice-versa. For this, we introduce a LaplaceKL objective that
penalizes for a low confidence. Another issue is a dependency on labeled data,
which are expensive to obtain and susceptible to error. To address both issues
we propose an adversarial training framework that leverages unlabeled data to
improve model performance. Our method claims state-of-the-art on all of the
300W benchmarks and ranks second-to-best on the Annotated Facial Landmarks in
the Wild (AFLW) dataset. Furthermore, our model is robust with a reduced size:
1/8 the number of channels (i.e., 0.0398MB) is comparable to state-of-that-art
in real-time on CPU. Thus, we show that our method is of high practical value
to real-life application.
</p>
"
http://arxiv.org/abs/1903.11635,Fourier Entropy-Influence Conjecture for Random Linear Threshold Functions. (arXiv:1903.11635v1 [cs.CC]),"<p>The Fourier-Entropy Influence (FEI) Conjecture states that for any Boolean
function $f:\{+1,-1\}^n \to \{+1,-1\}$, the Fourier entropy of $f$ is at most
its influence up to a universal constant factor. While the FEI conjecture has
been proved for many classes of Boolean functions, it is still not known
whether it holds for the class of Linear Threshold Functions. A natural
question is: Does the FEI conjecture hold for a `random' linear threshold
function? In this paper, we answer this question in the affirmative. We
consider two natural distributions on the weights defining a linear threshold
function, namely uniform distribution on $[-1,1]$ and Normal distribution.
</p>
"
http://arxiv.org/abs/1903.11640,Fundamental Limits of Covert Packet Insertion. (arXiv:1903.11640v1 [cs.CR]),"<p>Covert communication conceals the existence of the transmission from a
watchful adversary. We consider the fundamental limits for covert
communications via packet insertion over packet channels whose packet timings
are governed by a renewal process of rate $\lambda$. Authorized transmitter
Jack sends packets to authorized receiver Steve, and covert transmitter Alice
wishes to transmit packets to covert receiver Bob without being detected by
watchful adversary Willie. Willie cannot authenticate the source of the
packets. Hence, he looks for statistical anomalies in the packet stream from
Jack to Steve to attempt detection of unauthorized packet insertion. First, we
consider a special case where the packet timings are governed by a Poisson
process and we show that Alice can covertly insert $\mathcal{O}(\sqrt{\lambda
T})$ packets for Bob in a time interval of length $T$; conversely, if Alice
inserts $\omega(\sqrt{\lambda T})$, she will be detected by Willie with high
probability. Then, we extend our results to general renewal channels and show
that in a stream of $N$ packets transmitted by Jack, Alice can covertly insert
$\mathcal{O}(\sqrt{N})$ packets; if she inserts $\omega(\sqrt{N})$ packets, she
will be detected by Willie with high probability.
</p>
"
http://arxiv.org/abs/1903.11649,Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment. (arXiv:1903.11649v1 [cs.CV]),"<p>We address the problem of grounding free-form textual phrases by using weak
supervision from image-caption pairs. We propose a novel end-to-end model that
uses caption-to-image retrieval as a `downstream' task to guide the process of
phrase localization. Our method, as a first step, infers the latent
correspondences between regions-of-interest (RoIs) and phrases in the caption
and creates a discriminative image representation using these matched RoIs. In
a subsequent step, this (learned) representation is aligned with the caption.
Our key contribution lies in building this `caption-conditioned' image encoding
which tightly couples both the tasks and allows the weak supervision to
effectively guide visual grounding. We provide an extensive empirical and
qualitative analysis to investigate the different components of our proposed
model and compare it with competitive baselines. For phrase localization, we
report an improvement of 4.9% (absolute) over the prior state-of-the-art on the
VisualGenome dataset. We also report results that are at par with the
state-of-the-art on the downstream caption-to-image retrieval task on COCO and
Flickr30k datasets.
</p>
"
http://arxiv.org/abs/1903.11650,Lens-based Millimeter Wave Reconfigurable Antenna NOMA. (arXiv:1903.11650v1 [cs.IT]),"<p>This paper proposes a new multiple access technique based on the millimeter
wave lens-based reconfigurable antenna systems. In particular, to support a
large number of groups of users with different angles of departures (AoDs), we
integrate recently proposed reconfigurable antenna multiple access (RAMA) into
non-orthogonal multiple access (NOMA). The proposed technique, named
reconfigurable antenna NOMA (RA-NOMA), divides the users with respect to their
AoDs and channel gains. Users with different AoDs and comparable channel gains
are served via RAMA while users with the same AoDs but different channel gains
are served via NOMA. This technique results in the independence of the number
of radio frequency chains from the number of NOMA groups. Further, we derive
the feasibility conditions and show that the power allocation for RA-NOMA is a
convex problem. We then derive the maximum achievable sum-rate of RA-NOMA.
Simulation results show that RA-NOMA outperforms conventional orthogonal
multiple access (OMA) as well as the combination of RAMA with the OMA
techniques.
</p>
"
http://arxiv.org/abs/1903.11665,Microservice Transition and its Granularity Problem: A Systematic Mapping Study. (arXiv:1903.11665v1 [cs.SE]),"<p>Microservices have gained wide recognition and acceptance in software
industries as an emerging architectural style for autonomic, scalable, and more
reliable computing. The transition to microservices has been highly motivated
by the need for better alignment of technical design decisions with improving
value potentials of architectures. Despite microservices' popularity, research
still lacks disciplined understanding of transition and consensus on the
principles and activities underlying ""micro-ing"" architectures. In this paper,
we report on a systematic mapping study that consolidates various views,
approaches and activities that commonly assist in the transition to
microservices. The study aims to provide a better understanding of the
transition; it also contributes a working definition of the transition and
technical activities underlying it. We term the transition and technical
activities leading to microservice architectures as microservitization. We then
shed light on a fundamental problem of microservitization: microservice
granularity and reasoning about its adaptation as first-class entities. This
study reviews state-of-the-art and -practice related to reasoning about
microservice granularity; it reviews modelling approaches, aspects considered,
guidelines and processes used to reason about microservice granularity. This
study identifies opportunities for future research and development related to
reasoning about microservice granularity.
</p>
"
http://arxiv.org/abs/1903.11670,The minimum value of the Colless index. (arXiv:1903.11670v1 [q-bio.PE]),"<p>The Colless index is one of the oldest and most widely used balance indices
for rooted bifurcating trees. Despite its popularity, its minimum value on the
space $\mathcal{T}_n$ of rooted bifurcating trees with $n$ leaves is only known
when $n$ is a power of 2. In this paper we fill this gap in the literature, by
providing a formula that computes, for each $n$, the minimum Colless index on
$\mathcal{T}_n$, and characterizing those trees where this minimum value is
reached.
</p>
"
http://arxiv.org/abs/1903.11672,MuSE-ing on the Impact of Utterance Ordering On Crowdsourced Emotion Annotations. (arXiv:1903.11672v1 [cs.SD]),"<p>Emotion recognition algorithms rely on data annotated with high quality
labels. However, emotion expression and perception are inherently subjective.
There is generally not a single annotation that can be unambiguously declared
""correct"". As a result, annotations are colored by the manner in which they
were collected. In this paper, we conduct crowdsourcing experiments to
investigate this impact on both the annotations themselves and on the
performance of these algorithms. We focus on one critical question: the effect
of context. We present a new emotion dataset, Multimodal Stressed Emotion
(MuSE), and annotate the dataset using two conditions: randomized, in which
annotators are presented with clips in random order, and contextualized, in
which annotators are presented with clips in order. We find that contextual
labeling schemes result in annotations that are more similar to a speaker's own
self-reported labels and that labels generated from randomized schemes are most
easily predictable by automated systems.
</p>
"
http://arxiv.org/abs/1903.11673,Adversarial Deep Learning in EEG Biometrics. (arXiv:1903.11673v1 [cs.LG]),"<p>Deep learning methods for person identification based on
electroencephalographic (EEG) brain activity encounters the problem of
exploiting the temporally correlated structures or recording session specific
variability within EEG. Furthermore, recent methods have mostly trained and
evaluated based on single session EEG data. We address this problem from an
invariant representation learning perspective. We propose an adversarial
inference approach to extend such deep learning models to learn
session-invariant person-discriminative representations that can provide
robustness in terms of longitudinal usability. Using adversarial learning
within a deep convolutional network, we empirically assess and show
improvements with our approach based on longitudinally collected EEG data for
person identification from half-second EEG epochs.
</p>
"
http://arxiv.org/abs/1903.11674,On Inversely Proportional Hypermutations with Mutation Potential. (arXiv:1903.11674v1 [cs.NE]),"<p>Artificial Immune Systems (AIS) employing hypermutations with linear static
mutation potential have recently been shown to be very effective at escaping
local optima of combinatorial optimisation problems at the expense of being
slower during the exploitation phase compared to standard evolutionary
algorithms. In this paper we prove that considerable speed-ups in the
exploitation phase may be achieved with dynamic inversely proportional mutation
potentials (IPM) and argue that the potential should decrease inversely to the
distance to the optimum rather than to the difference in fitness. Afterwards we
define a simple (1+1)~Opt-IA, that uses IPM hypermutations and ageing, for
realistic applications where optimal solutions are unknown. The aim of the AIS
is to approximate the ideal behaviour of the inversely proportional
hypermutations better and better as the search space is explored. We prove that
such desired behaviour, and related speed-ups, occur for a well-studied bimodal
benchmark function called \textsc{TwoMax}. Furthermore, we prove that the
(1+1)~Opt-IA with IPM efficiently optimises a third bimodal function,
\textsc{Cliff}, by escaping its local optima while Opt-IA with static potential
cannot, thus requires exponential expected runtime in the distance between the
cliff and the optimum.
</p>
"
http://arxiv.org/abs/1903.11677,Exact Byzantine Consensus on Undirected Graphs under Local Broadcast Model. (arXiv:1903.11677v1 [cs.DC]),"<p>This paper considers the Byzantine consensus problem for nodes with binary
inputs. The nodes are interconnected by a network represented as an undirected
graph, and the system is assumed to be synchronous. Under the classical
point-to-point communication model, it is well-known [7] that the following two
conditions are both necessary and sufficient to achieve Byzantine consensus
among $n$ nodes in the presence of up to $f$ Byzantine faulty nodes: $n \ge
3f+1$ and vertex connectivity at least $2f+1$. In the classical point-to-point
communication model, it is possible for a faulty node to equivocate, i.e.,
transmit conflicting information to different neighbors. Such equivocation is
possible because messages sent by a node to one of its neighbors are not
overheard by other neighbors.
</p>
<p>This paper considers the local broadcast model. In contrast to the
point-to-point communication model, in the local broadcast model, messages sent
by a node are received identically by all of its neighbors. Thus, under the
local broadcast model, attempts by a node to send conflicting information can
be detected by its neighbors. Under this model, we show that the following two
conditions are both necessary and sufficient for Byzantine consensus: vertex
connectivity at least $\lfloor 3f/2 \rfloor + 1$ and minimum node degree at
least $2f$. Observe that the local broadcast model results in a lower
requirement for connectivity and the number of nodes $n$, as compared to the
point-to-point communication model.
</p>
<p>We extend the above results to a hybrid model that allows some of the
Byzantine faulty nodes to equivocate. The hybrid model bridges the gap between
the point-to-point and local broadcast models, and helps to precisely
characterize the trade-off between equivocation and network requirements.
</p>
"
http://arxiv.org/abs/1903.11678,Tree Search vs Optimization Approaches for Map Generation. (arXiv:1903.11678v1 [cs.AI]),"<p>Search-based procedural content generation uses stochastic global
optimization algorithms to search spaces of game content. However, it has been
found that tree search can be competitive with evolution on certain
optimization problems. We investigate the applicability of several tree search
methods to map generation and compare them systematically with several
optimization algorithms, including evolutionary algorithms. For purposes of
comparison, we use a simplified map generation problem where only passable and
impassable tiles exist, three different map representations, and a set of
objectives that are representative of those commonly found in actual level
generation problem. While the results suggest that evolutionary algorithms
produce good maps faster, several tree search methods can perform very well
given sufficient time, and there are interesting differences in the character
of the generated maps depending on the algorithm chosen, even for the same
representation and objective.
</p>
"
http://arxiv.org/abs/1903.11680,Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks. (arXiv:1903.11680v1 [cs.LG]),"<p>Modern neural networks are typically trained in an over-parameterized regime
where the parameters of the model far exceed the size of the training data. Due
to over-parameterization these neural networks in principle have the capacity
to (over)fit any set of labels including pure noise. Despite this high fitting
capacity, somewhat paradoxically, neural network models trained via first-order
methods continue to predict well on yet unseen test data. In this paper we take
a step towards demystifying this phenomena. In particular we show that first
order methods such as gradient descent are provably robust to noise/corruption
on a constant fraction of the labels despite over-parametrization under a rich
dataset model. In particular: i) First, we show that in the first few
iterations where the updates are still in the vicinity of the initialization
these algorithms only fit to the correct labels essentially ignoring the noisy
labels. ii) Secondly, we prove that to start to overfit to the noisy labels
these algorithms must stray rather far from from the initial model which can
only occur after many more iterations. Together, these show that gradient
descent with early stopping is provably robust to label noise and shed light on
empirical robustness of deep networks as well as commonly adopted heuristics to
prevent overfitting.
</p>
"
http://arxiv.org/abs/1903.11682,From closed to open access: A case study of flipped journals. (arXiv:1903.11682v1 [cs.DL]),"<p>In recent years, increased stakeholder pressure to transition research to
Open Access has led to many journals ""flipping"" from a toll access to an open
access publishing model. Changing the publishing model can influence the
decision of authors to submit their papers to a journal, and increased article
accessibility may influence citation behaviour. The aim of this paper is to
show changes in the number of published articles and citations after the
flipping of a journal. We analysed a set of 171 journals in the Web of Science
(WoS) which flipped to open access. In addition to comparing the number of
articles, average relative citation (ARC) and normalized impact factor (IF) are
applied, respectively, as bibliometric indicators at the article and journal
level, to trace the transformation of flipped journals covered. Our results
show that flipping mostly has had positive effects on journal's IF. But it has
had no obvious citation advantage for the articles. We also see a decline in
the number of published articles after flipping. We can conclude that flipping
to open access can improve the performance of journals, despite decreasing the
tendency of authors to submit their articles and no better citation advantages
for articles.
</p>
"
http://arxiv.org/abs/1903.11683,"Outlier-Robust Spatial Perception: Hardness, General-Purpose Algorithms, and Guarantees. (arXiv:1903.11683v1 [stat.ML])","<p>Spatial perception is the backbone of many robotics applications, and spans a
broad range of research problems, including localization and mapping, point
cloud alignment, and relative pose estimation from camera images. Robust
spatial perception is jeopardized by the presence of incorrect data
association, and in general, outliers. Although techniques to handle outliers
do exist, they can fail in unpredictable manners (e.g., RANSAC, robust
estimators), or can have exponential runtime (e.g., branch-and-bound). In this
paper, we advance the state of the art in outlier rejection by making three
contributions. First, we show that even a simple linear instance of outlier
rejection is inapproximable: in the worst-case one cannot design a
quasi-polynomial time algorithm that computes an approximate solution
efficiently. Our second contribution is to provide the first per-instance
sub-optimality bounds to assess the approximation quality of a given outlier
rejection outcome. Our third contribution is to propose a simple
general-purpose algorithm, named adaptive trimming, to remove outliers. Our
algorithm leverages recently-proposed global solvers that are able to solve
outlier-free problems, and iteratively removes measurements with large errors.
We demonstrate the proposed algorithm on three spatial perception problems: 3D
registration, two-view geometry, and SLAM. The results show that our algorithm
outperforms several state-of-the-art methods across applications while being a
general-purpose method.
</p>
"
http://arxiv.org/abs/1903.11688,Rallying Adversarial Techniques against Deep Learning for Network Security. (arXiv:1903.11688v1 [cs.CR]),"<p>Recent advances in artificial intelligence and the increasing need for
powerful defensive measures in the domain of network security, have led to the
adoption of deep learning approaches for use in network intrusion detection
systems. These methods have achieved superior performance against conventional
network attacks, which enable the deployment of practical security systems to
unique and dynamic sectors. Adversarial machine learning, unfortunately, has
recently shown that deep learning models are inherently vulnerable to
adversarial modifications on their input data. Because of this susceptibility,
the deep learning models deployed to power a network defense could in fact be
the weakest entry point for compromising a network system. In this paper, we
show that by modifying on average as little as 1.38 of the input features, an
adversary can generate malicious inputs which effectively fool a deep learning
based NIDS. Therefore, when designing such systems, it is crucial to consider
the performance from not only the conventional network security perspective but
also the adversarial machine learning domain.
</p>
"
http://arxiv.org/abs/1903.11690,Optimization of Inf-Convolution Regularized Nonconvex Composite Problems. (arXiv:1903.11690v1 [math.OC]),"<p>In this work, we consider nonconvex composite problems that involve
inf-convolution with a Legendre function, which gives rise to an anisotropic
generalization of the proximal mapping and Moreau-envelope. In a convex setting
such problems can be solved via alternating minimization of a splitting
formulation, where the consensus constraint is penalized with a Legendre
function. In contrast, for nonconvex models it is in general unclear that this
approach yields stationary points to the infimal convolution problem. To this
end we analytically investigate local regularity properties of the
Moreau-envelope function under prox-regularity, which allows us to establish
the equivalence between stationary points of the splitting model and the
original inf-convolution model. We apply our theory to characterize stationary
points of the penalty objective, which is minimized by the elastic averaging
SGD (EASGD) method for distributed training. Numerically, we demonstrate the
practical relevance of the proposed approach on the important task of
distributed training of deep neural networks.
</p>
"
http://arxiv.org/abs/1903.11691,Echo State Networks with Self-Normalizing Activations on the Hyper-Sphere. (arXiv:1903.11691v1 [cs.NE]),"<p>Among the various architectures of Recurrent Neural Networks, Echo State
Networks (ESNs) emerged due to their simplified and inexpensive training
procedure. These networks are known to be sensitive to the setting of
hyper-parameters, which critically affect their behaviour. Results show that
their performance is usually maximized in a narrow region of hyper-parameter
space called edge of chaos. Finding such a region requires searching in
hyper-parameter space in a sensible way: hyper-parameter configurations
marginally outside such a region might yield networks exhibiting fully
developed chaos, hence producing unreliable computations. The performance gain
due to optimizing hyper-parameters can be studied by considering the
memory--nonlinearity trade-off, i.e., the fact that increasing the nonlinear
behavior of the network degrades its ability to remember past inputs, and
vice-versa. In this paper, we propose a model of ESNs that eliminates critical
dependence on hyper-parameters, resulting in networks that provably cannot
enter a chaotic regime and, at the same time, denotes nonlinear behaviour in
phase space characterised by a large memory of past inputs, comparable to the
one of linear networks. Our contribution is supported by experiments
corroborating our theoretical findings, showing that the proposed model
displays dynamics that are rich-enough to approximate many common nonlinear
systems used for benchmarking.
</p>
"
http://arxiv.org/abs/1903.11693,Highly cited references in PLOS ONE and their in-text usage over time. (arXiv:1903.11693v1 [cs.DL]),"<p>In this article, we describe highly cited publications in a PLOS ONE
full-text corpus. For these publications, we analyse the citation contexts
concerning their position in the text and their age at the time of citing. By
selecting the perspective of highly cited papers, we can distinguish them based
on the context during citation even if we do not have any other information
source or metrics. We describe the top cited references based on how, when and
in which context they are cited. The focus of this study is on a time
perspective to explain the nature of the reception of highly cited papers. We
have found that these references are distinguishable by the IMRaD sections of
their citation. And further, we can show that the section usage of highly cited
papers is time-dependent. The longer the citation interval, the higher the
probability that a reference is cited in a method section.
</p>
"
http://arxiv.org/abs/1903.11694,"Studying the Impact of Power Capping on MapReduce-based, Data-intensive Mini-applications on Intel KNL and KNM Architectures. (arXiv:1903.11694v1 [cs.DC])","<p>In this poster, we quantitatively measure the impacts of data movement on
performance in MapReduce-based applications when executed on HPC systems. We
leverage the PAPI 'powercap' component to identify ideal conditions for
execution of our applications in terms of (1) dataset characteristics (i.e.,
unique words); (2) HPC system (i.e., KNL and KNM); and (3) implementation of
the MapReduce programming model (i.e., with or without combiner optimizations).
Results confirm the high energy and runtime costs of data movement, and the
benefits of the combiner optimization on these costs.
</p>
"
http://arxiv.org/abs/1903.11696,Stable prediction with radiomics data. (arXiv:1903.11696v1 [stat.ML]),"<p>Motivation: Radiomics refers to the high-throughput mining of quantitative
features from radiographic images. It is a promising field in that it may
provide a non-invasive solution for screening and classification. Standard
machine learning classification and feature selection techniques, however, tend
to display inferior performance in terms of (the stability of) predictive
performance. This is due to the heavy multicollinearity present in radiomic
data. We set out to provide an easy-to-use approach that deals with this
problem.
</p>
<p>Results: We developed a four-step approach that projects the original
high-dimensional feature space onto a lower-dimensional latent-feature space,
while retaining most of the covariation in the data. It consists of (i)
penalized maximum likelihood estimation of a redundancy filtered correlation
matrix. The resulting matrix (ii) is the input for a maximum likelihood factor
analysis procedure. This two-stage maximum-likelihood approach can be used to
(iii) produce a compact set of stable features that (iv) can be directly used
in any (regression-based) classifier or predictor. It outperforms other
classification (and feature selection) techniques in both external and internal
validation settings regarding survival in squamous cell cancers.
</p>
"
http://arxiv.org/abs/1903.11700,A Conceptual Framework for Assessing Anonymization-Utility Trade-Offs Based on Principal Component Analysis. (arXiv:1903.11700v1 [cs.CR]),"<p>An anonymization technique for databases is proposed that employs Principal
Component Analysis. The technique aims at releasing the least possible amount
of information, while preserving the utility of the data released in response
to queries. The general scheme is described, and alternative metrics are
proposed to assess utility, based respectively on matrix norms; correlation
coefficients; divergence measures, and quality indices of database images. This
approach allows to properly measure the utility of output data and incorporate
that measure in the anonymization method.
</p>
"
http://arxiv.org/abs/1903.11701,"Zero-shot Image Recognition Using Relational Matching, Adaptation and Calibration. (arXiv:1903.11701v1 [cs.CV])","<p>Zero-shot learning (ZSL) for image classification focuses on recognizing
novel categories that have no labeled data available for training. The learning
is generally carried out with the help of mid-level semantic descriptors
associated with each class. This semantic-descriptor space is generally shared
by both seen and unseen categories. However, ZSL suffers from hubness, domain
discrepancy and biased-ness towards seen classes. To tackle these problems, we
propose a three-step approach to zero-shot learning. Firstly, a mapping is
learned from the semantic-descriptor space to the image-feature space. This
mapping learns to minimize both one-to-one and pairwise distances between
semantic embeddings and the image features of the corresponding classes.
Secondly, we propose test-time domain adaptation to adapt the semantic
embedding of the unseen classes to the test data. This is achieved by finding
correspondences between the semantic descriptors and the image features.
Thirdly, we propose scaled calibration on the classification scores of the seen
classes. This is necessary because the ZSL model is biased towards seen classes
as the unseen classes are not used in the training. Finally, to validate the
proposed three-step approach, we performed experiments on four benchmark
datasets where the proposed method outperformed previous results. We also
studied and analyzed the performance of each component of our proposed ZSL
framework.
</p>
"
http://arxiv.org/abs/1903.11702,Efficient Nonlinear Fourier Transform Algorithms of Order Four on Equispaced Grid. (arXiv:1903.11702v1 [cs.NA]),"<p>We explore two classes of exponential integrators in this letter to design
nonlinear Fourier transform (NFT) algorithms with a desired accuracy-complexity
trade-off and a convergence order of $4$ on an equispaced grid. The integrating
factor based method in the class of Runge-Kutta methods yield algorithms with
complexity $O(N\log^2N)$ (where $N$ is the number of samples of the signal)
which have superior accuracy-complexity trade-off than any of the fast methods
known currently. The integrators based on Magnus series expansion, namely,
standard and commutator-free Magnus methods yield algorithms of complexity
$O(N^2)$ that have superior error behavior even for moderately small step-sizes
and higher signal strengths.
</p>
"
http://arxiv.org/abs/1903.11703,Recurrent Neural Networks For Accurate RSSI Indoor Localization. (arXiv:1903.11703v1 [eess.SP]),"<p>This paper proposes recurrent neuron networks (RNNs) for a fingerprinting
indoor localization using WiFi. Instead of locating user's position one at a
time as in the cases of conventional algorithms, our RNN solution aims at
trajectory positioning and takes into account the relation among the received
signal strength indicator (RSSI) measurements in a trajectory. Furthermore, a
weighted average filter is proposed for both input RSSI data and sequential
output locations to enhance the accuracy among the temporal fluctuations of
RSSI. The results using different types of RNN including vanilla RNN, long
short-term memory (LSTM), gated recurrent unit (GRU) and bidirectional LSTM
(BiLSTM) are presented. On-site experiments demonstrate that the proposed
structure achieves an average localization error of $0.75$ m with $80\%$ of the
errors under $1$ m, which outperforms the conventional KNN algorithms and
probabilistic algorithms by approximately $30\%$ under the same test
environment.
</p>
"
http://arxiv.org/abs/1903.11712,A Multi Hidden Recurrent Neural Network with a Modified Grey Wolf Optimizer. (arXiv:1903.11712v1 [cs.NE]),"<p>Identifying university students' weaknesses results in better learning and
can function as an early warning system to enable students to improve. However,
the satisfaction level of existing systems is not promising. New and dynamic
hybrid systems are needed to imitate this mechanism. A hybrid system (a
modified Recurrent Neural Network with an adapted Grey Wolf Optimizer) is used
to forecast students' outcomes. This proposed system would improve instruction
by the faculty and enhance the students' learning experiences. The results show
that a modified recurrent neural network with an adapted Grey Wolf Optimizer
has the best accuracy when compared with other models.
</p>
"
http://arxiv.org/abs/1903.11714,High Performance Monte Carlo Simulation of Ising Model on TPU Clusters. (arXiv:1903.11714v1 [cs.DC]),"<p>Large scale deep neural networks profited from an emerging class of AI
accelerators. Although the accelerators are specialized for machine learning,
some of their designs are general enough for other computing intensive
applications. Cloud TPU, as one of them, offers tremendous computing resources
and is easily accessible through TensorFlow by expressing the computation in a
graph. In this paper, we leverage this powerful hardware combined with the
expressiveness of TensorFlow to simulate the Ising model on a $2$-dimensional
lattice. We modify the computationally intensive part of the checkerboard
algorithm into matrix operations to exploit Cloud TPU's highly efficient matrix
unit. In our experiments, we demonstrate that our implementation outperforms
the best published benchmarks to our knowledge by 60% in single core and 250%
in multiple cores with linear scaling. We also show the performance improvement
of using low precision arithmetic---bfloat16 instead of float32---without
sacrificing any accuracy.
</p>
"
http://arxiv.org/abs/1903.11719,Fairness in Algorithmic Decision Making: An Excursion Through the Lens of Causality. (arXiv:1903.11719v1 [cs.LG]),"<p>As virtually all aspects of our lives are increasingly impacted by
algorithmic decision making systems, it is incumbent upon us as a society to
ensure such systems do not become instruments of unfair discrimination on the
basis of gender, race, ethnicity, religion, etc. We consider the problem of
determining whether the decisions made by such systems are discriminatory,
through the lens of causal models. We introduce two definitions of group
fairness grounded in causality: fair on average causal effect (FACE), and fair
on average causal effect on the treated (FACT). We use the Rubin-Neyman
potential outcomes framework for the analysis of cause-effect relationships to
robustly estimate FACE and FACT. We demonstrate the effectiveness of our
proposed approach on synthetic data. Our analyses of two real-world data sets,
the Adult income data set from the UCI repository (with gender as the protected
attribute), and the NYC Stop and Frisk data set (with race as the protected
attribute), show that the evidence of discrimination obtained by FACE and FACT,
or lack thereof, is often in agreement with the findings from other studies. We
further show that FACT, being somewhat more nuanced compared to FACE, can yield
findings of discrimination that differ from those obtained using FACE.
</p>
"
http://arxiv.org/abs/1903.11720,Performance Analysis and Enhancements for In-Band Full-Duplex Wireless Local Area Networks. (arXiv:1903.11720v1 [cs.NI]),"<p>In-Band Full-Duplex (IBFD) is a technique that enables a wireless node to
simultaneously transmit a signal and receive another on the same assigned
frequency. Thus, IBFD wireless systems can provide up to twice the channel
capacity compared to conventional Half-Duplex (HD) systems. In order to study
the feasibility of IBFD networks, reliable models are needed to capture
anticipated benefits of IBFD above the physical layer (PHY). In this paper, an
accurate analytical model based on Discrete-Time Markov Chain (DTMC) analysis
for IEEE 802.11 Distributed Coordination Function (DCF) with IBFD capabilities
is proposed. The model captures all parameters necessary to calculate important
performance metrics which quantify enhancements introduced as a result of IBFD
solutions. Additionally, two frame aggregation schemes for Wireless Local Area
Networks (WLANs) with IBFD features are proposed to increase the efficiency of
data transmission. Matching analytical and simulation results with less than 1%
average errors confirm that the proposed frame aggregation schemes further
improve the overall throughput by up to 24% and reduce latency by up to 47% in
practical IBFD-WLANs. More importantly, the results assert that IBFD
transmission can only reduce latency to a suboptimal point in WLANs, but frame
aggregation is necessary to minimize it.
</p>
"
http://arxiv.org/abs/1903.11722,Resource Allocation Mechanism for Media Handling Services in Cloud Multimedia Conferencing. (arXiv:1903.11722v1 [cs.MM]),"<p>Multimedia conferencing is the conversational exchange of multimedia content
between multiple parties. It has a wide range of applications (e.g., Massively
Multiplayer Online Games (MMOGs) and distance learning). Media handling
services (e.g., video mixing, transcoding, and compressing) are critical to
multimedia conferencing. However, efficient resource usage and scalability
still remain important challenges. Unfortunately, the cloud-based approaches
proposed so far have several deficiencies in terms of efficiency in resource
usage and scaling, while meeting Quality of Service (QoS) requirements. This
paper proposes a solution which optimizes resource allocation and scales in
terms of the number of participants while guaranteeing QoS. Moreover, our
solution composes different media handling services to support the
participants' demands. We formulate the resource allocation problem
mathematically as an Integer Linear Programming (ILP) problem and design a
heuristic for it. We evaluate our proposed solution for different numbers of
participants and different participants' geographical distributions. Simulation
results show that our resource allocation mechanism can compose the media
handling services and allocate the required resources in an optimal manner
while honoring the QoS in terms of end-to-end delay.
</p>
"
http://arxiv.org/abs/1903.11723,The Semantic Web Rule Language Expressiveness Extensions-A Survey. (arXiv:1903.11723v1 [cs.AI]),"<p>The Semantic Web Rule Language (SWRL) is a direct extension of OWL 2 DL with
a subset of RuleML, and it is designed to be the rule language of the Semantic
Web. This paper explores the state-of-the-art of SWRL's expressiveness
extensions proposed over time. As a motivation, the effectiveness of the
SWRL/OWL combination in modeling domain facts is discussed while some of the
common expressive limitations of the combination are also highlighted. The
paper then classifies and presents the relevant language extensions of the SWRL
and their added expressive powers to the original SWRL definition. Furthermore,
it provides a comparative analysis of the syntax and semantics of the proposed
extensions. In conclusion, the decidability requirement and usability of each
expressiveness extension are evaluated towards an efficient inclusion into the
OWL ontologies.
</p>
"
http://arxiv.org/abs/1903.11725,Skill Acquisition via Automated Multi-Coordinate Cost Balancing. (arXiv:1903.11725v1 [cs.RO]),"<p>We propose a learning framework, named Multi-Coordinate Cost Balancing
(MCCB), to address the problem of acquiring point-to-point movement skills from
demonstrations. MCCB encodes demonstrations simultaneously in multiple
differential coordinates that specify local geometric properties. MCCB
generates reproductions by solving a convex optimization problem with a
multi-coordinate cost function and linear constraints on the reproductions,
such as initial, target, and via points. Further, since the relative importance
of each coordinate system in the cost function might be unknown for a given
skill, MCCB learns optimal weighting factors that balance the cost function. We
demonstrate the effectiveness of MCCB via detailed experiments conducted on one
handwriting dataset and three complex skill datasets.
</p>
"
http://arxiv.org/abs/1903.11726,"Radiological images and machine learning: trends, perspectives, and prospects. (arXiv:1903.11726v1 [eess.IV])","<p>The application of machine learning to radiological images is an increasingly
active research area that is expected to grow in the next five to ten years.
Recent advances in machine learning have the potential to recognize and
classify complex patterns from different radiological imaging modalities such
as x-rays, computed tomography, magnetic resonance imaging and positron
emission tomography imaging. In many applications, machine learning based
systems have shown comparable performance to human decision-making. The
applications of machine learning are the key ingredients of future clinical
decision making and monitoring systems. This review covers the fundamental
concepts behind various machine learning techniques and their applications in
several radiological imaging areas, such as medical image segmentation, brain
function studies and neurological disease diagnosis, as well as computer-aided
systems, image registration, and content-based image retrieval systems.
Synchronistically, we will briefly discuss current challenges and future
directions regarding the application of machine learning in radiological
imaging. By giving insight on how take advantage of machine learning powered
applications, we expect that clinicians can prevent and diagnose diseases more
accurately and efficiently.
</p>
"
http://arxiv.org/abs/1903.11728,Network Slimming by Slimmable Networks: Towards One-Shot Architecture Search for Channel Numbers. (arXiv:1903.11728v1 [cs.CV]),"<p>We study how to set channel numbers in a neural network to achieve better
accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or
model size). A simple and one-shot solution, named AutoSlim, is presented.
Instead of training many network samples and searching with reinforcement
learning, we train a single slimmable network to approximate the network
accuracy of different channel configurations. We then iteratively evaluate the
trained slimmable model and greedily slim the layer with minimal accuracy drop.
By this single pass, we can obtain the optimized channel configurations under
different resource constraints. We present experiments with MobileNet v1,
MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We
show significant improvements over their default channel configurations. We
also achieve better accuracy than recent channel pruning methods and neural
architecture search methods.
</p>
<p>Notably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at
305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2
(301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our
AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3%
better accuracy than MobileNet-v1 (569M FLOPs). Code and models will be
available at: https://github.com/JiahuiYu/slimmable_networks
</p>
"
http://arxiv.org/abs/1903.11729,Simulating Imperial Dynamics and Conflict in the Ancient World. (arXiv:1903.11729v1 [physics.soc-ph]),"<p>The development of models to capture large-scale dynamics in human history is
one of the core contributions of the cliodynamics field. Crucially and most
often, these models are assessed by their predictive capability on some
macro-scale and aggregated measure, compared to manually curated historical
data. We consider the model predicting large-scale complex societies from
Turchin et al. (2013), where the evaluation is done on the prediction of
""imperial density"": the relative frequency with which a geographical area
belonged to large-scale polities over a certain time window. We implement the
model and release both code and data for reproducibility. Furthermore, we
assess its behaviour against three historical data sets: the relative size of
simulated polities vs historical ones; the spatial correlation of simulated
imperial density with historical population density; the spatial correlation of
simulated conflict vs historical conflict. At the global level, we show good
agreement with the population density (R2&lt;0.75), and some agreement with
historical conflict in Europe (R2&lt;0.42, a lower result possibly due to
historical data bias). Despite being overall good at capturing these important
effects, the model currently fails to reproduce the shapes of individual
imperial polities. Nonetheless, we discuss a way forward by matching the
probabilistic imperial strength from simulations to inferred networked
communities from real settlement data.
</p>
"
http://arxiv.org/abs/1903.11741,InfoMask: Masked Variational Latent Representation to Localize Chest Disease. (arXiv:1903.11741v1 [cs.CV]),"<p>The scarcity of richly annotated medical images is limiting supervised deep
learning based solutions to medical image analysis tasks, such as localizing
discriminatory radiomic disease signatures. Therefore, it is desirable to
leverage unsupervised and weakly supervised models. Most recent weakly
supervised localization methods apply attention maps or region proposals in a
multiple instance learning formulation. While attention maps can be noisy,
leading to erroneously highlighted regions, it is not simple to decide on an
optimal window/bag size for multiple instance learning approaches. In this
paper, we propose a learned spatial masking mechanism to filter out irrelevant
background signals from attention maps. The proposed method minimizes mutual
information between a masked variational representation and the input while
maximizing the information between the masked representation and class labels.
This results in more accurate localization of discriminatory regions. We tested
the proposed model on the ChestX-ray8 dataset to localize pneumonia from chest
X-ray images without using any pixel-level or bounding-box annotations.
</p>
"
http://arxiv.org/abs/1903.11748,Medical Time Series Classification with Hierarchical Attention-based Temporal Convolutional Networks: A Case Study of Myotonic Dystrophy Diagnosis. (arXiv:1903.11748v1 [cs.LG]),"<p>Myotonia, which refers to delayed muscle relaxation after contraction, is the
main symptom of myotonic dystrophy patients. We propose a hierarchical
attention-based temporal convolutional network (HA-TCN) for myotonic dystrohpy
diagnosis from handgrip time series data, and introduce mechanisms that enable
model explainability. We compare the performance of the HA-TCN model against
that of benchmark TCN models, LSTM models with and without attention
mechanisms, and SVM approaches with handcrafted features. In terms of
classification accuracy and F1 score, we found all deep learning models have
similar levels of performance, and they all outperform SVM. Further, the HA-TCN
model outperforms its TCN counterpart with regards to computational efficiency
regardless of network depth, and in terms of performance particularly when the
number of hidden layers is small. Lastly, HA-TCN models can consistently
identify relevant time series segments in the relaxation phase of the handgrip
time series, and exhibit increased robustness to noise when compared to
attention-based LSTM models.
</p>
"
http://arxiv.org/abs/1903.11749,Distributed Algorithms for Fully Personalized PageRank on Large Graphs. (arXiv:1903.11749v1 [cs.SI]),"<p>Personalized PageRank (PPR) has enormous applications, such as link
prediction and recommendation systems for social networks, which often require
the fully PPR to be known. Besides, most of real-life graphs are edge-weighted,
e.g., the interaction between users on the Facebook network. However, it is
computationally difficult to compute the fully PPR, especially on large graphs,
not to mention that most existing approaches do not consider the weights of
edges. In particular, the existing approach cannot handle graphs with billion
edges on a moderate-size cluster. To address this problem, this paper presents
a novel study on the computation of fully edge-weighted PPR on large graphs
using the distributed computing framework. Specifically, we employ the Monte
Carlo approximation that performs a large number of random walks from each node
of the graph, and exploits the parallel pipeline framework to reduce the
overall running time of the fully PPR. Based on that, we develop several
optimization techniques which (i) alleviate the issue of large nodes that could
explode the memory space, (ii) pre-compute short walks for small nodes that
largely speedup the computation of random walks, and (iii) optimize the amount
of random walks to compute in each pipeline that significantly reduces the
overhead. With extensive experiments on a variety of real-life graph datasets,
we demonstrate that our solution is several orders of magnitude faster than the
state-of-the-arts, and meanwhile, largely outperforms the baseline algorithms
in terms of accuracy.
</p>
"
http://arxiv.org/abs/1903.11750,Navigation in the Presence of Obstacles for an Agile Autonomous Underwater Vehicle. (arXiv:1903.11750v1 [cs.RO]),"<p>Navigation underwater traditionally is done by keeping a safe distance from
obstacles, resulting in ""fly-overs"" of the area of interest. An Autonomous
Underwater Vehicle (AUV) moving through a cluttered space, such as a shipwreck,
or a decorated cave is an extremely challenging problem and has not been
addressed in the past. This paper proposed a novel navigation framework
utilizing an enhanced version of Trajopt for fast 3D path-optimization with
near-optimal guarantees for AUVs. A sampling based correction procedure ensures
that the planning is not limited by local minima, enabling navigation through
narrow spaces. The method is shown, both on simulation and in-pool experiments,
to be fast enough to enable real-time autonomous navigation for an Aqua2 AUV
with strong safety guarantees.
</p>
"
http://arxiv.org/abs/1903.11751,Regularized Stochastic Block Model for robust community detection in complex networks. (arXiv:1903.11751v1 [cs.SI]),"<p>The stochastic block model is able to generate different network partitions,
ranging from traditional assortative communities to disassortative structures.
Since the degree-corrected stochastic block model does not specify which mixing
pattern is desired, the inference algorithms, which discover the most likely
partition of the networks nodes, are likely to get trapped in the local optima
of the log-likelihood. Here we introduce a new model constraining nodes'
internal degrees ratios in the objective function to stabilize the inference of
block models from the observed network data. Given the regularized model, the
inference algorithms, such as Markov chain Monte Carlo, reliably finds
assortative or disassortive structure as directed by the value of a single
parameter. We show experimentally that the inference of our proposed model
quickly converges to the desired assortative or disassortative partition while
the inference of degree-corrected stochastic block model gets often trapped at
the inferior local optimal partitions when the traditional assortative
community structure is not strong in the observed networks.
</p>
"
http://arxiv.org/abs/1903.11752,ThunderNet: Towards Real-time Generic Object Detection. (arXiv:1903.11752v1 [cs.CV]),"<p>Real-time generic object detection on mobile platforms is a crucial but
challenging computer vision task. However, previous CNN-based detectors suffer
from enormous computational cost, which hinders them from real-time inference
in computation-constrained scenarios. In this paper, we investigate the
effectiveness of two-stage detectors in real-time generic detection and propose
a lightweight two-stage detector named ThunderNet. In the backbone part, we
analyze the drawbacks in previous lightweight backbones and present a
lightweight backbone designed for object detection. In the detection part, we
exploit an extremely efficient RPN and detection head design. To generate more
discriminative feature representation, we design two efficient architecture
blocks, Context Enhancement Module and Spatial Attention Module. At last, we
investigate the balance between the input resolution, the backbone, and the
detection head. Compared with lightweight one-stage detectors, ThunderNet
achieves superior performance with only 40% of the computational cost on PASCAL
VOC and COCO benchmarks. Without bells and whistles, our model runs at 24.1 fps
on an ARM-based device. To the best of our knowledge, this is the first
real-time detector reported on ARM platforms. Code will be released for paper
reproduction.
</p>
"
http://arxiv.org/abs/1903.11763,Finite Time Encryption Schedule in the Presence of an Eavesdropper with Operation Cost. (arXiv:1903.11763v1 [cs.SY]),"<p>In this paper, we consider a remote state estimation problem in the presence
of an eavesdropper. A smart sensor takes measurement of a discrete linear
time-invariant (LTI) process and sends its local state estimate through a
wireless network to a remote estimator. An eavesdropper can overhear the sensor
transmissions with a certain probability. To enhance the system privacy level,
we propose a novel encryption strategy to minimize a linear combination of the
expected error covariance at the remote estimator and the negative of the
expected error covariance at the eavesdropper, taking into account the cost of
the encryption process. We prove the existence of an optimal deterministic and
Markovian policy for such an encryption strategy over a finite time horizon.
Two situations, namely, with or without knowledge of the eavesdropper
estimation error covariance are studied and the optimal schedule is shown to
satisfy the threshold-like structure in both cases. Numerical examples are
given to illustrate the results.
</p>
"
http://arxiv.org/abs/1903.11764,When an attacker meets a cipher-image in 2018: A Year in Review. (arXiv:1903.11764v1 [cs.CR]),"<p>This paper aims to review the encountered technical contradictions when an
attacker meets the cipher-images encrypted by the image encryption schemes
(algorithms) proposed in 2018 from the viewpoint of an image cryptanalyst. The
most representative works among them are selected and classified according to
their essential structures. Almost all image cryptanalysis works published in
2018 are surveyed due to their small number. The challenging problems on design
and analysis of image encryption schemes are summarized to receive the
attentions of both designers and attackers (cryptanalysts) of image encryption
schemes, which may promote solving scenario-oriented image security problems
with new technologies.
</p>
"
http://arxiv.org/abs/1903.11765,Connecting Program Synthesis and Reachability: Automatic Program Repair using Test-Input Generation. (arXiv:1903.11765v1 [cs.PL]),"<p>We prove that certain formulations of program synthesis and reachability are
equivalent. Specifically, our constructive proof shows the reductions between
the template-based synthesis problem, which generates a program in a
pre-specified form, and the reachability problem, which decides the
reachability of a program location. This establishes a link between the two
research fields and allows for the transfer of techniques and results between
them.
</p>
<p>To demonstrate the equivalence, we develop a program repair prototype using
reachability tools. We transform a buggy program and its required specification
into a specific program containing a location reachable only when the original
program can be repaired, and then apply an off-the-shelf test-input generation
tool on the transformed program to find test values to reach the desired
location. Those test values correspond to repairs for the original programm.
Preliminary results suggest that our approach compares favorably to other
repair methods.
</p>
"
http://arxiv.org/abs/1903.11768,SymInfer: Inferring Program Invariants using Symbolic States. (arXiv:1903.11768v1 [cs.SE]),"<p>We introduce a new technique for inferring program invariants that uses
symbolic states generated by symbolic execution. Symbolic states, which consist
of path conditions and constraints on local variables, are a compact
description of sets of concrete program states and they can be used for both
invariant inference and invariant verification. Our technique uses a
counterexample-based algorithm that creates concrete states from symbolic
states, infers candidate invariants from concrete states, and then verifies or
refutes candidate invariants using symbolic states. The refutation case
produces concrete counterexamples that prevent spurious results and allow the
technique to obtain more precise invariants. This process stops when the
algorithm reaches a stable set of invariants.
</p>
<p>We present SymInfer, a tool that implements these ideas to automatically
generate invariants at arbitrary locations in a Java program. The tool obtains
symbolic states from Symbolic PathFinder and uses existing algorithms to infer
complex (potentially nonlinear) numerical invariants. Our preliminary results
show that SymInfer is effective in using symbolic states to generate precise
and useful invariants for proving program safety and analyzing program runtime
complexity. We also show that SymInfer outperforms existing invariant
generation systems.
</p>
"
http://arxiv.org/abs/1903.11770,An Improved Approach for Semantic Graph Composition with CCG. (arXiv:1903.11770v1 [cs.CL]),"<p>This paper builds on previous work using Combinatory Categorial Grammar (CCG)
to derive a transparent syntax-semantics interface for Abstract Meaning
Representation (AMR) parsing. We define new semantics for the CCG combinators
that is better suited to deriving AMR graphs. In particular, we define
symmetric alternatives for the application and composition combinators: these
require that the two constituents being combined overlap in one AMR relation.
We also provide a new semantics for type raising, which is necessary for
certain constructions. Using these mechanisms, we suggest an analysis of
eventive nouns, which present a challenge for deriving AMR graphs. Our
theoretical analysis will facilitate future work on robust and transparent AMR
parsing using CCG.
</p>
"
http://arxiv.org/abs/1903.11771,A Large-Scale Multi-Length Headline Corpus for Improving Length-Constrained Headline Generation Model Evaluation. (arXiv:1903.11771v1 [cs.CL]),"<p>Browsing news articles on multiple devices is now possible. The lengths of
news article headlines have precise upper bounds, dictated by the size of the
display of the relevant device or interface. Therefore, controlling the length
of headlines is essential when applying the task of headline generation to news
production. However, because there is no corpus of headlines of multiple
lengths for a given article, prior researches on controlling output length in
headline generation have not discussed whether the evaluation of the setting
that uses a single length reference can evaluate multiple length outputs
appropriately. In this paper, we introduce two corpora (JNC and JAMUL) to
confirm the validity of prior experimental settings and provide for the next
step toward the goal of controlling output length in headline generation. The
JNC provides common supervision data for headline generation. The JAMUL is a
large-scale evaluation dataset for headlines of three different lengths
composed by professional editors. We report new findings on these corpora; for
example, while the longest length reference summary can appropriately evaluate
the existing methods controlling output length, the methods do not manage the
selection of words according to length constraint.
</p>
"
http://arxiv.org/abs/1903.11774,How to pick the domain randomization parameters for sim-to-real transfer of reinforcement learning policies?. (arXiv:1903.11774v1 [cs.LG]),"<p>Recently, reinforcement learning (RL) algorithms have demonstrated remarkable
success in learning complicated behaviors from minimally processed input.
However, most of this success is limited to simulation. While there are
promising successes in applying RL algorithms directly on real systems, their
performance on more complex systems remains bottle-necked by the relative data
inefficiency of RL algorithms. Domain randomization is a promising direction of
research that has demonstrated impressive results using RL algorithms to
control real robots. At a high level, domain randomization works by training a
policy on a distribution of environmental conditions in simulation. If the
environments are diverse enough, then the policy trained on this distribution
will plausibly generalize to the real world. A human-specified design choice in
domain randomization is the form and parameters of the distribution of
simulated environments. It is unclear how to the best pick the form and
parameters of this distribution and prior work uses hand-tuned distributions.
This extended abstract demonstrates that the choice of the distribution plays a
major role in the performance of the trained policies in the real world and
that the parameter of this distribution can be optimized to maximize the
performance of the trained policies in the real world
</p>
"
http://arxiv.org/abs/1903.11775,Atrial Fibrillation Detection Using Deep Features and Convolutional Networks. (arXiv:1903.11775v1 [cs.LG]),"<p>Atrial fibrillation is a cardiac arrhythmia that affects an estimated 33.5
million people globally and is the potential cause of 1 in 3 strokes in people
over the age of 60. Detection and diagnosis of atrial fibrillation (AFIB) is
done noninvasively in the clinical environment through the evaluation of
electrocardiograms (ECGs). Early research into automated methods for the
detection of AFIB in ECG signals focused on traditional bio-medical signal
analysis to extract important features for use in statistical classification
models. Artificial intelligence models have more recently been used that employ
convolutional and/or recurrent network architectures. In this work, significant
time and frequency domain characteristics of the ECG signal are extracted by
applying the short-time Fourier trans-form and then visually representing the
information in a spectrogram. Two different classification approaches were
investigated that utilized deep features in the spectrograms construct-ed from
ECG segments. The first approach used a pretrained DenseNet model to extract
features that were then classified using Support Vector Machines, and the
second approach used the spectrograms as direct input into a convolutional
network. Both approaches were evaluated against the MIT-BIH AFIB dataset, where
the convolutional network approach achieved a classification accuracy of
93.16%. While these results do not surpass established automated atrial
fibrillation detection methods, they are promising and warrant further
investigation given they did not require any noise prefiltering, hand-crafted
features, nor a reliance on beat detection.
</p>
"
http://arxiv.org/abs/1903.11777,What you get is what you see: Decomposing Epistemic Planning using Functional STRIPS. (arXiv:1903.11777v1 [cs.AI]),"<p>Epistemic planning --- planning with knowledge and belief --- is essential in
many multi-agent and human-agent interaction domains. Most state-of-the-art
epistemic planners solve this problem by compiling to propositional classical
planning, for example, generating all possible knowledge atoms, or compiling
epistemic formula to normal forms. However, these methods become
computationally infeasible as problems grow. In this paper, we decompose
epistemic planning by delegating reasoning about epistemic formula to an
external solver. We do this by modelling the problem using \emph{functional
STRIPS}, which is more expressive than standard STRIPS and supports the use of
external, black-box functions within action models. Exploiting recent work that
demonstrates the relationship between what an agent `sees' and what it knows,
we allow modellers to provide new implementations of externals functions. These
define what agents see in their environment, allowing new epistemic logics to
be defined without changing the planner. As a result, it increases the
capability and flexibility of the epistemic model itself, and avoids the
exponential pre-compilation step. We ran evaluations on well-known epistemic
planning benchmarks to compare with an existing state-of-the-art planner, and
on new scenarios based on different external functions. The results show that
our planner scales significantly better than the state-of-the-art planner
against which we compared, and can express problems more succinctly.
</p>
"
http://arxiv.org/abs/1903.11779,BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames. (arXiv:1903.11779v1 [cs.CV]),"<p>Semi-supervised video object segmentation has made significant progress on
real and challenging videos in recent years. The current paradigm for
segmentation methods and benchmark datasets is to segment objects in video
provided a single annotation in the first frame. However, we find that
segmentation performance across the entire video varies dramatically when
selecting an alternative frame for annotation. This paper address the problem
of learning to suggest the single best frame across the video for user
annotation---this is, in fact, never the first frame of video. We achieve this
by introducing BubbleNets, a novel deep sorting network that learns to select
frames using a performance-based loss function that enables the conversion of
expansive amounts of training examples from already existing datasets. Using
BubbleNets, we are able to achieve an 11% relative improvement in segmentation
performance on the DAVIS benchmark without any changes to the underlying method
of segmentation.
</p>
"
http://arxiv.org/abs/1903.11780,Wasserstein Dependency Measure for Representation Learning. (arXiv:1903.11780v1 [cs.LG]),"<p>Mutual information maximization has emerged as a powerful learning objective
for unsupervised representation learning obtaining state-of-the-art performance
in applications such as object recognition, speech recognition, and
reinforcement learning. However, such approaches are fundamentally limited
since a tight lower bound of mutual information requires sample size
exponential in the mutual information. This limits the applicability of these
approaches for prediction tasks with high mutual information, such as in video
understanding or reinforcement learning. In these settings, such techniques are
prone to overfit, both in theory and in practice, and capture only a few of the
relevant factors of variation. This leads to incomplete representations that
are not optimal for downstream tasks. In this work, we empirically demonstrate
that mutual information-based representation learning approaches do fail to
learn complete representations on a number of designed and real-world tasks. To
mitigate these problems we introduce the Wasserstein dependency measure, which
learns more complete representations by using the Wasserstein distance instead
of the KL divergence in the mutual information estimator. We show that a
practical approximation to this theoretically motivated solution, constructed
using Lipschitz constraint techniques from the GAN literature, achieves
substantially improved results on tasks where incomplete representations are a
major challenge.
</p>
"
http://arxiv.org/abs/1903.11782,Anywhere Decoding: Low-Overhead Uplink Interference Management for Wireless Networks. (arXiv:1903.11782v1 [cs.IT]),"<p>Inter-cell interference (ICI) is one of the major performance-limiting
factors in the context of modern cellular systems. To tackle ICI, coordinated
multi-point (CoMP) schemes have been proposed as a key technology for
next-generation mobile communication systems. Although CoMP schemes offer
promising theoretical gains, their performance could degrade significantly
because of practical issues such as limited backhaul. To address this issue, we
explore a novel uplink interference management scheme called anywhere decoding,
which requires exchanging just a few bits of information per coding interval
among the base stations (BSs). In spite of the low overhead of anywhere
decoding, we observe considerable gains in the outage probability performance
of cell-edge users, compared to no cooperation between BSs. Additionally,
asymptotic results of the outage probability for high-SNR regimes demonstrate
that anywhere decoding schemes achieve full spatial diversity through multiple
decoding opportunities, and they are within 1.5 dB of full cooperation.
</p>
"
http://arxiv.org/abs/1903.11783,A dataset for resolving referring expressions in spoken dialogue via contextual query rewrites (CQR). (arXiv:1903.11783v1 [cs.CL]),"<p>We present Contextual Query Rewrite (CQR) a dataset for multi-domain
task-oriented spoken dialogue systems that is an extension of the Stanford
dialog corpus (Eric et al., 2017a). While previous approaches have addressed
the issue of diverse schemas by learning candidate transformations (Naik et
al., 2018), we instead model the reference resolution task as a user query
reformulation task, where the dialog state is serialized into a natural
language query that can be executed by the downstream spoken language
understanding system. In this paper, we describe our methodology for creating
the query reformulation extension to the dialog corpus, and present an initial
set of experiments to establish a baseline for the CQR task. We have released
the corpus to the public [1] to support further research in this area.
</p>
"
http://arxiv.org/abs/1903.11785,A Fast Free-viewpoint Video Synthesis Algorithm for Sports Scenes. (arXiv:1903.11785v1 [cs.CV]),"<p>In this paper, we report on a parallel freeviewpoint video synthesis
algorithm that can efficiently reconstruct a high-quality 3D scene
representation of sports scenes. The proposed method focuses on a scene that is
captured by multiple synchronized cameras featuring wide-baselines. The
following strategies are introduced to accelerate the production of a
free-viewpoint video taking the improvement of visual quality into account: (1)
a sparse point cloud is reconstructed using a volumetric visual hull approach,
and an exact 3D ROI is found for each object using an efficient connected
components labeling algorithm. Next, the reconstruction of a dense point cloud
is accelerated by implementing visual hull only in the ROIs; (2) an accurate
polyhedral surface mesh is built by estimating the exact intersections between
grid cells and the visual hull; (3) the appearance of the reconstructed
presentation is reproduced in a view-dependent manner that respectively renders
the non-occluded and occluded region with the nearest camera and its
neighboring cameras. The production for volleyball and judo sequences
demonstrates the effectiveness of our method in terms of both execution time
and visual quality.
</p>
"
http://arxiv.org/abs/1903.11787,Successive-Cancellation Decoding of Linear Source Code. (arXiv:1903.11787v1 [cs.IT]),"<p>This paper investigates the error probability of several decoding methods for
a source code with decoder side information, where the decoding methods are: 1)
symbol-wise maximum a posteriori decoding, 2) successive-cancellation decoding,
and 3) stochastic successive-cancellation decoding. The proof of the
effectiveness of a decoding method is reduced to that for an arbitrary decoding
method, where `effective' means that the error probability goes to zero as $n$
goes to infinity. Furthermore, we revisit the polar source code showing that
stochastic successive-cancellation decoding, as well as successive-cancellation
decoding, is effective for this code.
</p>
"
http://arxiv.org/abs/1903.11788,Cherenkov Detectors Fast Simulation Using Neural Networks. (arXiv:1903.11788v1 [hep-ex]),"<p>We propose a way to simulate Cherenkov detector response using a generative
adversarial neural network to bypass low-level details. This network is trained
to reproduce high level features of the simulated detector events based on
input observables of incident particles. This allows the dramatic increase of
simulation speed. We demonstrate that this approach provides simulation
precision which is consistent with the baseline and discuss possible
implications of these results.
</p>
"
http://arxiv.org/abs/1903.11789,Step Change Improvement in ADMET Prediction with PotentialNet Deep Featurization. (arXiv:1903.11789v1 [cs.LG]),"<p>The Absorption, Distribution, Metabolism, Elimination, and Toxicity (ADMET)
properties of drug candidates are estimated to account for up to 50% of all
clinical trial failures. Predicting ADMET properties has therefore been of
great interest to the cheminformatics and medicinal chemistry communities in
recent decades. Traditional cheminformatics approaches, whether the learner is
a random forest or a deep neural network, leverage fixed fingerprint feature
representations of molecules. In contrast, in this paper, we learn the features
most relevant to each chemical task at hand by representing each molecule
explicitly as a graph, where each node is an atom and each edge is a bond. By
applying graph convolutions to this explicit molecular representation, we
achieve, to our knowledge, unprecedented accuracy in prediction of ADMET
properties. By challenging our methodology with rigorous cross-validation
procedures and prospective analyses, we show that deep featurization better
enables molecular predictors to not only interpolate but also extrapolate to
new regions of chemical space.
</p>
"
http://arxiv.org/abs/1903.11791,Hierarchical Pooling Structure for Weakly Labeled Sound Event Detection. (arXiv:1903.11791v1 [cs.SD]),"<p>Sound event detection with weakly labeled data is considered as a problem of
multi-instance learning. And the choice of pooling function is the key to
solving this problem. In this paper, we proposed a hierarchical pooling
structure to improve the performance of weakly labeled sound event detection
system. Proposed pooling structure has made remarkable improvements on three
types of pooling function without adding any parameters. Moreover, our system
has achieved competitive performance on Task 4 of Detection and Classification
of Acoustic Scenes and Events (DCASE) 2017 Challenge using hierarchical pooling
structure.
</p>
"
http://arxiv.org/abs/1903.11800,Pyramid Mask Text Detector. (arXiv:1903.11800v1 [cs.CV]),"<p>Scene text detection, an essential step of scene text recognition system, is
to locate text instances in natural scene images automatically. Some recent
attempts benefiting from Mask R-CNN formulate scene text detection task as an
instance segmentation problem and achieve remarkable performance. In this
paper, we present a new Mask R-CNN based framework named Pyramid Mask Text
Detector (PMTD) to handle the scene text detection. Instead of binary text mask
generated by the existing Mask R-CNN based methods, our PMTD performs
pixel-level regression under the guidance of location-aware supervision,
yielding a more informative soft text mask for each text instance. As for the
generation of text boxes, PMTD reinterprets the obtained 2D soft mask into 3D
space and introduces a novel plane clustering algorithm to derive the optimal
text box on the basis of 3D shape. Experiments on standard datasets demonstrate
that the proposed PMTD brings consistent and noticeable gain and clearly
outperforms state-of-the-art methods. Specifically, it achieves an F-measure of
80.13% on ICDAR 2017 MLT dataset.
</p>
"
http://arxiv.org/abs/1903.11807,On the Spectral Efficiency for Massive MIMO Systems With Imperfect Spacial Covariance Information. (arXiv:1903.11807v1 [cs.IT]),"<p>This paper studies the impact of imperfect channel covariance information on
the uplink and downlink spectral efficiencies of a time division duplexed (TDD)
massive multiple-input multiple-output (MIMO) system. Specifically, we derive
analytical lower bounds on the uplink and downlink spectral efficiencies of a
user with imperfect knowledge on channel vectors, as well as that of the
channel covariance matrices, of the users. We consider a linear minimum mean
squared estimator (LMMSE) and element-wise LMMSE channel estimation for the
unknown channel in this paper. These analytical bounds enable us to choose the
sample size for covariance matrix estimation to meet spectral efficiency
requirements. The accurate agreement between derived bounds and simulated
bounds based on random samples of channel vector and covariance matrices is
shown.
</p>
"
http://arxiv.org/abs/1903.11808,Exploiting the Shipping Lane Information for Energy-Efficient Maritime Communications. (arXiv:1903.11808v1 [cs.IT]),"<p>Energy efficiency is a crucial issue for maritime communications, due to the
limitation of geographically available base station sites. Different from
previous studies, we promote the energy efficiency by exploiting the specific
characteristics of maritime channels and user mobility. Particularly, we
utilize the shipping lane information to obtain the long-term position
information of marine users, from which the large-scale channel state
information is estimated. Based on that, the resource allocation is jointly
optimized for all users and all time slots during the voyage, leading to a
mixed 0-1 non-convex programming problem. We transform this problem into a
convex one by means of variable substitution and time-sharing relaxation, and
propose an iterative algorithm to solve it based on the Lagrangian dual
decomposition method. Simulation results demonstrate that the proposed scheme
can significantly reduce the power consumption compared with existing
approaches, due to the global optimization over a much larger time span by
utilizing the shipping lane information.
</p>
"
http://arxiv.org/abs/1903.11812,Analysis of distracted pedestrians' waiting time: Head-Mounted Immersive Virtual Reality application. (arXiv:1903.11812v1 [cs.HC]),"<p>This paper analyzes the distracted pedestrians' waiting time before crossing
the road in three conditions: 1) not distracted, 2) distracted with a
smartphone and 3) distracted with a smartphone in the presence of virtual
flashing LED lights on the crosswalk as a safety measure. For the means of data
collection, we adapted an in-house developed virtual immersive reality
environment (VIRE). A total of 42 volunteers participated in the experiment.
Participants' positions and head movements were recorded and used to calculate
walking speeds, acceleration and deceleration rates, surrogate safety measures,
time spent playing smartphone game, etc. After a descriptive analysis on the
data, the effects of these variables on pedestrians' waiting time are analyzed
by employing a cox proportional hazard model. Several factors were identified
as having impact on waiting time. The results show that an increase in initial
walk speed, percentage of time the head was oriented toward smartphone during
crossing, bigger minimum missed gaps and unsafe crossings resulted in shorter
waiting times. On the other hand, an increase in the percentage of time the
head was oriented toward smartphone during waiting time, crossing time and maze
solving time, means longer waiting times for participants.
</p>
"
http://arxiv.org/abs/1903.11814,"Hybrid Satellite-Terrestrial Communication Networks for the Maritime Internet of Things: Key Technologies, Opportunities, and Challenges. (arXiv:1903.11814v1 [cs.NI])","<p>With the rapid development of marine activities, there has been an increasing
number of maritime mobile terminals, as well as a growing demand for high-speed
and ultra-reliable maritime communications to keep them connected.
Traditionally, the maritime Internet of Things (IoT) is enabled by maritime
satellites. However, satellites are seriously restricted by their high latency
and relatively low data rate. As an alternative, shore &amp; island-based base
stations (BSs) can be built to extend the coverage of terrestrial networks
using fourth-generation (4G), fifth-generation (5G), and beyond 5G services.
Unmanned aerial vehicles can also be exploited to serve as aerial maritime BSs.
Despite of all these approaches, there are still open issues for an efficient
maritime communication network (MCN). For example, due to the complicated
electromagnetic propagation environment, the limited geometrically available BS
sites, and rigorous service demands from mission-critical applications,
conventional communication and networking theories and methods should be
tailored for maritime scenarios. Towards this end, we provide a survey on the
demand for maritime communications, the state-of-the-art MCNs, and key
technologies for enhancing transmission efficiency, extending network coverage,
and provisioning maritime-specific services. Future challenges in developing an
environment-aware, service-driven, and integrated satellite-air-ground MCN to
be smart enough to utilize external auxiliary information, e.g., sea state and
atmosphere conditions, are also discussed.
</p>
"
http://arxiv.org/abs/1903.11816,FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation. (arXiv:1903.11816v1 [cs.CV]),"<p>Modern approaches for semantic segmentation usually employ dilated
convolutions in the backbone to extract high-resolution feature maps, which
brings heavy computation complexity and memory footprint. To replace the time
and memory consuming dilated convolutions, we propose a novel joint upsampling
module named Joint Pyramid Upsampling (JPU) by formulating the task of
extracting high-resolution feature maps into a joint upsampling problem. With
the proposed JPU, our method reduces the computation complexity by more than
three times without performance loss. Experiments show that JPU is superior to
other upsampling modules, which can be plugged into many existing approaches to
reduce computation complexity and improve performance. By replacing dilated
convolutions with the proposed JPU module, our method achieves the
state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and
ADE20K dataset (final score of 0.5584) while running 3 times faster.
</p>
"
http://arxiv.org/abs/1903.11821,SRDGAN: learning the noise prior for Super Resolution with Dual Generative Adversarial Networks. (arXiv:1903.11821v1 [cs.MM]),"<p>Single Image Super Resolution (SISR) is the task of producing a high
resolution (HR) image from a given low-resolution (LR) image. It is a well
researched problem with extensive commercial applications such as digital
camera, video compression, medical imaging and so on. Most super resolution
works focus on the features learning architecture, which can recover the
texture details as close as possible. However, these works suffer from the
following challenges: (1) The low-resolution (LR) training images are
artificially synthesized using HR images with bicubic downsampling, which have
much richer-information than real demosaic-upscaled mobile images. The mismatch
between training and inference mobile data heavily blocks the improvement of
practical super resolution algorithms. (2) These methods cannot effectively
handle the blind distortions during super resolution in practical applications.
In this work, an end-to-end novel framework, including high-to-low network and
low-to-high network, is proposed to solve the above problems with dual
Generative Adversarial Networks (GAN). First, the above mismatch problems are
well explored with the high-to-low network, where clear high-resolution image
and the corresponding realistic low-resolution image pairs can be generated.
Moreover, a large-scale General Mobile Super Resolution Dataset, GMSR, is
proposed, which can be utilized for training or as a fair comparison benchmark
for super resolution methods. Second, an effective low-to-high network (super
resolution network) is proposed in the framework. Benefiting from the GMSR
dataset and novel training strategies, the super resolution model can
effectively handle detail recovery and denoising at the same time.
</p>
"
http://arxiv.org/abs/1903.11833,Skip prediction using boosting trees based on acoustic features of tracks in sessions. (arXiv:1903.11833v1 [cs.IR]),"<p>The Spotify Sequential Skip Prediction Challenge focuses on predicting if a
track in a session will be skipped by the user or not. In this paper, we
describe our approach to this problem and the final system that was submitted
to the challenge by our team from the Music Technology Group (MTG) under the
name ""aferraro"". This system consists in combining the predictions of multiple
boosting trees models trained with features extracted from the sessions and the
tracks. The proposed approach achieves good overall performance (MAA of 0.554),
with our model ranked 14th out of more than 600 submissions in the final
leaderboard.
</p>
"
http://arxiv.org/abs/1903.11834,Feature Fusion Encoder Decoder Network For Automatic Liver Lesion Segmentation. (arXiv:1903.11834v1 [cs.CV]),"<p>Liver lesion segmentation is a difficult yet critical task for medical image
analysis. Recently, deep learning based image segmentation methods have
achieved promising performance, which can be divided into three categories: 2D,
2.5D and 3D, based on the dimensionality of the models. However, 2.5D and 3D
methods can have very high complexity and 2D methods may not perform
satisfactorily. To obtain competitive performance with low complexity, in this
paper, we propose a Feature-fusion Encoder-Decoder Network (FED-Net) based 2D
segmentation model to tackle the challenging problem of liver lesion
segmentation from CT images. Our feature fusion method is based on the
attention mechanism, which fuses high-level features carrying semantic
information with low-level features having image details. Additionally, to
compensate for the information loss during the upsampling process, a dense
upsampling convolution and a residual convolutional structure are proposed. We
tested our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS)
Challenge and achieved competitive results compared with other state-of-the-art
methods.
</p>
"
http://arxiv.org/abs/1903.11835,A Survey on Graph Kernels. (arXiv:1903.11835v1 [cs.LG]),"<p>Graph kernels have become an established and widely-used technique for
solving classification tasks on graphs. This survey gives a comprehensive
overview of techniques for kernel-based graph classification developed in the
past 15 years. We describe and categorize graph kernels based on properties
inherent to their design, such as the nature of their extracted graph features,
their method of computation and their applicability to problems in practice. In
an extensive experimental evaluation, we study the classification accuracy of a
large suite of graph kernels on established benchmarks as well as new datasets.
We compare the performance of popular kernels with several baseline methods and
study the effect of applying a Gaussian RBF kernel to the metric induced by a
graph kernel. In doing so, we find that simple baselines become competitive
after this transformation on some datasets. Moreover, we study the extent to
which existing graph kernels agree in their predictions (and prediction errors)
and obtain a data-driven categorization of kernels as result. Finally, based on
our experimental results, we derive a practitioner's guide to kernel-based
graph classification.
</p>
"
http://arxiv.org/abs/1903.11844,DDoS Attack Detection Method Based on Network Abnormal Behavior in Big Data Environment. (arXiv:1903.11844v1 [cs.CR]),"<p>Distributed denial of service (DDoS) attack becomes a rapidly growing problem
with the fast development of the Internet. The existing DDoS attack detection
methods have time-delay and low detection rate. This paper presents a DDoS
attack detection method based on network abnormal behavior in a big data
environment. Based on the characteristics of flood attack, the method filters
the network flows to leave only the 'many-to-one' network flows to reduce the
interference from normal network flows and improve the detection accuracy. We
define the network abnormal feature value (NAFV) to reflect the state changes
of the old and new IP address of 'many-to-one' network flows. Finally, the DDoS
attack detection method based on NAFV real-time series is built to identify the
abnormal network flow states caused by DDoS attacks. The experiments show that
compared with similar methods, this method has higher detection rate, lower
false alarm rate and missing rate.
</p>
"
http://arxiv.org/abs/1903.11848,Sogou Machine Reading Comprehension Toolkit. (arXiv:1903.11848v1 [cs.CL]),"<p>Machine reading comprehension have been intensively studied in recent years,
and neural network-based models have shown dominant performances. In this
paper, we present a Sogou Machine Reading Comprehension (SMRC) toolkit that can
be used to provide the fast and efficient development of modern machine
comprehension models, including both published models and original prototypes.
To achieve this goal, the toolkit provides dataset readers, a flexible
preprocessing pipeline, necessary neural network components, and built-in
models, which make the whole process of data preparation, model construction,
and training easier.
</p>
"
http://arxiv.org/abs/1903.11849,Inertial Sensor Aided mmWave Beam Tracking to Support Cooperative Autonomous Driving. (arXiv:1903.11849v1 [eess.SP]),"<p>This paper presents an inertial sensor aided technique for beam alignment and
tracking in massive multiple-input multiple-output (MIMO) vehicle-to-vehicle
(V2V) communications based on millimeter waves (mmWave). Since directional
communications in vehicular scenarios are severely hindered by beam pointing
issues, a beam alignment procedure has to be periodically carried out to
guarantee the communication reliability. When dealing with massive MIMO links,
the beam sweeping approach is known to be time consuming and often unfeasible
due to latency constraints. To speed up the process, we propose a method that
exploits a-priori information on array dynamics provided by an inertial sensor
on transceivers to assist the beam alignment procedure. The proposed inertial
sensor aided technique allows a continuous tracking of the beam while
transmitting, avoiding frequent realignment phases. Numerical results based on
real measurements of on-transceiver accelerometers demonstrate a significant
gain in terms of V2V communication throughput with respect to conventional beam
alignment protocols.
</p>
"
http://arxiv.org/abs/1903.11850,Mining Discourse Markers for Unsupervised Sentence Representation Learning. (arXiv:1903.11850v1 [cs.CL]),"<p>Current state of the art systems in NLP heavily rely on manually annotated
datasets, which are expensive to construct. Very little work adequately
exploits unannotated data -- such as discourse markers between sentences --
mainly because of data sparseness and ineffective extraction methods. In the
present work, we propose a method to automatically discover sentence pairs with
relevant discourse markers, and apply it to massive amounts of data. Our
resulting dataset contains 174 discourse markers with at least 10k examples
each, even for rare markers such as coincidentally or amazingly We use the
resulting data as supervision for learning transferable sentence embeddings. In
addition, we show that even though sentence representation learning through
prediction of discourse markers yields state of the art results across
different transfer tasks, it is not clear that our models made use of the
semantic relation between sentences, thus leaving room for further
improvements. Our datasets are publicly available
(https://github.com/synapse-developpement/Discovery)
</p>
"
http://arxiv.org/abs/1903.11851,Feature Intertwiner for Object Detection. (arXiv:1903.11851v1 [cs.CV]),"<p>A well-trained model should classify objects with a unanimous score for every
category. This requires the high-level semantic features should be as much
alike as possible among samples. To achive this, previous works focus on
re-designing the loss or proposing new regularization constraints. In this
paper, we provide a new perspective. For each category, it is assumed that
there are two feature sets: one with reliable information and the other with
less reliable source. We argue that the reliable set could guide the feature
learning of the less reliable set during training - in spirit of student
mimicking teacher behavior and thus pushing towards a more compact class
centroid in the feature space. Such a scheme also benefits the reliable set
since samples become closer within the same category - implying that it is
easier for the classifier to identify. We refer to this mutual learning process
as feature intertwiner and embed it into object detection. It is well-known
that objects of low resolution are more difficult to detect due to the loss of
detailed information during network forward pass (e.g., RoI operation). We thus
regard objects of high resolution as the reliable set and objects of low
resolution as the less reliable set. Specifically, an intertwiner is designed
to minimize the distribution divergence between two sets. The choice of
generating an effective feature representation for the reliable set is further
investigated, where we introduce the optimal transport (OT) theory into the
framework. Samples in the less reliable set are better aligned with aid of OT
metric. Incorporated with such a plug-and-play intertwiner, we achieve an
evident improvement over previous state-of-the-arts.
</p>
"
http://arxiv.org/abs/1903.11854,Knowledge Management in Medium-Sized Software Consulting Companies: An investigation of Intranet-based Knowledge Management Tools for Knowledge Cartography and Knowledge Repositories for Learning Software Organisations. (arXiv:1903.11854v1 [cs.SE]),"<p>Companies that develop software have a pressure from customers to deliver
better solutions, and to deliver solutions faster and cheaper. Many researchers
have worked with suggestions on how to improve the development process;
software process improvement. As software development is a very knowledge
intensive task, both researchers and industry have recently turned their
attention to knowledge management as a means to improve software development.
This often involves developing technical tools, which many companies have spent
resources on. But the tools are often not used in practise by developers and
managers in the companies, and it is often unknown if the tools improve how
knowledge is managed. In order to build efficient knowledge management tools,
we need a better understanding of how the tools that exist are applied and used
in software development. We present and analyse eight case studies of knowledge
management initiatives from the literature. We found evidence of improved
software quality, reduced development costs and evidence of a better working
environment for developers as a result of these initiatives. Further, we
examine success criteria in knowledge management codification initiatives,
based on Intranet tools in medium-sized software companies. In addition, we
investigate how knowledge management tools are used for different purposes by
different groups of users in two software consulting companies. They use tools
both as support for personalization and codification strategies. The consulting
companies are two medium-sized Norwegian companies with 40 and 150 employees,
which work in development projects that lasts from a few weeks to several
years.
</p>
"
http://arxiv.org/abs/1903.11857,Analysis and Extension of the Evidential Reasoning Algorithm for Multiple Attribute Decision Analysis with Uncertainty. (arXiv:1903.11857v1 [cs.AI]),"<p>In multiple attribute decision analysis (MADA) problems, one often needs to
deal with assessment information with uncertainty. The evidential reasoning
approach is one of the most effective methods to deal with such MADA problems.
As kernel of the evidential reasoning approach, an original evidential
reasoning (ER) algorithm was firstly proposed by Yang et al, and later they
modified the ER algorithm in order to satisfy the proposed four synthesis
axioms with which a rational aggregation process needs to satisfy. However, up
to present, the essential difference of the two ER algorithms as well as the
rationality of the synthesis axioms are still unclear. In this paper, we
analyze the ER algorithms in Dempster-Shafer theory (DST) framework and prove
that the original ER algorithm follows the reliability discounting and
combination scheme, while the modified one follows the importance discounting
and combination scheme. Further we reveal that the four synthesis axioms are
not valid criteria to check the rationality of one attribute aggregation
algorithm. Based on these new findings, an extended ER algorithm is proposed to
take into account both the reliability and importance of different attributes,
which provides a more general attribute aggregation scheme for MADA with
uncertainty. A motorcycle performance assessment problem is examined to
illustrate the proposed algorithm.
</p>
"
http://arxiv.org/abs/1903.11860,Complete Disjoint coNP-Pairs but no Complete Total Polynomial Search Problems Relative to an Oracle. (arXiv:1903.11860v1 [cs.CC]),"<p>Consider the following conjectures:
</p>
<p>H1: the set TFNP of all total polynomial search problems has no complete
problems with respect to polynomial reductions.
</p>
<p>H2: there exists no many-one complete disjoint coNP-pair.
</p>
<p>We construct an oracle relative to which H1 holds and H2 does not hold. This
partially answers a question by Pudl\'ak [Pud17], who lists several hypotheses
and asks for oracles that show corresponding relativized hypotheses to be
different. As there exists a relativizable proof for the implication H1 -&gt; H2
[Pud17], the relativizations of the hypotheses H1 and H2 are neither
independent nor equivalent.
</p>
"
http://arxiv.org/abs/1903.11862,Smooth Adversarial Examples. (arXiv:1903.11862v1 [cs.CV]),"<p>This paper investigates the visual quality of the adversarial examples.
Recent papers propose to smooth the perturbations to get rid of high frequency
artefacts. In this work, smoothing has a different meaning as it perceptually
shapes the perturbation according to the visual content of the image to be
attacked. The perturbation becomes locally smooth on the flat areas of the
input image, but it may be noisy on its textured areas and sharp across its
edges.
</p>
<p>This operation relies on Laplacian smoothing, well-known in graph signal
processing, which we integrate in the attack pipeline. We benchmark several
attacks with and without smoothing under a white-box scenario and evaluate
their transferability. Despite the additional constraint of smoothness, our
attack has the same probability of success at lower distortion.
</p>
"
http://arxiv.org/abs/1903.11863,On Inertial Navigation and Attitude Initialization in Polar Areas. (arXiv:1903.11863v1 [cs.RO]),"<p>Inertial navigation and attitude initialization in polar areas become a hot
topic in recent years in the navigation community, as the widely-used
navigation mechanization of the local level frame encounters the inherent
singularity when the latitude approaches 90 degrees. Great endeavors have been
devoted to devising novel navigation mechanizations such as the grid or
transversal frames. This paper highlights the fact that the common Earth-frame
mechanization is sufficiently good to well handle the singularity problem in
polar areas. Simulation results are reported to demonstrate the singularity
problem and the effectiveness of the Earth-frame mechanization.
</p>
"
http://arxiv.org/abs/1903.11873,Inconsistency indices for incomplete pairwise comparisons matrices. (arXiv:1903.11873v1 [cs.DM]),"<p>Comparing alternatives in pairs is a very well known technique of ranking
creation. The answer to how reliable and trustworthy ranking is depends on the
inconsistency of the data from which it was created. There are many indices
used for determining the level of inconsistency among compared alternatives.
Unfortunately, most of them assume that the set of comparisons is complete,
i.e. every single alternative is compared to each other. This is not true and
the ranking must sometimes be made based on incomplete data. In order to fill
this gap, this work aims to adapt the selected twelve existing inconsistency
indices for the purpose of analyzing incomplete data sets. The modified indices
are subjected to Monte Carlo experiments. Those of them that achieved the best
results in the experiments carried out are recommended for use in practice.
</p>
"
http://arxiv.org/abs/1903.11874,Block stochastic gradient descent for large-scale tomographic reconstruction in a parallel network. (arXiv:1903.11874v1 [cs.DC]),"<p>Iterative algorithms have many advantages for linear tomographic image
reconstruction when compared to back-projection based methods. However,
iterative methods tend to have significantly higher computational complexity.
To overcome this, parallel processing schemes that can utilise several
computing nodes are desirable. Popular methods here are row action methods,
which update the entire image simultaneously and column action methods, which
require access to all measurements at each node. In large scale tomographic
reconstruction with limited storage capacity of each node, data communication
overheads between nodes becomes a significant performance limiting factor. To
reduce this overhead, we proposed a row action method BSGD. The method is based
on the stochastic gradient descent method but it does not update the entire
image at each iteration, which reduces between node communication. To further
increase convergence speeds, an importance sampling strategy is proposed. We
compare BSGD to other existing stochastic methods and show its effectiveness
and efficiency. Other properties of BSGD are also explored, including its
ability to incorporate total variation (TV) regularization and automatic
parameter tuning.
</p>
"
http://arxiv.org/abs/1903.11875,A Noise Mitigation Approach for VLC Systems. (arXiv:1903.11875v1 [cs.NI]),"<p>Visible Light Communication (VLC) is based on the dual use of the
illumination infrastructure for wireless data communication. The major interest
on this communication technology lies on its specific features to be a secure,
cost-effective wireless technology. Recently, this technology has gained an
important role as potential candidate for complementing traditional RF
communication systems. Anyway a major issue for the VLC development is a deep
comprehension of the noise and its impact on the received signal at the
receiver. In this work, we present a simple but effective approach to analyze
the noise and drastically reduce it through a signal processing method. In
order to validate the effectiveness of this analytical approach, we have
developed an USRP-based testbed. Experimental results have been carried out by
evaluating the symbol error rate (SER) and show the effectiveness of the noise
mitigation approach in different interference conditions and at different
distance between the transmitter and the receiver.
</p>
"
http://arxiv.org/abs/1903.11891,AED-Net: An Abnormal Event Detection Network. (arXiv:1903.11891v1 [cs.CV]),"<p>It is challenging to detect the anomaly in crowded scenes for quite a long
time. In this paper, a self-supervised framework, abnormal event detection
network (AED-Net), which is composed of PCAnet and kernel principal component
analysis (kPCA), is proposed to address this problem. Using surveillance video
sequences of different scenes as raw data, PCAnet is trained to extract
high-level semantics of crowd's situation. Next, kPCA,a one-class classifier,
is trained to determine anomaly of the scene. In contrast to some prevailing
deep learning methods,the framework is completely self-supervised because it
utilizes only video sequences in a normal situation. Experiments of global and
local abnormal event detection are carried out on UMN and UCSD datasets, and
competitive results with higher EER and AUC compared to other state-of-the-art
methods are observed. Furthermore, by adding local response normalization (LRN)
layer, we propose an improvement to original AED-Net. And it is proved to
perform better by promoting the framework's generalization capacity according
to the experiments.
</p>
"
http://arxiv.org/abs/1903.11899,Using Blockchain to Rein in The New Post-Truth World and Check The Spread of Fake News. (arXiv:1903.11899v1 [cs.CR]),"<p>In recent years, `fake news' has become a global issue that raises
unprecedented challenges for human society and democracy. This problem has
arisen due to the emergence of various concomitant phenomena such as (1) the
digitization of human life and the ease of disseminating news through social
networking applications (such as Facebook and WhatsApp); (2) the availability
of `big data' that allows customization of news feeds and the creation of
polarized so-called `filter-bubbles'; and (3) the rapid progress made by
generative machine learning (ML) and deep learning (DL) algorithms in creating
realistic-looking yet fake digital content (such as text, images, and videos).
There is a crucial need to combat the rampant rise of fake news and
disinformation. In this paper, we propose a high-level overview of a
blockchain-based framework for fake news prevention and highlight the various
design issues and consideration of such a blockchain-based framework for
tackling fake news.
</p>
"
http://arxiv.org/abs/1903.11900,Model Vulnerability to Distributional Shifts over Image Transformation Sets. (arXiv:1903.11900v1 [cs.LG]),"<p>We are concerned with the vulnerability of computer vision models to
distributional shifts. We cast this problem in terms of combinatorial
optimization, evaluating the regions in the input space where a (black-box)
model is more vulnerable. This is carried out by combining image
transformations from a given set and standard search algorithms. We embed this
idea in a training procedure, where we define new data augmentation rules over
iterations, accordingly to the image transformations that the current model is
most vulnerable to. An empirical evaluation on classification and semantic
segmentation problems suggests that the devised algorithm allows to train
models more robust against content-preserving image transformations, and in
general, against distributional shifts.
</p>
"
http://arxiv.org/abs/1903.11907,Meta-Learning surrogate models for sequential decision making. (arXiv:1903.11907v1 [stat.ML]),"<p>Meta-learning methods leverage past experience to learn data-driven inductive
biases from related problems, increasing learning efficiency on new tasks. This
ability renders them particularly suitable for sequential decision making with
limited experience. Within this problem family, we argue for the use of such
approaches in the study of model-based approaches to Bayesian Optimisation,
contextual bandits and Reinforcement Learning. We approach the problem by
learning distributions over functions using Neural Processes (NPs), a recently
introduced probabilistic meta-learning method. This allows the treatment of
model uncertainty to tackle the exploration/exploitation dilemma. We show that
NPs are suitable for sequential decision making on a diverse set of domains,
including adversarial task search, recommender systems and model-based
reinforcement learning.
</p>
"
http://arxiv.org/abs/1903.11916,Intelligent Processing in Vehicular Ad hoc Networks: a Survey. (arXiv:1903.11916v1 [cs.NI]),"<p>The intelligent Processing technique is more and more attractive to
researchers due to its ability to deal with key problems in Vehicular Ad hoc
networks. However, several problems in applying intelligent processing
technologies in VANETs remain open. The existing applications are
comprehensively reviewed and discussed, and classified into different
categories in this paper. Their strategies, advantages/disadvantages, and
performances are elaborated. By generalizing different tactics in various
applications related to different scenarios of VANETs and evaluating their
performances, several promising directions for future research have been
suggested.
</p>
"
http://arxiv.org/abs/1903.11919,Imbalanced Sentiment Classification Enhanced with Discourse Marker. (arXiv:1903.11919v1 [cs.CL]),"<p>Imbalanced data commonly exists in real world, espacially in
sentiment-related corpus, making it difficult to train a classifier to
distinguish latent sentiment in text data. We observe that humans often express
transitional emotion between two adjacent discourses with discourse markers
like ""but"", ""though"", ""while"", etc, and the head discourse and the tail
discourse 3 usually indicate opposite emotional tendencies. Based on this
observation, we propose a novel plug-and-play method, which first samples
discourses according to transitional discourse markers and then validates
sentimental polarities with the help of a pretrained attention-based model. Our
method increases sample diversity in the first place, can serve as a upstream
preprocessing part in data augmentation. We conduct experiments on three public
sentiment datasets, with several frequently used algorithms. Results show that
our method is found to be consistently effective, even in highly imbalanced
scenario, and easily be integrated with oversampling method to boost the
performance on imbalanced sentiment classification.
</p>
"
http://arxiv.org/abs/1903.11936,Evolving Boolean Functions with Conjunctions and Disjunctions via Genetic Programming. (arXiv:1903.11936v1 [cs.NE]),"<p>Recently it has been proved that simple GP systems can efficiently evolve the
conjunction of $n$ variables if they are equipped with the minimal required
components. In this paper, we make a considerable step forward by analysing the
behaviour and performance of the GP system for evolving a Boolean function with
unknown components, i.e., the function may consist of both conjunctions and
disjunctions. We rigorously prove that if the target function is the
conjunction of $n$ variables, then the RLS-GP using the complete truth table to
evaluate program quality evolves the exact target function in $O(\ell n \log^2
n)$ iterations in expectation, where $\ell \geq n$ is a limit on the size of
any accepted tree. When, as in realistic applications, only a polynomial sample
of possible inputs is used to evaluate solution quality, we show how RLS-GP can
evolve a conjunction with any polynomially small generalisation error with
probability $1 - O(\log^2(n)/n)$. To produce our results we introduce a
super-multiplicative drift theorem that gives significantly stronger runtime
bounds when the expected progress is only slightly super-linear in the distance
from the optimum.
</p>
"
http://arxiv.org/abs/1903.11941,Application of Deep Learning Long Short-Term Memory in Energy Demand Forecasting. (arXiv:1903.11941v1 [cs.LG]),"<p>The smart metering infrastructure has changed how electricity is measured in
both residential and industrial application. The large amount of data collected
by smart meter per day provides a huge potential for analytics to support the
operation of a smart grid, an example of which is energy demand forecasting.
Short term energy forecasting can be used by utilities to assess if any
forecasted peak energy demand would have an adverse effect on the power system
transmission and distribution infrastructure. It can also help in load
scheduling and demand side management. Many techniques have been proposed to
forecast time series including Support Vector Machine, Artificial Neural
Network and Deep Learning. In this work we use Long Short Term Memory
architecture to forecast 3-day ahead energy demand across each month in the
year. The results show that 3-day ahead demand can be accurately forecasted
with a Mean Absolute Percentage Error of 3.15%. In addition to that, the paper
proposes way to quantify the time as a feature to be used in the training phase
which is shown to affect the network performance.
</p>
"
http://arxiv.org/abs/1903.11944,Taxonomies in DUI Design Patterns: A Systematic Approach for Removing Overlaps Among Design Patterns and Creating a Clear Hierarchy. (arXiv:1903.11944v1 [cs.HC]),"<p>Recently a library of design patterns for designing distributed user
interfaces (DUIs) was created to help researchers and designers to create user
interfaces and to provide an overview of solutions to common DUIs design
problems without requiring a significant amount of time to be spent on reading
domain-specific literature and exploring existing DUIs implementations. The
current version of the DUI design patterns library need to be assessed because
a lot of design patterns are overlapping each other and their relationships are
not clear enough to effectively find the most relevant design pattern for
solving particular design problem, so the purpose of this thesis is to mature
the DUI design patterns knowledge field by removing the duplicate design
patterns, their description and to create a taxonomy where each design pattern
should be organised in a way that will reduce redundancy, possibly leading to
grouping or eventually merging similar patterns and allow to navigate to
related patterns. To achieve the defined goals, the first target was to
investigate the possible overlaps among design patterns and their relevancy
with each other, in order to get these insights natural language processing
tool was built for extracting and analysing each design pattern research paper
to find potential codes. Later in this study thematic analysis was done with
domain experts to get themes, their description and higher level categories
from generated codes to organize all related design patterns in a clear
hierarchy. The outcomes of this thesis included the clarification of the
relationships among design patterns by creating a taxonomy, clarified the
description of individual design pattern, overlaps and duplicate design
patterns were removed and merged similar design patterns.
</p>
"
http://arxiv.org/abs/1903.11960,Learning Discrete Structures for Graph Neural Networks. (arXiv:1903.11960v1 [cs.LG]),"<p>Graph neural networks (GNNs) are a popular class of machine learning models
whose major advantage is their ability to incorporate a sparse and discrete
dependency structure between data points. Unfortunately, GNNs can only be used
when such a graph-structure is available. In practice, however, real-world
graphs are often noisy and incomplete or might not be available at all. With
this work, we propose to jointly learn the graph structure and the parameters
of graph convolutional networks (GCNs) by approximately solving a bilevel
program that learns a discrete probability distribution on the edges of the
graph. This allows one to apply GCNs not only in scenarios where the given
graph is incomplete or corrupted but also in those where a graph is not
available. We conduct a series of experiments that analyze the behavior of the
proposed method and demonstrate that it outperforms related methods by a
significant margin.
</p>
"
http://arxiv.org/abs/1903.11968,On the stability of periodic binary sequences with zone restriction. (arXiv:1903.11968v1 [cs.IT]),"<p>Traditional global stability measure for sequences is hard to determine
because of large search space. We propose the $k$-error linear complexity with
a zone restriction for measuring the local stability of sequences. Accordingly,
we can efficiently determine the global stability by studying a local stability
for these sequences. For several classes of sequences, we demonstrate that the
$k$-error linear complexity is identical to the $k$-error linear complexity
within a zone, while the length of a zone is much smaller than the whole period
when the $k$-error linear complexity is large. These sequences have periods
$2^n$, or $2^v r$ ($r$ odd prime and $2$ is primitive modulo $r$), or $2^v
p_1^{s_1} \cdots p_n^{s_n}$ ($p_i$ is an odd prime and $2$ is primitive modulo
$p_i$ and $p_i^2$, where $1\leq i \leq n$) respectively. In particular, we
completely determine the spectrum of $1$-error linear complexity with any zone
length for an arbitrary $2^n$-periodic binary sequence.
</p>
"
http://arxiv.org/abs/1903.11971,The Global Convergence Analysis of the Bat Algorithm Using a Markovian Framework and Dynamical System Theory. (arXiv:1903.11971v1 [math.OC]),"<p>The bat algorithm (BA) has been shown to be effective to solve a wider range
of optimization problems. However, there is not much theoretical analysis
concerning its convergence and stability. In order to prove the convergence of
the bat algorithm, we have built a Markov model for the algorithm and proved
that the state sequence of the bat population forms a finite homogeneous Markov
chain, satisfying the global convergence criteria. Then, we prove that the bat
algorithm can have global convergence. In addition, in order to enhance the
convergence performance of the algorithm, we have designed an updated model
using the dynamical system theory in terms of a dynamic matrix, and the
parameter ranges for the algorithm stability are then obtained. We then use
some benchmark functions to demonstrate that BA can indeed achieve global
optimality efficiently for these functions.
</p>
"
http://arxiv.org/abs/1903.11980,Probabilistic Analysis of Facility Location on Random Shortest Path Metrics. (arXiv:1903.11980v1 [cs.DS]),"<p>The facility location problem is an NP-hard optimization problem. Therefore,
approximation algorithms are often used to solve large instances. Such
algorithms often perform much better than worst-case analysis suggests.
Therefore, probabilistic analysis is a widely used tool to analyze such
algorithms. Most research on probabilistic analysis of NP-hard optimization
problems involving metric spaces, such as the facility location problem, has
been focused on Euclidean instances, and also instances with independent
(random) edge lengths, which are non-metric, have been researched. We would
like to extend this knowledge to other, more general, metrics.
</p>
<p>We investigate the facility location problem using random shortest path
metrics. We analyze some probabilistic properties for a simple greedy heuristic
which gives a solution to the facility location problem: opening the $\kappa$
cheapest facilities (with $\kappa$ only depending on the facility opening
costs). If the facility opening costs are such that $\kappa$ is not too large,
then we show that this heuristic is asymptotically optimal. On the other hand,
for large values of $\kappa$, the analysis becomes more difficult, and we
provide a closed-form expression as upper bound for the expected approximation
ratio. In the special case where all facility opening costs are equal this
closed-form expression reduces to $O(\sqrt[4]{\ln(n)})$ or $O(1)$ or even
$1+o(1)$ if the opening costs are sufficiently small.
</p>
"
http://arxiv.org/abs/1903.11981,Regularizing Trajectory Optimization with Denoising Autoencoders. (arXiv:1903.11981v1 [cs.LG]),"<p>Trajectory optimization with learned dynamics models can often suffer from
erroneous predictions of out-of-distribution trajectories. We propose to
regularize trajectory optimization by means of a denoising autoencoder that is
trained on the same trajectories as the dynamics model. We visually demonstrate
the effectiveness of the regularization in gradient-based trajectory
optimization for open-loop control of an industrial process. We compare with
recent model-based reinforcement learning algorithms on a set of popular motor
control tasks to demonstrate that the denoising regularization enables
state-of-the-art sample-efficiency. We demonstrate the efficacy of the proposed
method in regularizing both gradient-based and gradient-free trajectory
optimization.
</p>
"
http://arxiv.org/abs/1903.11983,Sentiment Analysis on IMDB Movie Comments and Twitter Data by Machine Learning and Vector Space Techniques. (arXiv:1903.11983v1 [cs.IR]),"<p>This study's goal is to create a model of sentiment analysis on a 2000 rows
IMDB movie comments and 3200 Twitter data by using machine learning and vector
space techniques; positive or negative preliminary information about the text
is to provide. In the study, a vector space was created in the KNIME Analytics
platform, and a classification study was performed on this vector space by
Decision Trees, Na\""ive Bayes and Support Vector Machines classification
algorithms. The conclusions obtained were compared in terms of each algorithms.
The classification results for IMDB movie comments are obtained as 94,00%,
73,20%, and 85,50% by Decision Tree, Naive Bayes and SVM algorithms. The
classification results for Twitter data set are presented as 82,76%, 75,44% and
72,50% by Decision Tree, Naive Bayes SVM algorithms as well. It is seen that
the best classification results presented in both data sets are which
calculated by SVM algorithm.
</p>
"
http://arxiv.org/abs/1903.11987,Universal chosen-ciphertext attack for a family of image encryption schemes. (arXiv:1903.11987v1 [cs.MM]),"<p>During the past decades, there is a great popularity employing nonlinear
dynamics and permutation-substitution architecture for image encryption. There
are three primary procedures in such encryption schemes, the key schedule
module for producing encryption factors, permutation for image scrambling and
substitution for pixel modification. Under the assumption of chosen-ciphertext
attack, we evaluate the security of a class of image ciphers which adopts
pixel-level permutation and modular addition for substitution. It is
mathematically revealed that the mapping from differentials of ciphertexts to
those of plaintexts are linear and has nothing to do with the key schedules,
permutation techniques and encryption rounds. Moreover, a universal
chosen-ciphertext attack is proposed and validated. Experimental results
demonstrate that the plaintexts can be directly reconstructed without any
security key or encryption elements. Related cryptographic discussions are also
given.
</p>
"
http://arxiv.org/abs/1903.11990,On the Stability and Generalization of Learning with Kernel Activation Functions. (arXiv:1903.11990v1 [stat.ML]),"<p>In this brief we investigate the generalization properties of a
recently-proposed class of non-parametric activation functions, the kernel
activation functions (KAFs). KAFs introduce additional parameters in the
learning process in order to adapt nonlinearities individually on a per-neuron
basis, exploiting a cheap kernel expansion of every activation value. While
this increase in flexibility has been shown to provide significant improvements
in practice, a theoretical proof for its generalization capability has not been
addressed yet in the literature. Here, we leverage recent literature on the
stability properties of non-convex models trained via stochastic gradient
descent (SGD). By indirectly proving two key smoothness properties of the
models under consideration, we prove that neural networks endowed with KAFs
generalize well when trained with SGD for a finite number of steps.
Interestingly, our analysis provides a guideline for selecting one of the
hyper-parameters of the model, the bandwidth of the scalar Gaussian kernel. A
short experimental evaluation validates the proof.
</p>
"
http://arxiv.org/abs/1903.11991,PAL: A fast DNN optimization method based on curvature information. (arXiv:1903.11991v1 [cs.LG]),"<p>We present a novel optimizer for deep neural networks that combines the ideas
of Netwon's method and line search to efficiently compute and utilize curvature
information. Our work is based on empirical observation suggesting that the
loss function can be approximated by a parabola in negative gradient direction.
Due to this approximation, we are able to perform a variable and loss function
dependent parameter update by jumping directly into the minimum of the
approximated parabola. To evaluate our optimizer, we performed multiple
comprehensive hyperparameter grid searches for which we trained more than 20000
networks in total. We can show that PAL outperforms RMSPROP, and can outperform
gradient descent with momentum and ADAM on large-scale high-dimensional machine
learning problems. Furthermore, PAL requires up to 52.2% less training epochs.
PyTorch and TensorFlow implementations are provided at
https://github.com/cogsys-tuebingen/PAL.
</p>
"
http://arxiv.org/abs/1903.11993,Fault and Performance Management in Multi-Cloud Based NFV using Shallow and Deep Predictive Structures. (arXiv:1903.11993v1 [cs.NI]),"<p>Deployment of Network Function Virtualization (NFV) over multiple clouds
accentuates its advantages like the flexibility of virtualization, proximity to
customers and lower total cost of operation. However, NFV over multiple clouds
has not yet attained the level of performance to be a viable replacement for
traditional networks. One of the reasons is the absence of a standard based
Fault, Configuration, Accounting, Performance and Security (FCAPS) framework
for the virtual network services. In NFV, faults and performance issues can
have complex geneses within virtual resources as well as virtual networks and
cannot be effectively handled by traditional rule-based systems. To tackle the
above problem, we propose a fault detection and localization model based on a
combination of shallow and deep learning structures. Relatively simpler
detection of 'fault' and 'no-fault' conditions or 'manifest' and 'impending'
faults have been effectively shown to be handled by shallow machine learning
structures like Support Vector Machine (SVM). Deeper structure, i.e. the
stacked autoencoder has been found to be useful for a more complex localization
function where a large amount of information needs to be worked through, in
different layers, to get to the root cause of the problem. We provide
evaluation results using a dataset adapted from logs of disruption in an
operator's live network fault datasets available on Kaggle and another based on
multivariate kernel density estimation and Markov sampling.
</p>
"
http://arxiv.org/abs/1903.11994,Efficient Virtual Network Function Placement Strategies for Cloud Radio Access Networks. (arXiv:1903.11994v1 [cs.NI]),"<p>The new generation of 5G mobile services places stringent requirements for
cellular network operators in terms of latency and costs. The latest trend in
radio access networks (RANs) is to pool the baseband units (BBUs) of multiple
radio base stations and to install them in a centralized infrastructure, such
as a cloud, for statistical multiplexing gains. The technology is known as
Cloud Radio Access Network (CRAN). Since cloud computing is gaining significant
traction and virtualized data centers are becoming popular as a cost-effective
infrastructure in the telecommunication industry, CRAN is being heralded as a
candidate technology to meet the expectations of radio access networks for 5G.
In CRANs, low energy base stations (BSs) are deployed over a small geographical
location and are connected to a cloud via finite capacity backhaul links.
Baseband processing unit (BBU) functions are implemented on the virtual
machines (VMs) in the cloud over commodity hardware. Such functions, built-in
software, are termed as virtual functions (VFs). The optimized placement of VFs
is necessary to reduce the total delays and minimize the overall costs to
operate CRANs. Our study considers the problem of optimal VF placement over
distributed virtual resources spread across multiple clouds, creating a
centralized BBU cloud. We propose a combinatorial optimization model and the
use of two heuristic approaches, which are, branch-and-bound (BnB) and
simulated annealing (SA) for the proposed optimal placement. In addition, we
propose enhancements to the standard BnB heuristic and compare the results with
standard BnB and SA approaches. The proposed enhancements improve the quality
of the solution in terms of latency and cost as well as reduce the execution
complexity significantly.
</p>
"
http://arxiv.org/abs/1903.11996,To boldly go where no sensor has gone before: The movement to place IoT in radical new spaces. (arXiv:1903.11996v1 [eess.SP]),"<p>In this article, we have presented an overview on a unified framework which
is called as the Internet of X-things (X-IoT) that will warrant the convergence
of emerging use cases of the Internet of Things everywhere around us, i.e.,
under the ground and oceans and even in the outer space. It is anticipated that
such a framework will foster the design and development of smart objects
capable of performing sensing under all-rounded environment and communication
technologies capable of offering ubiquitous connectivity of course with the
desired requirements. Through this framework, we get to know what has been done
since recently and how the technical challenges across the broad spectrum of
emerging use cases under the water, underground and over the space are
converging toward future solutions.
</p>
"
http://arxiv.org/abs/1903.11997,A gradual approach for maximising user conversion without compromising experience with high visual intensity website elements. (arXiv:1903.11997v1 [cs.HC]),"<p>The study develops and tests a method that can gradually find a sweet spot
between user experience and visual intensity of website elements to maximise
user conversion with minimal adverse effect. In the first phase of the study,
we develop the method. In the second stage, we test and evaluate the method via
an empirical study; also, an experiment was conducted within web interface with
the gradual intensity of visual elements.The findings reveal that negative
response grows faster than conversion when the visual intensity of the web
interface is increased. However, a saturation point, where there is coexistence
between maximum conversion and minimum negative response, can be found. The
findings imply that efforts to attract user attention should be pursued with
increased caution and that a gradual approach presented in this study helps in
finding a site-specific sweet-spot for a level of visual intensity by
incrementally adjusting the elements of the interface and tracking the changes
in user behaviour. Web marketing and advertising professionals often face the
dilemma of determining the optimal level of visual intensity of interface
element. Excessive use of marketing component and attention-grabbing visual
elements can lead to an inferior user experience and consequent user churn due
to growing intrusiveness. At the same time, too little visual intensity can
fail to steer users. The present study provides a gradual approach which aids
in finding a balance between user experience and visual intensity, maximising
user conversion and thus providing a practical solution for the problem.
</p>
"
http://arxiv.org/abs/1903.12001,Experimental Implementation of a New Non-redundant 6-DOF Quadrotor Manipulation System. (arXiv:1903.12001v1 [cs.RO]),"<p>This paper presents an experimental validation of a new quadrotor-based
aerial manipulator. A quadrotor is equipped with a 2-DOF robotic arm that is
designed with a new topology to enable the end-effector of the whole system to
track a 6-DOF trajectory. An identification experiment is carried out to find
out the system parameters. The mathematical model of the whole system is
presented. A measurement scheme is proposed to get the accurate pose of the
vehicle considering the motion of the manipulator below the quadrotor. The
system controller is designed and implemented based on PID with a gravity
compensation algorithm. System simulation is implemented in the MATLAB/SIMULINK
environment with real system parameters, to better emulate a realistic setup.
Real-time Experiments are conducted. Both simulation and experimental results
show the feasibility and a satisfactory efficiency of the proposed system in
achieving the position holding and transferring an object to a specific target
position.
</p>
"
http://arxiv.org/abs/1903.12003,High Fidelity Face Manipulation with Extreme Pose and Expression. (arXiv:1903.12003v1 [cs.CV]),"<p>Face manipulation has shown remarkable advances with the flourish of
Generative Adversarial Networks. However, due to the difficulties of
controlling the structure and texture in high-resolution, it is challenging to
simultaneously model pose and expression during manipulation. In this paper, we
propose a novel framework that simplifies face manipulation with extreme pose
and expression into two correlated stages: a boundary prediction stage and a
disentangled face synthesis stage. In the first stage, we propose to use a
boundary image for joint pose and expression modeling. An encoder-decoder
network is employed to predict the boundary image of the target face in a
semi-supervised way. Pose and expression estimators are used to improve the
prediction accuracy. In the second stage, the predicted boundary image and the
original face are encoded into the structure and texture latent space by two
encoder networks respectively. A proxy network and a feature threshold loss are
further imposed as constraints to disentangle the latent space. In addition, we
build up a new high quality Multi-View Face (MVF-HQ) database that contains
120K high-resolution face images of 479 identities with pose and expression
variations, which will be released soon. Qualitative and quantitative
experiments on four databases show that our method pushes forward the advance
of extreme face manipulation from 128 $\times$ 128 resolution to 1024 $\times$
1024 resolution, and significantly improves the face recognition performance
under large poses.
</p>
"
http://arxiv.org/abs/1903.12008,Handling Noisy Labels for Robustly Learning from Self-Training Data for Low-Resource Sequence Labeling. (arXiv:1903.12008v1 [cs.CL]),"<p>In this paper, we address the problem of effectively self-training neural
networks in a low-resource setting. Self-training is frequently used to
automatically increase the amount of training data. However, in a low-resource
scenario, it is less effective due to unreliable annotations created using
self-labeling of unlabeled data. We propose to combine self-training with noise
handling on the self-labeled data. Directly estimating noise on the combined
clean training set and self-labeled data can lead to corruption of the clean
data and hence, performs worse. Thus, we propose the Clean and Noisy Label
Neural Network which trains on clean and noisy self-labeled data simultaneously
by explicitly modelling clean and noisy labels separately. In our experiments
on Chunking and NER, this approach performs more robustly than the baselines.
Complementary to this explicit approach, noise can also be handled implicitly
with the help of an auxiliary learning task. To such a complementary approach,
our method is more beneficial than other baseline methods and together provides
the best performance overall.
</p>
"
http://arxiv.org/abs/1903.12011,Novel Artificial Human Optimization Field Algorithms - The Beginning. (arXiv:1903.12011v1 [cs.NE]),"<p>New Artificial Human Optimization (AHO) Field Algorithms can be created from
scratch or by adding the concept of Artificial Humans into other existing
Optimization Algorithms. Particle Swarm Optimization (PSO) has been very
popular for solving complex optimization problems due to its simplicity. In
this work, new Artificial Human Optimization Field Algorithms are created by
modifying existing PSO algorithms with AHO Field Concepts. These Hybrid PSO
Algorithms comes under PSO Field as well as AHO Field. There are Hybrid PSO
research articles based on Human Behavior, Human Cognition and Human Thinking
etc. But there are no Hybrid PSO articles which based on concepts like Human
Disease, Human Kindness and Human Relaxation. This paper proposes new AHO Field
algorithms based on these research gaps. Some existing Hybrid PSO algorithms
are given a new name in this work so that it will be easy for future AHO
researchers to find these novel Artificial Human Optimization Field Algorithms.
A total of 6 Artificial Human Optimization Field algorithms titled ""Human
Safety Particle Swarm Optimization (HuSaPSO)"", ""Human Kindness Particle Swarm
Optimization (HKPSO)"", ""Human Relaxation Particle Swarm Optimization (HRPSO)"",
""Multiple Strategy Human Particle Swarm Optimization (MSHPSO)"", ""Human Thinking
Particle Swarm Optimization (HTPSO)"" and ""Human Disease Particle Swarm
Optimization (HDPSO)"" are tested by applying these novel algorithms on Ackley,
Beale, Bohachevsky, Booth and Three-Hump Camel Benchmark Functions. Results
obtained are compared with PSO algorithm.
</p>
"
http://arxiv.org/abs/1903.12012,Forecasting model based on information-granulated GA-SVR and ARIMA for producer price index. (arXiv:1903.12012v1 [stat.AP]),"<p>The accuracy of predicting the Producer Price Index (PPI) plays an
indispensable role in government economic work. However, it is difficult to
forecast the PPI. In our research, we first propose an unprecedented hybrid
model based on fuzzy information granulation that integrates the GA-SVR and
ARIMA (Autoregressive Integrated Moving Average Model) models. The
fuzzy-information-granulation-based GA-SVR-ARIMA hybrid model is intended to
deal with the problem of imprecision in PPI estimation. The proposed model
adopts the fuzzy information-granulation algorithm to
pre-classification-process monthly training samples of the PPI, and produced
three different sequences of fuzzy information granules, whose Support Vector
Regression (SVR) machine forecast models were separately established for their
Genetic Algorithm (GA) optimization parameters. Finally, the residual errors of
the GA-SVR model were rectified through ARIMA modeling, and the PPI estimate
was reached. Research shows that the PPI value predicted by this hybrid model
is more accurate than that predicted by other models, including ARIMA, GRNN,
and GA-SVR, following several comparative experiments. Research also indicates
the precision and validation of the PPI prediction of the hybrid model and
demonstrates that the model has consistent ability to leverage the forecasting
advantage of GA-SVR in non-linear space and of ARIMA in linear space.
</p>
"
http://arxiv.org/abs/1903.12017,"Train, Sort, Explain: Learning to Diagnose Translation Models. (arXiv:1903.12017v1 [cs.CL])","<p>Evaluating translation models is a trade-off between effort and detail. On
the one end of the spectrum there are automatic count-based methods such as
BLEU, on the other end linguistic evaluations by humans, which arguably are
more informative but also require a disproportionately high effort. To narrow
the spectrum, we propose a general approach on how to automatically expose
systematic differences between human and machine translations to human experts.
Inspired by adversarial settings, we train a neural text classifier to
distinguish human from machine translations. A classifier that performs and
generalizes well after training should recognize systematic differences between
the two classes, which we uncover with neural explainability methods. Our
proof-of-concept implementation, DiaMaT, is open source. Applied to a dataset
translated by a state-of-the-art neural Transformer model, DiaMaT achieves a
classification accuracy of 75% and exposes meaningful differences between
humans and the Transformer, amidst the current discussion about human parity.
</p>
"
http://arxiv.org/abs/1903.12018,Team Optimal Decentralized State Estimation. (arXiv:1903.12018v1 [cs.SY]),"<p>We consider the problem of optimal decentralized estimation of a linear
stochastic process by multiple agents. Each agent receives a noisy observation
of the state of the process and delayed observations of its neighbors
(according to a pre-specified, strongly connected, communication graph). Based
on their observations, all agents generate a sequence of estimates of the state
of the process. The objective is to minimize the total expected weighted mean
square error between the state and the agents' estimates over a finite horizon.
In centralized estimation with weighted mean square error criteria, the optimal
estimator does not depend on the weight matrix in the cost function. We show
that this is not the case when the information is decentralized. The optimal
decentralized estimates depend on the weight matrix in the cost function. In
particular, we show that the optimal estimate consists of two parts: a common
estimate which is the conditional mean of the state given the common
information and a correction term which is a linear function of the offset of
the local information from the conditional expectation of the local information
given the common information. The corresponding gain depends on the weight
matrix as well as on the covariance between the offset of agents' local
information from the conditional mean of the local information given the common
information. We show that the local and common estimates can be computed from a
single Kalman filter and derive recursive expressions for computing the offset
covariances and the estimation gains.
</p>
"
http://arxiv.org/abs/1903.12019,Multimodal Deep Network Embedding with Integrated Structure and Attribute Information. (arXiv:1903.12019v1 [cs.LG]),"<p>Network embedding is the process of learning low-dimensional representations
for nodes in a network, while preserving node features. Existing studies only
leverage network structure information and focus on preserving structural
features. However, nodes in real-world networks often have a rich set of
attributes providing extra semantic information. It has been demonstrated that
both structural and attribute features are important for network analysis
tasks. To preserve both features, we investigate the problem of integrating
structure and attribute information to perform network embedding and propose a
Multimodal Deep Network Embedding (MDNE) method. MDNE captures the non-linear
network structures and the complex interactions among structures and
attributes, using a deep model consisting of multiple layers of non-linear
functions. Since structures and attributes are two different types of
information, a multimodal learning method is adopted to pre-process them and
help the model to better capture the correlations between node structure and
attribute information. We employ both structural proximity and attribute
proximity in the loss function to preserve the respective features and the
representations are obtained by minimizing the loss function. Results of
extensive experiments on four real-world datasets show that the proposed method
performs significantly better than baselines on a variety of tasks, which
demonstrate the effectiveness and generality of our method.
</p>
"
http://arxiv.org/abs/1903.12020,Describing like humans: on diversity in image captioning. (arXiv:1903.12020v1 [cs.CV]),"<p>Recently, the state-of-the-art models for image captioning have overtaken
human performance based on the most popular metrics, such as BLEU, METEOR,
ROUGE, and CIDEr. Does this mean we have solved the task of image captioning?
The above metrics only measure the similarity of the generated caption to the
human annotations, which reflects its accuracy. However, an image contains many
concepts and multiple levels of detail, and thus there is a variety of captions
that express different concepts and details that might be interesting for
different humans. Therefore only evaluating accuracy is not sufficient for
measuring the performance of captioning models --- the diversity of the
generated captions should also be considered. In this paper, we proposed a new
metric for measuring the diversity of image captions, which is derived from
latent semantic analysis and kernelized to use CIDEr similarity. We conduct
extensive experiments to re-evaluate recent captioning models in the context of
both diversity and accuracy. We find that there is still a large gap between
the model and human performance in terms of both accuracy and diversity and the
models that have optimized accuracy (CIDEr) have low diversity. We also show
that balancing the cross-entropy loss and CIDEr reward in reinforcement
learning during training can effectively control the tradeoff between diversity
and accuracy of the generated captions.
</p>
"
http://arxiv.org/abs/1903.12021,Counting the learnable functions of structured data. (arXiv:1903.12021v1 [cond-mat.dis-nn]),"<p>Cover's function counting theorem is a milestone in the theory of artificial
neural networks. It provides an answer to the fundamental question of
determining how many binary assignments (dichotomies) of $p$ points in $n$
dimensions can be linearly realized. Regrettably, it has proved hard to extend
the same approach to more advanced problems than the classification of points.
In particular, an emerging necessity is to find methods to deal with structured
data, and specifically with non-pointlike patterns. A prominent case is that of
invariant recognition, whereby identification of a stimulus is insensitive to
irrelevant transformations on the inputs (such as rotations or changes in
perspective in an image). An object is therefore represented by an extended
perceptual manifold, consisting of inputs that are classified similarly. Here,
we develop a function counting theory for structured data of this kind, by
extending Cover's combinatorial technique, and we derive analytical expressions
for the average number of dichotomies of generically correlated sets of
patterns. As an application, we obtain a closed formula for the capacity of a
binary classifier trained to distinguish general polytopes of any dimension.
These results may help extend our theoretical understanding of generalization,
feature extraction, and invariant object recognition by neural networks.
</p>
"
http://arxiv.org/abs/1903.12033,Repeatable and Reproducible Wireless Networking Experimentation through Trace-based Simulation. (arXiv:1903.12033v1 [cs.NI]),"<p>To properly validate wireless networking solutions we depend on
experimentation. Simulation very often produces less accurate results due to
the use of models that are simplifications of the real phenomena they try to
model. Networking experimentation may offer limited repeatability and
reproducibility. Being influenced by external random phenomena such as noise,
interference, and multipath, real experiments are hardly repeatable. In
addition, they are difficult to reproduce due to testbed operational
constraints and availability. Without repeatability and reproducibility, the
validation of the networking solution under evaluation is questionable. In this
paper, we show how the Trace-based Simulation (TS) approach can be used to
accurately repeat and reproduce real experiments and, consequently, introduce a
paradigm shift when it comes to the evaluation of wireless networking
solutions. We present an extensive evaluation of the TS approach using the
Fed4FIRE+ w-iLab.2 testbed. The results show that it is possible to repeat and
reproduce real experiments using ns-3 trace-based simulations with more
accuracy than in pure simulation, with average accuracy gains above 50%.
</p>
"
http://arxiv.org/abs/1903.12041,The Geography of Pok\'emon GO: Beneficial and Problematic Effects on Places and Movement. (arXiv:1903.12041v1 [cs.HC]),"<p>The widespread popularity of Pok\'emon GO presents the first opportunity to
observe the geographic effects of location-based gaming at scale. This paper
reports the results of a mixed methods study of the geography of Pok\'emon GO
that includes a five-country field survey of 375 Pok\'emon GO players and a
large scale geostatistical analysis of game elements. Focusing on the key
geographic themes of places and movement, we find that the design of Pok\'emon
GO reinforces existing geographically-linked biases (e.g. the game advantages
urban areas and neighborhoods with smaller minority populations), that
Pok\'emon GO may have instigated a relatively rare large-scale shift in global
human mobility patterns, and that Pok\'emon GO has geographically-linked safety
risks, but not those typically emphasized by the media. Our results point to
geographic design implications for future systems in this space such as a means
through which the geographic biases present in Pok\'emon GO may be
counteracted.
</p>
"
http://arxiv.org/abs/1903.12049,Road User Detection in Videos. (arXiv:1903.12049v1 [cs.CV]),"<p>Successive frames of a video are highly redundant, and the most popular
object detection methods do not take advantage of this fact. Using multiple
consecutive frames can improve detection of small objects or difficult examples
and can improve speed and detection consistency in a video sequence, for
instance by interpolating features between frames. In this work, a novel
approach is introduced to perform online video object detection using two
consecutive frames of video sequences involving road users. Two new models,
RetinaNet-Double and RetinaNet-Flow, are proposed, based respectively on the
concatenation of a target frame with a preceding frame, and the concatenation
of the optical flow with the target frame. The models are trained and evaluated
on three public datasets. Experiments show that using a preceding frame
improves performance over single frame detectors, but using explicit optical
flow usually does not.
</p>
"
http://arxiv.org/abs/1903.12050,Finding a planted clique by adaptive probing. (arXiv:1903.12050v1 [math.CO]),"<p>We consider a variant of the planted clique problem where we are allowed
unbounded computational time but can only investigate a small part of the graph
by adaptive edge queries. We determine (up to logarithmic factors) the number
of queries necessary both for detecting the presence of a planted clique and
for finding the planted clique.
</p>
<p>Specifically, let $G \sim G(n,1/2,k)$ be a random graph on $n$ vertices with
a planted clique of size $k$. We show that no algorithm that makes at most $q =
o(n^2 / k^2 + n)$ adaptive queries to the adjacency matrix of $G$ is likely to
find the planted clique. On the other hand, when $k \geq (2+\epsilon) \log_2 n$
there exists a simple algorithm (with unbounded computational power) that finds
the planted clique with high probability by making $q = O( (n^2 / k^2) \log^2 n
+ n \log n)$ adaptive queries. For detection, the additive $n$ term is not
necessary: the number of queries needed to detect the presence of a planted
clique is $n^2 / k^2$ (up to logarithmic factors).
</p>
"
http://arxiv.org/abs/1903.12058,Deep Neural Network Embedding Learning with High-Order Statistics for Text-Independent Speaker Verification. (arXiv:1903.12058v1 [eess.AS]),"<p>The x-vector based deep neural network (DNN) embedding systems have
demonstrated effectiveness for text-independent speaker verification. This
paper presents a multi-task learning architecture for training the speaker
embedding DNN, with the primary task of classifying the target speakers and the
auxiliary task of reconstructing the higher-order statistics of the original
input utterance. The proposed training strategy aggregates both the supervised
and unsupervised learning into one framework to make the speaker embeddings
more discriminative and robust. Experiments are carried out in the NIST SRE16
evaluation dataset and the VOiCES dataset. The results demonstrate that our
proposed method outperform the original x-vector approach with very low
additional complexity added.
</p>
"
http://arxiv.org/abs/1903.12060,Penobscot Dataset: Fostering Machine Learning Development for Seismic Interpretation. (arXiv:1903.12060v1 [physics.geo-ph]),"<p>We have seen in the past years the flourishing of machine and deep learning
algorithms in several applications such as image classification and
segmentation, object detection and recognition, among many others. This was
only possible, in part, because datasets like ImageNet -- with +14 million
labeled images -- were created and made publicly available, providing
researches with a common ground to compare their advances and extend the
state-of-the-art. Although we have seen an increasing interest in machine
learning in geosciences as well, we will only be able to achieve a significant
impact in our community if we collaborate to build such a common basis. This is
even more difficult when it comes to the Oil&amp;Gas industry, in which
confidentiality and commercial interests often hinder the sharing of datasets
with others. In this letter, we present the Penobscot interpretation dataset,
our contribution to the development of machine learning in geosciences, more
specifically in seismic interpretation. The Penobscot 3D seismic dataset was
acquired in the Scotian shelf, offshore Nova Scotia, Canada. The data is
publicly available and comprises pre- and pos-stack data, 5 horizons and well
logs of 2 wells. However, for the dataset to be of practical use for our tasks,
we had to reinterpret the seismic, generating 7 horizons separating different
seismic facies intervals. The interpreted horizons were used to generated
+100,000 labeled images for inlines and crosslines. To demonstrate the utility
of our dataset, results of two experiments are presented.
</p>
"
http://arxiv.org/abs/1903.12061,Depth from a polarisation + RGB stereo pair. (arXiv:1903.12061v1 [cs.CV]),"<p>In this paper, we propose a hybrid depth imaging system in which a
polarisation camera is augmented by a second image from a standard digital
camera. For this modest increase in equipment complexity over conventional
shape-from-polarisation, we obtain a number of benefits that enable us to
overcome longstanding problems with the polarisation shape cue. The stereo cue
provides a depth map which, although coarse, is metrically accurate. This is
used as a guide surface for disambiguation of the polarisation surface normal
estimates using a higher order graphical model. In turn, these are used to
estimate diffuse albedo. By extending a previous shape-from-polarisation method
to the perspective case, we show how to compute dense, detailed maps of
absolute depth, while retaining a linear formulation. We show that our hybrid
method is able to recover dense 3D geometry that is superior to
state-of-the-art shape-from-polarisation or two view stereo alone.
</p>
"
http://arxiv.org/abs/1903.12063,"Robust, fast and accurate: a 3-step method for automatic histological image registration. (arXiv:1903.12063v1 [cs.CV])","<p>We present a 3-step registration pipeline for differently stained
histological serial sections that consists of 1) a robust pre-alignment, 2) a
parametric registration computed on coarse resolution images, and 3) an
accurate nonlinear registration. In all three steps the NGF distance measure is
minimized with respect to an increasingly flexible transformation. We apply the
method in the ANHIR image registration challenge and evaluate its performance
on the training data. The presented method is robust (error reduction in 99.6%
of the cases), fast (runtime &lt; 4 seconds) and accurate (median relative target
registration error 0.0019).
</p>
"
http://arxiv.org/abs/1903.12064,Data4UrbanMobility: Towards Holistic Data Analytics for Mobility Applications in Urban Regions. (arXiv:1903.12064v1 [cs.CY]),"<p>With the increasing availability of mobility-related data, such as
GPS-traces, Web queries and climate conditions, there is a growing demand to
utilize this data to better understand and support urban mobility needs.
However, data available from the individual actors, such as providers of
information, navigation and transportation systems, is mostly restricted to
isolated mobility modes, whereas holistic data analytics over integrated data
sources is not sufficiently supported. In this paper we present our ongoing
research in the context of holistic data analytics to support urban mobility
applications in the Data4UrbanMobility (D4UM) project. First, we discuss
challenges in urban mobility analytics and present the D4UM platform we are
currently developing to facilitate holistic urban data analytics over
integrated heterogeneous data sources along with the available data sources.
Second, we present the MiC app - a tool we developed to complement available
datasets with intermodal mobility data (i.e. data about journeys that involve
more than one mode of mobility) using a citizen science approach. Finally, we
present selected use cases and discuss our future work.
</p>
"
http://arxiv.org/abs/1903.12065,Optimal Random Sampling from Distributed Streams Revisited. (arXiv:1903.12065v1 [cs.DC]),"<p>We give an improved algorithm for drawing a random sample from a large data
stream when the input elements are distributed across multiple sites which
communicate via a central coordinator. At any point in time the set of elements
held by the coordinator represent a uniform random sample from the set of all
the elements observed so far. When compared with prior work, our algorithms
asymptotically improve the total number of messages sent in the system as well
as the computation required of the coordinator. We also present a matching
lower bound, showing that our protocol sends the optimal number of messages up
to a constant factor with large probability. As a byproduct, we obtain an
improved algorithm for finding the heavy hitters across multiple distributed
sites.
</p>
"
http://arxiv.org/abs/1903.12069,The Virtual Doctor: An Interactive Artificial Intelligence based on Deep Learning for Non-Invasive Prediction of Diabetes. (arXiv:1903.12069v1 [cs.CY]),"<p>Artificial intelligence (AI) will pave the way to a new era in medicine.
However, currently available AI systems do not interact with a patient, e.g.,
for anamnesis, and thus are only used by the physicians for predictions in
diagnosis or prognosis. However, these systems are widely used, e.g., in
diabetes or cancer prediction. In the current study, we developed an AI that is
able to interact with a patient (virtual doctor) by using a speech recognition
and speech synthesis system and thus can autonomously interact with the
patient, which is particularly important for, e.g., rural areas, where the
availability of primary medical care is strongly limited by low population
densities. As a proof-of-concept, the system is able to predict type 2 diabetes
mellitus (T2DM) based on non-invasive sensors and deep neural networks.
Moreover, the system provides an easy-to-interpret probability estimation for
T2DM for a given patient. Besides the development of the AI, we further
analyzed the acceptance of young people for AI in healthcare to estimate the
impact of such system in the future.
</p>
"
http://arxiv.org/abs/1903.12070,Comprehensive Analysis of Dynamic Message Sign Impact on Driver Behavior: A Random Forest Approach. (arXiv:1903.12070v1 [cs.CY]),"<p>This study investigates the potential effects of different Dynamic Message
Signs (DMSs) on driver behavior using a full-scale high-fidelity driving
simulator. Different DMSs are categorized by their content, structure, and type
of messages. A random forest algorithm is used for three separate behavioral
analyses; a route diversion analysis, a route choice analysis and a compliance
analysis; to identify the potential and relative influences of different DMSs
on these aspects of driver behavior. A total of 390 simulation runs are
conducted using a sample of 65 participants from diverse socioeconomic
backgrounds. Results obtained suggest that DMSs displaying lane closure and
delay information with advisory messages are most influential with regards to
diversion while color-coded DMSs and DMSs with avoid route advice are the top
contributors impacting route choice decisions and DMS compliance. In this
first-of-a-kind study, based on the responses to the pre and post simulation
surveys as well as results obtained from the analysis of
driving-simulation-session data, the authors found that color-blind-friendly,
color-coded DMSs are more effective than alphanumeric DMSs - especially in
scenarios that demand high compliance from drivers. The increased effectiveness
may be attributed to reduced comprehension time and ease with which such DMSs
are understood by a greater percentage of road users.
</p>
"
http://arxiv.org/abs/1903.12071,Big Data Analytics and AI in Mental Healthcare. (arXiv:1903.12071v1 [cs.CY]),"<p>Mental health conditions cause a great deal of distress or impairment;
depression alone will affect 11% of the world's population. The application of
Artificial Intelligence (AI) and big-data technologies to mental health has
great potential for personalizing treatment selection, prognosticating,
monitoring for relapse, detecting and helping to prevent mental health
conditions before they reach clinical-level symptomatology, and even delivering
some treatments. However, unlike similar applications in other fields of
medicine, there are several unique challenges in mental health applications
which currently pose barriers towards the implementation of these technologies.
Specifically, there are very few widely used or validated biomarkers in mental
health, leading to a heavy reliance on patient and clinician derived
questionnaire data as well as interpretation of new signals such as digital
phenotyping. In addition, diagnosis also lacks the same objective 'gold
standard' as in other conditions such as oncology, where clinicians and
researchers can often rely on pathological analysis for confirmation of
diagnosis. In this chapter we discuss the major opportunities, limitations and
techniques used for improving mental healthcare through AI and big-data. We
explore both the computational, clinical and ethical considerations and best
practices as well as lay out the major researcher directions for the near
future.
</p>
"
http://arxiv.org/abs/1903.12073,Scope of Research on Particle Swarm Optimization Based Data Clustering. (arXiv:1903.12073v1 [cs.NE]),"<p>Optimization is nothing but a mathematical technique which finds maxima or
minima of any function of concern in some realistic region. Different
optimization techniques are proposed which are competing for the best solution.
Particle Swarm Optimization (PSO) is a new, advanced, and most powerful
optimization methodology that performs empirically well on several optimization
problems. It is the extensively used Swarm Intelligence (SI) inspired
optimization algorithm used for finding the global optimal solution in a
multifaceted search region. Data clustering is one of the challenging real
world applications that invite the eminent research works in variety of fields.
Applicability of different PSO variants to data clustering is studied in the
literature, and the analyzed research work shows that, PSO variants give poor
results for multidimensional data. This paper describes the different
challenges associated with multidimensional data clustering and scope of
research on optimizing the clustering problems using PSO. We also propose a
strategy to use hybrid PSO variant for clustering multidimensional numerical,
text and image data.
</p>
"
http://arxiv.org/abs/1903.12074,Interpretation of machine learning predictions for patient outcomes in electronic health records. (arXiv:1903.12074v1 [cs.CY]),"<p>Electronic health records are an increasingly important resource for
understanding the interactions between patient health, environment, and
clinical decisions. In this paper we report an empirical study of predictive
modeling of several patient outcomes using three state-of-the-art machine
learning methods. Our primary goal is to validate the models by interpreting
the importance of predictors in the final models. Central to interpretation is
the use of feature importance scores, which vary depending on the underlying
methodology. In order to assess feature importance, we compared univariate
statistical tests, information-theoretic measures, permutation testing, and
normalized coefficients from multivariate logistic regression models. In
general we found poor correlation between methods in their assessment of
feature importance, even when their performance is comparable and relatively
good. However, permutation tests applied to random forest and gradient boosting
models showed the most agreement, and the importance scores matched the
clinical interpretation most frequently.
</p>
"
http://arxiv.org/abs/1903.12075,An Empirical Exploration on the Supervision of PhD Students Closely Collaborating with Industry. (arXiv:1903.12075v1 [cs.CY]),"<p>With an increase of PhD students working in industry, there is a need to
understand what factors are influencing supervision for industrial students.
This paper aims at exploring the challenges and good approaches to supervision
of industrial PhD students. Data was collected through semi-structured
interviews of six PhD students and supervisors with experience in PhD studies
at several organizations in the embedded software industry in Sweden. The data
was anonymized and it was analyzed by means of thematic analysis. The results
indicate that there are many challenges and opportunities to improve the
supervision of industrial PhD students.
</p>
"
http://arxiv.org/abs/1903.12076,Toward a fitness landscape model of firms' IT-enabled dynamic capabilities. (arXiv:1903.12076v1 [cs.CY]),"<p>This chapter presents, extends and integrates a complexity science
perspective and applies this to IT-enabled dynamic capabilities (ITDCs) of
firms. By doing so, this chapter leverages statistical survey data and uses
them as parameters for a simulation using the NK-model. This NK-model creates
stochastically generated fitness landscapes that are parameterized using a
finite number of (N) elements, or capabilities, and (K) complex interactions
between those capabilities, and studies the performance (fitness) of systems.
We simulate firm efforts to adaptively explore and walk through a fitness
landscape of possible strategies of inter-related capabilities to reach toward
higher levels of fitness of ITDCs. Also, our fitness landscape model provides
realistic scenarios with a nexus of possible business strategies that can be
employed considering the current status, interdependency, and alignment among
capabilities in the organization. Our work suggests that firms achieve the
highest fitness values when the interdependency among the individual
capabilities is relatively small.
</p>
"
http://arxiv.org/abs/1903.12079,Deterrence and Prevention-based Model to Mitigate Information Security Insider Threats in Organisations. (arXiv:1903.12079v1 [cs.CY]),"<p>Previous studies show that information security breaches and privacy
violations are important issues for organisations and people. It is
acknowledged that decreasing the risk in this domain requires consideration of
the technological aspects of information security alongside human aspects.
Employees intentionally or unintentionally account for a significant portion of
the threats to information assets in organisations. This research presents a
novel conceptual framework to mitigate the risk of insiders using deterrence
and prevention approaches. Deterrence factors discourage employees from
engaging in information security misbehaviour in organisations, and situational
crime prevention factors encourage them to prevent information security
misconduct. Our findings show that perceived sanctions certainty and severity
significantly influence individuals' attitudes and deter them from information
security misconduct. In addition, the output revealed that increasing the
effort, risk and reducing the reward (benefits of crime) influence the
employees' attitudes towards prevent information security misbehaviour.
However, removing excuses and reducing provocations do not significantly
influence individuals' attitudes towards prevent information security
misconduct. Finally, the output of the data analysis also showed that
subjective norms, perceived behavioural control and attitude influence
individuals' intentions, and, ultimately, their behaviour towards avoiding
information security misbehaviour.
</p>
"
http://arxiv.org/abs/1903.12080,Detecting Activities of Daily Living and Routine Behaviours in Dementia Patients Living Alone Using Smart Meter Load Disaggregation. (arXiv:1903.12080v1 [cs.CY]),"<p>The emergence of an ageing population is a significant public health concern.
This has led to an increase in the number of people living with progressive
neurodegenerative disorders like dementia. Consequently, the strain this is
places on health and social care services means providing 24-hour monitoring is
not sustainable. Technological intervention is being considered, however no
solution exists to non-intrusively monitor the independent living needs of
patients with dementia. As a result many patients hit crisis point before
intervention and support is provided. In parallel, patient care relies on
feedback from informal carers about significant behavioural changes. Yet, not
all people have a social support network and early intervention in dementia
care is often missed. The smart meter rollout has the potential to change this.
Using machine learning and signal processing techniques, a home energy supply
can be disaggregated to detect which home appliances are turned on and off.
This will allow Activities of Daily Living (ADLs) to be assessed, such as
eating and drinking, and observed changes in routine to be detected for early
intervention. The primary aim is to help reduce deterioration and enable
patients to stay in their homes for longer. A Support Vector Machine (SVM) and
Random Decision Forest classifier are modelled using data from three test
homes. The trained models are then used to monitor two patients with dementia
during a six-month clinical trial undertaken in partnership with Mersey Care
NHS Foundation Trust. In the case of load disaggregation for appliance
detection, the SVM achieved (AUC=0.86074, Sen=0.756 and Spec=0.92838). While
the Decision Forest achieved (AUC=0.9429, Sen=0.9634 and Spec=0.9634). ADLs are
also analysed to identify the behavioural patterns of the occupant while
detecting alterations in routine.
</p>
"
http://arxiv.org/abs/1903.12084,Definition of Internet of Things (IoT) Cyber Risk Discussion on a Transformation Roadmap for Standardisation of Regulations Risk Maturity Strategy Design and Impact Assessment. (arXiv:1903.12084v1 [cs.CR]),"<p>A comparative empirical analysis is performed to define a high-level
potential target state followed by a high-level transformation roadmap,
describing how company can achieve their target state, based on their current
state. The transformation roadmap is used to adapt the Goal-Oriented Approach
and the IoT Micro Mart model.
</p>
"
http://arxiv.org/abs/1903.12085,More Parallelism in Dijkstra's Single-Source Shortest Path Algorithm. (arXiv:1903.12085v1 [cs.DC]),"<p>Dijkstra's algorithm for the Single-Source Shortest Path (SSSP) problem is
notoriously hard to parallelize in $o(n)$ depth, $n$ being the number of
vertices in the input graph, without increasing the required parallel work
unreasonably. Crauser et al.\ (1998) presented observations that allow to
identify more than a single vertex at a time as correct and correspondingly
more edges to be relaxed simultaneously. Their algorithm runs in parallel
phases, and for certain random graphs they showed that the number of phases is
$O(n^{1/3})$ with high probability. A work-efficient CRCW PRAM with this depth
was given, but no implementation on a real, parallel system.
</p>
<p>In this paper we strengthen the criteria of Crauser et al., and discuss
tradeoffs between work and number of phases in their implementation. We present
simulation results with a range of common input graphs for the depth that an
ideal parallel algorithm that can apply the criteria at no cost and parallelize
relaxations without conflicts can achieve. These results show that the number
of phases is indeed a small root of $n$, but still off from the shortest path
length lower bound that can also be computed.
</p>
<p>We give a shared-memory parallel implementation of the most work-efficient
version of a Dijkstra's algorithm running in parallel phases, which we compare
to an own implementation of the well-known $\Delta$-stepping algorithm. We can
show that the work-efficient SSSP algorithm applying the criteria of Crauser et
al. is competitive to and often better than $\Delta$-stepping on our chosen
input graphs. Despite not providing an $o(n)$ guarantee on the number of
required phases, criteria allowing concurrent relaxation of many correct
vertices may be a viable approach to practically fast, parallel SSSP
implementations.
</p>
"
http://arxiv.org/abs/1903.12086,"Towards a Theory of Systems Engineering Processes: A Principal-Agent Model of a One-Shot, Shallow Process. (arXiv:1903.12086v1 [cs.MA])","<p>Systems engineering processes coordinate the effort of different individuals
to generate a product satisfying certain requirements. As the involved
engineers are self-interested agents, the goals at different levels of the
systems engineering hierarchy may deviate from the system-level goals which may
cause budget and schedule overruns. Therefore, there is a need of a systems
engineering theory that accounts for the human behavior in systems design. To
this end, the objective of this paper is to develop and analyze a
principal-agent model of a one-shot (single iteration), shallow (one level of
hierarchy) systems engineering process. We assume that the systems engineer
maximizes the expected utility of the system, while the subsystem engineers
seek to maximize their expected utilities. Furthermore, the systems engineer is
unable to monitor the effort of the subsystem engineer and may not have a
complete information about their types or the complexity of the design task.
However, the systems engineer can incentivize the subsystem engineers by
proposing specific contracts. To obtain an optimal incentive, we pose and solve
numerically a bi-level optimization problem. Through extensive simulations, we
study the optimal incentives arising from different system-level value
functions under various combinations of effort costs, problem-solving skills,
and task complexities.
</p>
"
http://arxiv.org/abs/1903.12087,A Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet. (arXiv:1903.12087v1 [eess.AS]),"<p>Neural speech synthesis algorithms are a promising new approach for coding
speech at very low bitrate. They have so far demonstrated quality that far
exceeds traditional vocoders, at the cost of very high complexity. In this
work, we present a low-bitrate neural vocoder based on the LPCNet model. The
use of linear prediction and sparse recurrent networks makes it possible to
achieve real-time operation on general-purpose hardware. We demonstrate that
LPCNet operating at 1.6 kb/s achieves significantly higher quality than MELP
and that uncompressed LPCNet can exceed the quality of a waveform codec
operating at low bitrate. This opens the way for new codec designs based on
neural synthesis models.
</p>
"
http://arxiv.org/abs/1903.12088,GANs-NQM: A Generative Adversarial Networks based No Reference Quality Assessment Metric for RGB-D Synthesized Views. (arXiv:1903.12088v1 [cs.MM]),"<p>In this paper, we proposed a no-reference (NR) quality metric for RGB plus
image-depth (RGB-D) synthesis images based on Generative Adversarial Networks
(GANs), namely GANs-NQM. Due to the failure of the inpainting on dis-occluded
regions in RGB-D synthesis process, to capture the non-uniformly distributed
local distortions and to learn their impact on perceptual quality are
challenging tasks for objective quality metrics. In our study, based on the
characteristics of GANs, we proposed i) a novel training strategy of GANs for
RGB-D synthesis images using existing large-scale computer vision datasets
rather than RGB-D dataset; ii) a referenceless quality metric based on the
trained discriminator by learning a `Bag of Distortion Word' (BDW) codebook and
a local distortion regions selector; iii) a hole filling inpainter, i.e., the
generator of the trained GANs, for RGB-D dis-occluded regions as a side
outcome. According to the experimental results on IRCCyN/IVC DIBR database, the
proposed model outperforms the state-of-the-art quality metrics, in addition,
is more applicable in real scenarios. The corresponding context inpainter also
shows appealing results over other inpainting algorithms.
</p>
"
http://arxiv.org/abs/1903.12090,Learning to Weight for Text Classification. (arXiv:1903.12090v1 [cs.LG]),"<p>In information retrieval (IR) and related tasks, term weighting approaches
typically consider the frequency of the term in the document and in the
collection in order to compute a score reflecting the importance of the term
for the document. In tasks characterized by the presence of training data (such
as text classification) it seems logical that the term weighting function
should take into account the distribution (as estimated from training data) of
the term across the classes of interest. Although `supervised term weighting'
approaches that use this intuition have been described before, they have failed
to show consistent improvements. In this article we analyse the possible
reasons for this failure, and call consolidated assumptions into question.
Following this criticism we propose a novel supervised term weighting approach
that, instead of relying on any predefined formula, learns a term weighting
function optimised on the training set of interest; we dub this approach
\emph{Learning to Weight} (LTW). The experiments that we run on several
well-known benchmarks, and using different learning methods, show that our
method outperforms previous term weighting approaches in text classification.
</p>
"
http://arxiv.org/abs/1903.12091,Nonlinear Model Predictive Control for Distributed Motion Planning in Road Intersections Using PANOC. (arXiv:1903.12091v1 [cs.SY]),"<p>The coordination of highly automated vehicles (or agents) in road
intersections is an inherently nonconvex and challenging problem. In this
paper, we propose a distributed motion planning scheme under reasonable
vehicle-to-vehicle communication requirements. Each agent solves a nonlinear
model predictive control problem in real time and transmits their planned
trajectory to other agents, which may have conflicting objectives. The problem
formulation is augmented with conditional constraints that enable the agents to
decide whether to wait at a stopping line, if crossing is not possible. The
involved nonconvex problems are solved very efficiently using the proximal
averaged Newton method for optimal control (PANOC). We demonstrate the
efficiency of the proposed approach in a realistic intersection crossing
scenario.
</p>
"
http://arxiv.org/abs/1903.12092,Deep Neural Network Embeddings with Gating Mechanisms for Text-Independent Speaker Verification. (arXiv:1903.12092v1 [eess.AS]),"<p>In this paper, gating mechanisms are applied in deep neural network (DNN)
training for x-vector-based text-independent speaker verification. First, a
gated convolution neural network (GCNN) is employed for modeling the
frame-level embedding layers. Compared with the time-delay DNN (TDNN), the GCNN
can obtain more expressive frame-level representations through carefully
designed memory cell and gating mechanisms. Moreover, we propose a novel
gated-attention statistics pooling strategy in which the attention scores are
shared with the output gate. The gated-attention statistics pooling combines
both gating and attention mechanisms into one framework; therefore, we can
capture more useful information in the temporal pooling layer. Experiments are
carried out using the NIST SRE16 and SRE18 evaluation datasets. The results
demonstrate the effectiveness of the GCNN and show that the proposed
gated-attention statistics pooling can further improve the performance.
</p>
"
http://arxiv.org/abs/1903.12094,Barking up the Right Tree: Improving Cross-Corpus Speech Emotion Recognition with Adversarial Discriminative Domain Generalization (ADDoG). (arXiv:1903.12094v1 [cs.LG]),"<p>Automatic speech emotion recognition provides computers with critical context
to enable user understanding. While methods trained and tested within the same
dataset have been shown successful, they often fail when applied to unseen
datasets. To address this, recent work has focused on adversarial methods to
find more generalized representations of emotional speech. However, many of
these methods have issues converging, and only involve datasets collected in
laboratory conditions. In this paper, we introduce Adversarial Discriminative
Domain Generalization (ADDoG), which follows an easier to train ""meet in the
middle"" approach. The model iteratively moves representations learned for each
dataset closer to one another, improving cross-dataset generalization. We also
introduce Multiclass ADDoG, or MADDoG, which is able to extend the proposed
method to more than two datasets, simultaneously. Our results show consistent
convergence for the introduced methods, with significantly improved results
when not using labels from the target dataset. We also show how, in most cases,
ADDoG and MADDoG can be used to improve upon baseline state-of-the-art methods
when target dataset labels are added and in-the-wild data are considered. Even
though our experiments focus on cross-corpus speech emotion, these methods
could be used to remove unwanted factors of variation in other settings.
</p>
"
http://arxiv.org/abs/1903.12099,Recommendation Systems for Tourism Based on Social Networks: A Survey. (arXiv:1903.12099v1 [cs.IR]),"<p>Nowadays, recommender systems are present in many daily activities such as
online shopping, browsing social networks, etc. Given the rising demand for
reinvigoration of the tourist industry through information technology,
recommenders have been included into tourism websites such as Expedia, Booking
or Tripadvisor, among others. Furthermore, the amount of scientific papers
related to recommender systems for tourism is on solid and continuous growth
since 2004. Much of this growth is due to social networks that, besides to
offer researchers the possibility of using a great mass of available and
constantly updated data, they also enable the recommendation systems to become
more personalised, effective and natural. This paper reviews and analyses many
research publications focusing on tourism recommender systems that use social
networks in their projects. We detail their main characteristics, like which
social networks are exploited, which data is extracted, the applied
recommendation techniques, the methods of evaluation, etc. Through a
comprehensive literature review, we aim to collaborate with the future
recommender systems, by giving some clear classifications and descriptions of
the current tourism recommender systems.
</p>
"
http://arxiv.org/abs/1903.12101,Extending Signature-based Intrusion Detection Systems WithBayesian Abductive Reasoning. (arXiv:1903.12101v1 [cs.CR]),"<p>Evolving cybersecurity threats are a persistent challenge for
systemadministrators and security experts as new malwares are continu-ally
released. Attackers may look for vulnerabilities in commercialproducts or
execute sophisticated reconnaissance campaigns tounderstand a targets network
and gather information on securityproducts like firewalls and intrusion
detection / prevention systems(network or host-based). Many new attacks tend to
be modificationsof existing ones. In such a scenario, rule-based systems fail
to detectthe attack, even though there are minor differences in conditions
/attributes between rules to identify the new and existing attack. Todetect
these differences the IDS must be able to isolate the subset ofconditions that
are true and predict the likely conditions (differentfrom the original) that
must be observed. In this paper, we proposeaprobabilistic abductive
reasoningapproach that augments an exist-ing rule-based IDS (snort [29]) to
detect these evolved attacks by (a)Predicting rule conditions that are likely
to occur (based on existingrules) and (b) able to generate new snort rules when
provided withseed rule (i.e. a starting rule) to reduce the burden on experts
toconstantly update them. We demonstrate the effectiveness of theapproach by
generating new rules from the snort 2012 rules set andtesting it on the MACCDC
2012 dataset [6].
</p>
"
http://arxiv.org/abs/1903.12107,Quality Assessment of Free-viewpoint Videos by Quantifying the Elastic Changes of Multi-Scale Motion Trajectories. (arXiv:1903.12107v1 [cs.MM]),"<p>Virtual viewpoints synthesis is an essential process for many immersive
applications including Free-viewpoint TV (FTV). A widely used technique for
viewpoints synthesis is Depth-Image-Based-Rendering (DIBR) technique. However,
such techniques may introduce challenging non-uniform spatial-temporal
structure-related distortions. Most of the existing state-of-the-art quality
metrics fail to handle these distortions, especially the temporal structure
inconsistencies observed during the switch of different viewpoints. To tackle
this problem, an elastic metric and multi-scale trajectory based video quality
metric (EM-VQM) is proposed in this paper. Dense motion trajectory is first
used as a proxy for selecting temporal sensitive regions, where local geometric
distortions might significantly diminish the perceived quality. Afterwards, the
amount of temporal structure inconsistencies and unsmooth viewpoints
transitions are quantified by calculating 1) the amount of motion trajectory
deformations with elastic metric and, 2) the spatial-temporal structural
dissimilarity. According to the comprehensive experimental results on two FTV
video datasets, the proposed metric outperforms the state-of-the-art metrics
designed for free-viewpoint videos significantly and achieves a gain of 12.86%
and 16.75% in terms of median Pearson linear correlation coefficient values on
the two datasets compared to the best one, respectively.
</p>
"
http://arxiv.org/abs/1903.12110,Building Automated Survey Coders via Interactive Machine Learning. (arXiv:1903.12110v1 [cs.IR]),"<p>Software systems trained via machine learning to automatically classify
open-ended answers (a.k.a. verbatims) are by now a reality. Still, their
adoption in the survey coding industry has been less widespread than it might
have been. Among the factors that have hindered a more massive takeup of this
technology are the effort involved in manually coding a sufficient amount of
training data, the fact that small studies do not seem to justify this effort,
and the fact that the process needs to be repeated anew when brand new coding
tasks arise. In this paper we will argue for an approach to building verbatim
classifiers that we will call ""Interactive Learning"", and that addresses all
the above problems. We will show that, for the same amount of training effort,
interactive learning delivers much better coding accuracy than standard
""non-interactive"" learning. This is especially true when the amount of data we
are willing to manually code is small, which makes this approach attractive
also for small-scale studies. Interactive learning also lends itself to reusing
previously trained classifiers for dealing with new (albeit related) coding
tasks. Interactive learning also integrates better in the daily workflow of the
survey specialist, and delivers a better user experience overall.
</p>
"
http://arxiv.org/abs/1903.12113,A Counterexample-guided Approach to Finding Numerical Invariants. (arXiv:1903.12113v1 [cs.SE]),"<p>Numerical invariants, e.g., relationships among numerical variables in a
program, represent a useful class of properties to analyze programs. General
polynomial invariants represent more complex numerical relations, but they are
often required in many scientific and engineering applications. We present
NumInv, a tool that implements a counterexample-guided invariant generation
(CEGIR) technique to automatically discover numerical invariants, which are
polynomial equality and inequality relations among numerical variables. This
CEGIR technique infers candidate invariants from program traces and then checks
them against the program source code using the KLEE test-input generation tool.
If the invariants are incorrect KLEE returns counterexample traces, which help
the dynamic inference obtain better results. Existing CEGIR approaches often
require sound invariants, however NumInv sacrifices soundness and produces
results that KLEE cannot refute within certain time bounds. This design and the
use of KLEE as a verifier allow NumInv to discover useful and important
numerical invariants for many challenging programs.
</p>
<p>Preliminary results show that NumInv generates required invariants for
understanding and verifying correctness of programs involving complex
arithmetic. We also show that NumInv discovers polynomial invariants that
capture precise complexity bounds of programs used to benchmark existing static
complexity analysis techniques. Finally, we show that NumInv performs
competitively comparing to state of the art numerical invariant analysis tools.
</p>
"
http://arxiv.org/abs/1903.12117,Many Task Learning with Task Routing. (arXiv:1903.12117v1 [cs.CV]),"<p>Typical multi-task learning (MTL) methods rely on architectural adjustments
and a large trainable parameter set to jointly optimize over several tasks.
However, when the number of tasks increases so do the complexity of the
architectural adjustments and resource requirements. In this paper, we
introduce a method which applies a conditional feature-wise transformation over
the convolutional activations that enables a model to successfully perform a
large number of tasks. To distinguish from regular MTL, we introduce Many Task
Learning (MaTL) as a special case of MTL where more than 20 tasks are performed
by a single model. Our method dubbed Task Routing (TR) is encapsulated in a
layer we call the Task Routing Layer (TRL), which applied in an MaTL scenario
successfully fits hundreds of classification tasks in one model. We evaluate
our method on 5 datasets against strong baselines and state-of-the-art
approaches.
</p>
"
http://arxiv.org/abs/1903.12118,From Motions to Emotions: Can the Fundamental Emotions be Expressed in a Robot Swarm?. (arXiv:1903.12118v1 [cs.RO]),"<p>This paper explores the expressive capabilities of a swarm of miniature
mobile robots within the context of inter-robot interactions and their mapping
to the so-called fundamental emotions. In particular, we investigate how motion
and shape descriptors that are psychologically associated with different
emotions can be incorporated into different swarm behaviors for the purpose of
artistic expositions. Based on these characterizations from social psychology,
a set of swarm behaviors is created, where each behavior corresponds to a
fundamental emotion. The effectiveness of these behaviors was evaluated in a
survey in which the participants were asked to associate different swarm
behaviors with the fundamental emotions. The results of the survey show that
most of the research participants assigned to each video the emotion intended
to be portrayed by design. These results confirm that abstract descriptors
associated with the different fundamental emotions in social psychology provide
useful motion characterizations that can be effectively transformed into
expressive behaviors for a swarm of simple ground mobile robots.
</p>
"
http://arxiv.org/abs/1903.12125,Nearest-Neighbor Neural Networks for Geostatistics. (arXiv:1903.12125v1 [stat.ML]),"<p>Kriging is the predominant method used for spatial prediction, but relies on
the assumption that predictions are linear combinations of the observations.
Kriging often also relies on additional assumptions such as normality and
stationarity. We propose a more flexible spatial prediction method based on the
Nearest-Neighbor Neural Network (4N) process that embeds deep learning into a
geostatistical model. We show that the 4N process is a valid stochastic process
and propose a series of new ways to construct features to be used as inputs to
the deep learning model based on neighboring information. Our model framework
outperforms some existing state-of-art geostatistical modelling methods for
simulated non-Gaussian data and is applied to a massive forestry dataset.
</p>
"
http://arxiv.org/abs/1903.12127,Using Latent Class Analysis to Identify ARDS Sub-phenotypes for Enhanced Machine Learning Predictive Performance. (arXiv:1903.12127v1 [cs.LG]),"<p>In this work, we utilize Machine Learning for early recognition of patients
at high risk of acute respiratory distress syndrome (ARDS), which is critical
for successful prevention strategies for this devastating syndrome. The
difficulty in early ARDS recognition stems from its complex and heterogenous
nature. In this study, we integrate knowledge of the heterogeneity of ARDS
patients into predictive model building. Using MIMIC-III data, we first apply
latent class analysis (LCA) to identify homogeneous sub-groups in the ARDS
population, and then build predictive models on the partitioned data. The
results indicate that significantly improved performances of prediction can be
obtained for two of the three identified sub-phenotypes of ARDS. Experiments
suggests that identifying sub-phenotypes is beneficial for building predictive
model for ARDS.
</p>
"
http://arxiv.org/abs/1903.12133,A Multimodal Emotion Sensing Platform for Building Emotion-Aware Applications. (arXiv:1903.12133v1 [cs.HC]),"<p>Humans use a host of signals to infer the emotional state of others. In
general, computer systems that leverage signals from multiple modalities will
be more robust and accurate in the same task. We present a multimodal affect
and context sensing platform. The system is composed of video, audio and
application analysis pipelines that leverage ubiquitous sensors (camera and
microphone) to log and broadcast emotion data in real-time. The platform is
designed to enable easy prototyping of novel computer interfaces that sense,
respond and adapt to human emotion. This paper describes the different audio,
visual and application processing components and explains how the data is
stored and/or broadcast for other applications to consume. We hope that this
platform helps advance the state-of-the-art in affective computing by enabling
development of novel human-computer interfaces.
</p>
"
http://arxiv.org/abs/1903.12135,Sparse Reconstruction from Hadamard Matrices: A Lower Bound. (arXiv:1903.12135v1 [cs.IT]),"<p>We give a short argument that yields a new lower bound on the number of
subsampled rows from a bounded, orthonormal matrix necessary to form a matrix
with the restricted isometry property. We show that for a $N \times N$ Hadamard
matrix, one cannot recover all $k$-sparse vectors unless the number of
subsampled rows is $\Omega(k \log^2 N)$.
</p>
"
http://arxiv.org/abs/1903.12136,Distilling Task-Specific Knowledge from BERT into Simple Neural Networks. (arXiv:1903.12136v1 [cs.CL]),"<p>In the natural language processing literature, neural networks are becoming
increasingly deeper and complex. The recent poster child of this trend is the
deep language representation model, which includes BERT, ELMo, and GPT. These
developments have led to the conviction that previous-generation, shallower
neural networks for language understanding are obsolete. In this paper,
however, we demonstrate that rudimentary, lightweight neural networks can still
be made competitive without architecture changes, external training data, or
additional input features. We propose to distill knowledge from BERT, a
state-of-the-art language representation model, into a single-layer BiLSTM, as
well as its siamese counterpart for sentence-pair tasks. Across multiple
datasets in paraphrasing, natural language inference, and sentiment
classification, we achieve comparable results with ELMo, while using roughly
100 times fewer parameters and 15 times less inference time.
</p>
"
http://arxiv.org/abs/1903.12139,Automatic Defect Segmentation on Leather with Deep Learning. (arXiv:1903.12139v1 [cs.CV]),"<p>Leather is a natural and durable material created through a process of
tanning of hides and skins of animals. The price of the leather is subjective
as it is highly sensitive to its quality and surface defects condition. In the
literature, there are very few works investigating on the defects detection for
leather using automatic image processing techniques. The manual defect
inspection process is essential in an leather production industry to control
the quality of the finished products. However, it is tedious, as it is labour
intensive, time consuming, causes eye fatigue and often prone to human error.
In this paper, a fully automatic defect detection and marking system on a calf
leather is proposed. The proposed system consists of a piece of leather, LED
light, high resolution camera and a robot arm. Succinctly, a machine vision
method is presented to identify the position of the defects on the leather
using a deep learning architecture. Then, a series of processes are conducted
to predict the defect instances, including elicitation of the leather images
with a robot arm, train and test the images using a deep learning architecture
and determination of the boundary of the defects using mathematical derivation
of the geometry. Note that, all the processes do not involve human
intervention, except for the defect ground truths construction stage. The
proposed algorithm is capable to exhibit 91.5% segmentation accuracy on the
train data and 70.35% on the test data. We also report confusion matrix,
F1-score, precision and specificity, sensitivity performance metrics to further
verify the effectiveness of the proposed approach.
</p>
"
http://arxiv.org/abs/1903.12141,Improving MAE against CCE under Label Noise. (arXiv:1903.12141v1 [cs.LG]),"<p>Label noise is inherent in many deep learning tasks when the training set
becomes large. A typical approach to tackle noisy labels is using robust loss
functions. Categorical cross entropy (CCE) is a successful loss function in
many applications. However, CCE is also notorious for fitting samples with
corrupted labels easily. In contrast, mean absolute error (MAE) is
noise-tolerant theoretically, but it generally works much worse than CCE in
practice. In this work, we have three main points. First, to explain why MAE
generally performs much worse than CCE, we introduce a new understanding of
them fundamentally by exposing their intrinsic sample weighting schemes from
the perspective of every sample's gradient magnitude with respect to logit
vector. Consequently, we find that MAE's differentiation degree over training
examples is too small so that informative ones cannot contribute enough against
the non-informative during training. Therefore, MAE generally underfits
training data when noise rate is high. Second, based on our finding, we propose
an improved MAE (IMAE), which inherits MAE's good noise-robustness. Moreover,
the differentiation degree over training data points is controllable so that
IMAE addresses the underfitting problem of MAE. Third, the effectiveness of
IMAE against CCE and MAE is evaluated empirically with extensive experiments,
which focus on image classification under synthetic corrupted labels and video
retrieval under real noisy labels.
</p>
"
http://arxiv.org/abs/1903.12146,Improved Lower Bounds for the Restricted Isometry Property of Subsampled Fourier Matrices. (arXiv:1903.12146v1 [cs.IT]),"<p>Let $A$ be an $N \times N$ Fourier matrix over
$\mathbb{F}_p^{\log{N}/\log{p}}$ for some prime $p$. We improve upon known
lower bounds for the number of rows of $A$ that must be sampled so that the
resulting matrix $M$ satisfies the restricted isometry property for $k$-sparse
vectors. This property states that $\|Mv\|_2^2$ is approximately $\|v\|_2^2$
for all $k$-sparse vectors $v$. In particular, if $k = \Omega( \log^2{N})$, we
show that $\Omega(k\log{k}\log{N}/\log{p})$ rows must be sampled to satisfy the
restricted isometry property with constant probability.
</p>
"
http://arxiv.org/abs/1903.12150,Dynamic Streaming Spectral Sparsification in Nearly Linear Time and Space. (arXiv:1903.12150v1 [cs.DS]),"<p>In this paper we consider the problem of computing spectral approximations to
graphs in the single pass dynamic streaming model. We provide a linear
sketching based solution that given a stream of edge insertions and deletions
to a $n$-node undirected graph, uses $\tilde O(n)$ space, processes each update
in $\tilde O(1)$ time, and with high probability recovers a spectral sparsifier
in $\tilde O(n)$ time. Prior to our work, state of the art results either used
near optimal $\tilde O(n)$ space complexity, but brute-force $\Omega(n^2)$
recovery time [Kapralov et al.'14], or with subquadratic runtime, but
polynomially suboptimal space complexity [Ahn et al.'14, Kapralov et al.'19].
</p>
<p>Our main technical contribution is a novel method for `bucketing' vertices of
the input graph into clusters that allows fast recovery of edges of
sufficiently large effective resistance. Our algorithm first buckets vertices
of the graph by performing ball-carving using (an approximation to) its
effective resistance metric, and then recovers the high effective resistance
edges from a sketched version of an electrical flow between vertices in a
bucket, taking nearly linear time in the number of vertices overall. This
process is performed at different geometric scales to recover a sample of edges
with probabilities proportional to effective resistances and obtain an actual
sparsifier of the input graph.
</p>
<p>This work provides both the first efficient $\ell_2$-sparse recovery
algorithm for graphs and new primitives for manipulating the effective
resistance embedding of a graph, both of which we hope have further
applications.
</p>
"
http://arxiv.org/abs/1903.12152,3D Whole Brain Segmentation using Spatially Localized Atlas Network Tiles. (arXiv:1903.12152v1 [cs.CV]),"<p>Detailed whole brain segmentation is an essential quantitative technique,
which provides a non-invasive way of measuring brain regions from a structural
magnetic resonance imaging (MRI). Recently, deep convolution neural network
(CNN) has been applied to whole brain segmentation. However, restricted by
current GPU memory, 2D based methods, downsampling based 3D CNN methods, and
patch-based high-resolution 3D CNN methods have been the de facto standard
solutions. 3D patch-based high resolution methods typically yield superior
performance among CNN approaches on detailed whole brain segmentation (&gt;100
labels), however, whose performance are still commonly inferior compared with
multi-atlas segmentation methods (MAS) due to the following challenges: (1) a
single network is typically used to learn both spatial and contextual
information for the patches, (2) limited manually traced whole brain volumes
are available (typically less than 50) for training a network. In this work, we
propose the spatially localized atlas network tiles (SLANT) method to
distribute multiple independent 3D fully convolutional networks (FCN) for
high-resolution whole brain segmentation. To address the first challenge,
multiple spatially distributed networks were used in the SLANT method, in which
each network learned contextual information for a fixed spatial location. To
address the second challenge, auxiliary labels on 5111 initially unlabeled
scans were created by multi-atlas segmentation for training. Since the method
integrated multiple traditional medical image processing methods with deep
learning, we developed a containerized pipeline to deploy the end-to-end
solution. From the results, the proposed method achieved superior performance
compared with multi-atlas segmentation methods, while reducing the
computational time from &gt;30 hours to 15 minutes
(https://github.com/MASILab/SLANTbrainSeg).
</p>
"
http://arxiv.org/abs/1903.12157,Resilient Combination of Complementary CNN and RNN Features for Text Classification through Attention and Ensembling. (arXiv:1903.12157v1 [cs.CL]),"<p>State-of-the-art methods for text classification include several distinct
steps of pre-processing, feature extraction and post-processing. In this work,
we focus on end-to-end neural architectures and show that the best performance
in text classification is obtained by combining information from different
neural modules. Concretely, we combine convolution, recurrent and attention
modules with ensemble methods and show that they are complementary. We
introduce ECGA, an end-to-end go-to architecture for novel text classification
tasks. We prove that it is efficient and robust, as it attains or surpasses the
state-of-the-art on varied datasets, including both low and high data regimes.
</p>
"
http://arxiv.org/abs/1903.12161,Fast video object segmentation with Spatio-Temporal GANs. (arXiv:1903.12161v1 [cs.CV]),"<p>Learning descriptive spatio-temporal object models from data is paramount for
the task of semi-supervised video object segmentation. Most existing approaches
mainly rely on models that estimate the segmentation mask based on a reference
mask at the first frame (aided sometimes by optical flow or the previous mask).
These models, however, are prone to fail under rapid appearance changes or
occlusions due to their limitations in modelling the temporal component. On the
other hand, very recently, other approaches learned long-term features using a
convolutional LSTM to leverage the information from all previous video frames.
Even though these models achieve better temporal representations, they still
have to be fine-tuned for every new video sequence. In this paper, we present
an intermediate solution and devise a novel GAN architecture, FaSTGAN, to learn
spatio-temporal object models over finite temporal windows. To achieve this, we
concentrate all the heavy computational load to the training phase with two
critics that enforce spatial and temporal mask consistency over the last K
frames. Then at test time, we only use a relatively light regressor, which
reduces the inference time considerably. As a result, our approach combines a
high resiliency to sudden geometric and photometric object changes with
efficiency at test time (no need for fine-tuning nor post-processing). We
demonstrate that the accuracy of our method is on par with state-of-the-art
techniques on the challenging YouTube-VOS and DAVIS datasets, while running at
32 fps, about 4x faster than the closest competitor.
</p>
"
http://arxiv.org/abs/1903.12164,Cache-Version Selection and Content Placement for Adaptive Video Streaming in Wireless Edge Networks. (arXiv:1903.12164v1 [cs.NI]),"<p>Wireless edge networks are promising to provide better video streaming
services to mobile users by provisioning computing and storage resources at the
edge of wireless network. However, due to the diversity of user interests, user
devices, video versions or resolutions, cache sizes, network conditions, etc.,
it is challenging to decide where to place the video contents, and which cache
and video version a mobile user device should select. In this paper, we study
the joint optimization of cache-version selection and content placement for
adaptive video streaming in wireless edge networks. We propose a set of
practical distributed algorithms that operate at each user device and each
network cache to maximize the overall network utility. In addition to proving
that our algorithms indeed achieve the optimal performance, we implement our
algorithms as well as several baseline algorithms on ndnSIM, an ns-3 based
Named Data Networking simulator. Simulation evaluations demonstrate that our
algorithms significantly outperform conventional heuristic solutions for
adaptive video streaming.
</p>
"
http://arxiv.org/abs/1903.12165,Faster Spectral Sparsification in Dynamic Streams. (arXiv:1903.12165v1 [cs.DS]),"<p>Graph sketching has emerged as a powerful technique for processing massive
graphs that change over time (i.e., are presented as a dynamic stream of edge
updates) over the past few years, starting with the work of Ahn, Guha and
McGregor (SODA'12) on graph connectivity via sketching. In this paper we
consider the problem of designing spectral approximations to graphs, or
spectral sparsifiers, using a small number of linear measurements, with the
additional constraint that the sketches admit an efficient recovery scheme.
</p>
<p>Prior to our work, sketching algorithms were known with near optimal $\tilde
O(n)$ space complexity, but $\Omega(n^2)$ time decoding (brute-force over all
potential edges of the input graph), or with subquadratic time, but rather
large $\Omega(n^{5/3})$ space complexity (due to their reliance on a rather
weak relation between connectivity and effective resistances). In this paper we
first show how a simple relation between effective resistances and edge
connectivity leads to an improved $\widetilde O(n^{3/2})$ space and time
algorithm, which we show is a natural barrier for connectivity based
approaches. Our main result then gives the first algorithm that achieves
subquadratic recovery time, i.e. avoids brute-force decoding, and at the same
time nontrivially uses the effective resistance metric, achieving
$n^{1.4+o(1)}$ space and recovery time.
</p>
<p>Our main technical contribution is a novel method for `bucketing' vertices of
the input graph into clusters that allows fast recovery of edges of high
effective resistance: the buckets are formed by performing ball-carving on the
input graph using (an approximation to) its effective resistance metric. We
feel that this technique is likely to be of independent interest.
</p>
"
http://arxiv.org/abs/1903.12174,TensorMask: A Foundation for Dense Object Segmentation. (arXiv:1903.12174v1 [cs.CV]),"<p>Sliding-window object detectors that generate bounding-box object predictions
over a dense, regular grid have advanced rapidly and proven popular. In
contrast, modern instance segmentation approaches are dominated by methods that
first detect object bounding boxes, and then crop and segment these regions, as
popularized by Mask R-CNN. In this work, we investigate the paradigm of dense
sliding-window instance segmentation, which is surprisingly under-explored. Our
core observation is that this task is fundamentally different than other dense
prediction tasks such as semantic segmentation or bounding-box object
detection, as the output at every spatial location is itself a geometric
structure with its own spatial dimensions. To formalize this, we treat dense
instance segmentation as a prediction task over 4D tensors and present a
general framework called TensorMask that explicitly captures this geometry and
enables novel operators on 4D tensors. We demonstrate that the tensor view
leads to large gains over baselines that ignore this structure, and leads to
results comparable to Mask R-CNN. These promising results suggest that
TensorMask can serve as a foundation for novel advances in dense mask
prediction and a more complete understanding of the task. Code will be made
available.
</p>
"
http://arxiv.org/abs/1405.4806,Undecidability of model-checking branching-time properties of stateless probabilistic pushdown process. (arXiv:1405.4806v9 [cs.LO] UPDATED),"<p>In this paper, we settle a problem in probabilistic verification of
infinite--state process (specifically, {\it probabilistic pushdown process}).
We show that model checking {\it stateless probabilistic pushdown process}
(pBPA) against {\it probabilistic computational tree logic} (PCTL) is
undecidable.
</p>
"
http://arxiv.org/abs/1610.02336,Approximation Algorithms for Multi-Multiway Cut and Multicut Problems on Directed Graphs. (arXiv:1610.02336v4 [cs.DS] UPDATED),"<p>In this paper, we present two approximation algorithms for the directed
multi-multiway cut and directed multicut problems. The so called region growing
paradigm \cite{1} is modified and used for these two cut problems on directed
graphs. By using this paradigm, we give for each problem an approximation
algorithm such that both algorithms have the approximate factor $O(k)$ the same
as the previous works done on these problems. However, the previous works need
to solve $k$ linear programming, whereas our algorithms require only one linear
programming. Therefore, our algorithms improve the running time of the previous
algorithms.
</p>
"
http://arxiv.org/abs/1612.07562,On the function approximation error for risk-sensitive reinforcement learning. (arXiv:1612.07562v14 [cs.LG] UPDATED),"<p>In this paper we obtain several informative error bounds on function
approximation for the policy evaluation algorithm proposed by Basu et al. when
the aim is to find the risk-sensitive cost represented using exponential
utility. The main idea is to use classical Bapat's inequality and to use
Perron-Frobenius eigenvectors (exists if we assume irreducible Markov chain) to
get the new bounds. The novelty of our approach is that we use the
irreduciblity of Markov chain to get the new bounds whereas the earlier work by
Basu et al. used spectral variation bound which is true for any matrix. We also
give examples where all our bounds achieve the ""actual error"" whereas the
earlier bound given by Basu et al. is much weaker in comparison. We show that
this happens due to the absence of difference term in the earlier bound which
is always present in all our bounds when the state space is large.
Additionally, we discuss how all our bounds compare with each other. As a
corollary of our main result we provide a bound between largest eigenvalues of
two irreducibile matrices in terms of the matrix entries.
</p>
"
http://arxiv.org/abs/1703.05834,Alignment of the Virtual Scene to the Tracking Space of a Mixed Reality Head-Mounted Display. (arXiv:1703.05834v4 [cs.HC] UPDATED),"<p>With the mounting global interest for optical see-through head-mounted
displays (OST-HMDs) across medical, industrial and entertainment settings, many
systems with different capabilities are rapidly entering the market. Despite
such variety, they all require display calibration to create a proper mixed
reality environment. With the aid of tracking systems, it is possible to
register rendered graphics with tracked objects in the real world. We propose a
calibration procedure to properly align the coordinate system of a 3D virtual
scene that the user sees with that of the tracker. Our method takes a blackbox
approach towards the HMD calibration, where the tracker's data is its input and
the 3D coordinates of a virtual object in the observer's eye is the output; the
objective is thus to find the 3D projection that aligns the virtual content
with its real counterpart. In addition, a faster and more intuitive version of
this calibration is introduced in which the user simultaneously aligns multiple
points of a single virtual 3D object with its real counterpart; this reduces
the number of required repetitions in the alignment from 20 to only 4, which
leads to a much easier calibration task for the user. In this paper, both
internal (HMD camera) and external tracking systems are studied. We perform
experiments with Microsoft HoloLens, taking advantage of its self localization
and spatial mapping capabilities to eliminate the requirement for line of sight
from the HMD to the object or external tracker. The experimental results
indicate an accuracy of up to 4 mm in the average reprojection error based on
two separate evaluation methods. We further perform experiments with the
internal tracking on the Epson Moverio BT-300 to demonstrate that the method
can provide similar results with other HMDs.
</p>
"
http://arxiv.org/abs/1705.03557,DeepTingle. (arXiv:1705.03557v2 [cs.CL] UPDATED),"<p>DeepTingle is a text prediction and classification system trained on the
collected works of the renowned fantastic gay erotica author Chuck Tingle.
Whereas the writing assistance tools you use everyday (in the form of
predictive text, translation, grammar checking and so on) are trained on
generic, purportedly ""neutral"" datasets, DeepTingle is trained on a very
specific, internally consistent but externally arguably eccentric dataset. This
allows us to foreground and confront the norms embedded in data-driven
creativity and productivity assistance tools. As such tools effectively
function as extensions of our cognition into technology, it is important to
identify the norms they embed within themselves and, by extension, us.
DeepTingle is realized as a web application based on LSTM networks and the
GloVe word embedding, implemented in JavaScript with Keras-JS.
</p>
"
http://arxiv.org/abs/1705.06058,Pitfalls and Best Practices in Algorithm Configuration. (arXiv:1705.06058v3 [cs.AI] UPDATED),"<p>Good parameter settings are crucial to achieve high performance in many areas
of artificial intelligence (AI), such as propositional satisfiability solving,
AI planning, scheduling, and machine learning (in particular deep learning).
Automated algorithm configuration methods have recently received much attention
in the AI community since they replace tedious, irreproducible and error-prone
manual parameter tuning and can lead to new state-of-the-art performance.
However, practical applications of algorithm configuration are prone to several
(often subtle) pitfalls in the experimental design that can render the
procedure ineffective. We identify several common issues and propose best
practices for avoiding them. As one possibility for automatically handling as
many of these as possible, we also propose a tool called GenericWrapper4AC.
</p>
"
http://arxiv.org/abs/1707.05881,On Thin Air Reads: Towards an Event Structures Model of Relaxed Memory. (arXiv:1707.05881v5 [cs.PL] UPDATED),"<p>To model relaxed memory, we propose confusion-free event structures over an
alphabet with a justification relation. Executions are modeled by justified
configurations, where every read event has a justifying write event.
Justification alone is too weak a criterion, since it allows cycles of the kind
that result in so-called thin-air reads. Acyclic justification forbids such
cycles, but also invalidates event reorderings that result from compiler
optimizations and dynamic instruction scheduling. We propose the notion of
well-justification, based on a game-like model, which strikes a middle ground.
</p>
<p>We show that well-justified configurations satisfy the DRF theorem: in any
data-race free program, all well-justified configurations are sequentially
consistent. We also show that rely-guarantee reasoning is sound for
well-justified configurations, but not for justified configurations. For
example, well-justified configurations are type-safe.
</p>
<p>Well-justification allows many, but not all reorderings performed by relaxed
memory. In particular, it fails to validate the commutation of independent
reads. We discuss variations that may address these shortcomings.
</p>
"
http://arxiv.org/abs/1708.01688,Abstract Hidden Markov Models: a monadic account of quantitative information flow. (arXiv:1708.01688v3 [cs.LO] UPDATED),"<p>Hidden Markov Models, HMM's, are mathematical models of Markov processes with
state that is hidden, but from which information can leak. They are typically
represented as 3-way joint-probability distributions.
</p>
<p>We use HMM's as denotations of probabilistic hidden-state sequential
programs: for that, we recast them as `abstract' HMM's, computations in the
Giry monad $\mathbb{D}$, and we equip them with a partial order of increasing
security. However to encode the monadic type with hiding over some state
$\mathcal{X}$ we use $\mathbb{D}\mathcal{X}\to \mathbb{D}^2\mathcal{X}$ rather
than the conventional $\mathcal{X}{\to}\mathbb{D}\mathcal{X}$ that suffices for
Markov models whose state is not hidden. We illustrate the
$\mathbb{D}\mathcal{X}\to \mathbb{D}^2\mathcal{X}$ construction with a small
Haskell prototype.
</p>
<p>We then present uncertainty measures as a generalisation of the extant
diversity of probabilistic entropies, with characteristic analytic properties
for them, and show how the new entropies interact with the order of increasing
security. Furthermore, we give a `backwards' uncertainty-transformer semantics
for HMM's that is dual to the `forwards' abstract HMM's - it is an analogue of
the duality between forwards, relational semantics and backwards,
predicate-transformer semantics for imperative programs with demonic choice.
</p>
<p>Finally, we argue that, from this new denotational-semantic viewpoint, one
can see that the Dalenius desideratum for statistical databases is actually an
issue in compositionality. We propose a means for taking it into account.
</p>
"
http://arxiv.org/abs/1709.07605,mts: A light framework for parallelizing tree search codes. (arXiv:1709.07605v2 [cs.DC] UPDATED),"<p>We describe mts, a generic framework for parallelizing certain types of tree
search programs including reverse search, backtracking, branch and bound and
satisfiability testing. It abstracts and generalizes the ideas used in
parallelizing lrs, a reverse search code for vertex enumeration. mts supports
sharing information between processes which is important for applications such
as satisfiability testing and branch-and-bound. No parallelization is
implemented in the legacy single processor programs minimizing the changes
needed and simplying debugging. mts is written in C, uses MPI for
parallelization and can be used on a network of computers. We give four
examples of reverse search codes parallelized by using mts: topological sorts,
spanning trees, triangulations and Galton-Watson trees. We also give a
parallelization of two codes for satisfiability testing. We give experimental
results comparing the parallel codes with other codes for the same problems.
</p>
"
http://arxiv.org/abs/1710.10718,An Approximation Algorithm for Optimal Clique Cover Delivery in Coded Caching. (arXiv:1710.10718v3 [cs.IT] UPDATED),"<p>Coded caching can significantly reduce the communication bandwidth
requirement for satisfying users' demands by utilizing the multicasting gain
among multiple users. Most existing works assume that the users follow the
prescriptions for content placement made by the system. However, users may
prefer to decide what files to cache. To address this issue, we consider a
network consisting of a file server connected through a shared link to $K$
users, each equipped with a cache which has been already filled arbitrarily.
Given an arbitrary content placement, the goal is to find a delivery strategy
for the server that minimizes the load of the shared link. In this paper, we
focus on a specific class of coded multicasting delivery schemes known as the
""clique cover delivery scheme"". We first formulate the optimal clique cover
delivery problem as a combinatorial optimization problem. Using a connection
with the weighted set cover problem, we propose an approximation algorithm and
show that it provides an approximation ratio of $(1 + \log K)$, while the
approximation ratio for the existing coded delivery schemes is linear in $K$.
Numerical simulations show that our proposed algorithm provides a considerable
bandwidth reduction over the existing coded delivery schemes for almost all
content placement schemes.
</p>
"
http://arxiv.org/abs/1711.00113,Proving Soundness of Extensional Normal-Form Bisimilarities. (arXiv:1711.00113v4 [cs.LO] UPDATED),"<p>Normal-form bisimilarity is a simple, easy-to-use behavioral equivalence that
relates terms in $\lambda$-calculi by decomposing their normal forms into
bisimilar subterms. Moreover, it typically allows for powerful up-to
techniques, such as bisimulation up to context, which simplify bisimulation
proofs even further. However, proving soundness of these relations becomes
complicated in the presence of $\eta$-expansion and usually relies on ad hoc
proof methods which depend on the language. In this paper we propose a more
systematic proof method to show that an extensional normal-form bisimilarity
along with its corresponding up to context technique are sound. We illustrate
our technique with three calculi: the call-by-value $\lambda$-calculus, the
call-by-value $\lambda$-calculus with the delimited-control operators shift and
reset, and the call-by-value $\lambda$-calculus with the abortive control
operators call/cc and abort. In the first two cases, there was previously no
sound up to context technique validating the $\eta$-law, whereas no theory of
normal-form bisimulations for a calculus with call/cc and abort has been
presented before. Our results have been fully formalized in the Coq proof
assistant.
</p>
"
http://arxiv.org/abs/1712.05500,Ergodicity of some classes of cellular automata subject to noise. (arXiv:1712.05500v3 [math.PR] UPDATED),"<p>Cellular automata (CA) are dynamical systems on symbolic configurations on
the lattice. They are also used as models of massively parallel computers. As
dynamical systems, one would like to understand the effect of small random
perturbations on the dynamics of CA. As models of computation, they can be used
to study the reliability of computation against noise.
</p>
<p>We consider various families of CA (nilpotent, permutive, gliders, CA with a
spreading symbol, surjective, algebraic) and prove that they are highly
unstable against noise, meaning that they forget their initial conditions under
slightest positive noise. This is manifested as the ergodicity of the resulting
probabilistic CA. The proofs involve a collection of different techniques
(couplings, entropy, Fourier analysis), depending on the dynamical properties
of the underlying deterministic CA and the type of noise.
</p>
"
http://arxiv.org/abs/1801.04613,Software Defined Networks based Smart Grid Communication: A Comprehensive Survey. (arXiv:1801.04613v4 [cs.NI] UPDATED),"<p>The current power grid is no longer a feasible solution due to
ever-increasing user demand of electricity, old infrastructure, and reliability
issues and thus require transformation to a better grid a.k.a., smart grid
(SG). The key features that distinguish SG from the conventional electrical
power grid are its capability to perform two-way communication, demand side
management, and real time pricing. Despite all these advantages that SG will
bring, there are certain issues which are specific to SG communication system.
For instance, network management of current SG systems is complex, time
consuming, and done manually. Moreover, SG communication (SGC) system is built
on different vendor specific devices and protocols. Therefore, the current SG
systems are not protocol independent, thus leading to interoperability issue.
Software defined network (SDN) has been proposed to monitor and manage the
communication networks globally. This article serves as a comprehensive survey
on SDN-based SGC. In this article, we first discuss taxonomy of advantages of
SDNbased SGC.We then discuss SDN-based SGC architectures, along with case
studies. Our article provides an in-depth discussion on routing schemes for
SDN-based SGC. We also provide detailed survey of security and privacy schemes
applied to SDN-based SGC. We furthermore present challenges, open issues, and
future research directions related to SDN-based SGC.
</p>
"
http://arxiv.org/abs/1801.07528,Computer-Assisted Proving of Combinatorial Conjectures Over Finite Domains: A Case Study of a Chess Conjecture. (arXiv:1801.07528v4 [cs.LO] UPDATED),"<p>There are several approaches for using computers in deriving mathematical
proofs. For their illustration, we provide an in-depth study of using computer
support for proving one complex combinatorial conjecture -- correctness of a
strategy for the chess KRK endgame. The final, machine verifiable, result
presented in this paper is that there is a winning strategy for white in the
KRK endgame generalized to $n \times n$ board (for natural $n$ greater than
$3$). We demonstrate that different approaches for computer-based theorem
proving work best together and in synergy and that the technology currently
available is powerful enough for providing significant help to humans deriving
complex proofs.
</p>
"
http://arxiv.org/abs/1802.04634,Lattice Functions for the Analysis of Analog-to-Digital Conversion. (arXiv:1802.04634v2 [eess.SP] UPDATED),"<p>Analog-to-digital (A/D) converters are the common interface between analog
signals and the domain of digital discrete-time signal processing. In essence,
this domain simultaneously incorporates quantization both in amplitude and
time, i.e. amplitude quantization and uniform time sampling. Thus, we view A/D
conversion as a sampling process in both the time and amplitude domains based
on the observation that the underlying continuous-time signals representing
digital sequences can be sampled in a lattice---i.e. at points restricted to
lie on a uniform grid both in time and amplitude. We refer to them as lattice
functions. This is in contrast with the traditional approach based on the
classical sampling theorem and quantization error analysis. The latter has been
mainly addressed with the help of probabilistic models, or deterministic ones
either confined to very particular scenarios or considering worst-case
assumptions. In this paper, we provide a deterministic theoretical analysis and
framework for the functions involved in digital discrete-time processing. We
show that lattice functions possess a rich analytic structure in the context of
integral-valued entire functions of exponential type. We derive set and
spectral properties of this class of functions. This allows us to prove in a
deterministic way and for general bandlimited functions a fundamental lower
bound on the maximum frequency component introduced by quantization that is
independent of the resolution of the quantizer.
</p>
"
http://arxiv.org/abs/1802.04672,Delta-Ramp Encoder for Amplitude Sampling and its Interpretation as Time Encoding. (arXiv:1802.04672v2 [eess.SP] UPDATED),"<p>The theoretical basis for conventional acquisition of bandlimited signals
typically relies on uniform time sampling and assumes infinite-precision
amplitude values. In this paper, we explore signal representation and recovery
based on uniform amplitude sampling with assumed infinite precision timing
information. The approach is based on the delta-ramp encoder which consists of
applying a one-level level-crossing detector to the result of adding an
appropriate sawtooth-like waveform to the input signal. The output samples are
the time instants of these level crossings, thus representing a time-encoded
version of the input signal. For theoretical purposes, this system can be
equivalently analyzed by reversibly transforming through ramp addition a
nonmonotonic input signal into a monotonic one which is then uniformly sampled
in amplitude. The monotonic function is then represented by the times at which
the signal crosses a predefined and equally-spaced set of amplitude values. We
refer to this technique as amplitude sampling. The time sequence generated can
be interpreted alternatively as nonuniform time sampling of the original source
signal. We derive duality and frequency-domain properties for the functions
involved in the transformation. Iterative algorithms are proposed and
implemented for recovery of the original source signal. As indicated in the
simulations, the proposed iterative amplitude-sampling algorithm achieves a
faster convergence rate than frame-based reconstruction for nonuniform
sampling. The performance can also be improved by appropriate choice of the
parameters while maintaining the same sampling density.
</p>
"
http://arxiv.org/abs/1803.08988,Evaluating Sentence-Level Relevance Feedback for High-Recall Information Retrieval. (arXiv:1803.08988v2 [cs.IR] UPDATED),"<p>This study uses a novel simulation framework to evaluate whether the time and
effort necessary to achieve high recall using active learning is reduced by
presenting the reviewer with isolated sentences, as opposed to full documents,
for relevance feedback. Under the weak assumption that more time and effort is
required to review an entire document than a single sentence, simulation
results indicate that the use of isolated sentences for relevance feedback can
yield comparable accuracy and higher efficiency, relative to the
state-of-the-art Baseline Model Implementation (BMI) of the AutoTAR Continuous
Active Learning (""CAL"") method employed in the TREC 2015 and 2016 Total Recall
Track.
</p>
"
http://arxiv.org/abs/1805.04772,VAMS: Verifiable Auditing of Access to Confidential Data. (arXiv:1805.04772v4 [cs.CR] UPDATED),"<p>The sharing of personal data has the potential to bring sub-stantial benefits
both to individuals and society, but only if people have confidence that their
data will not be used in-appropriately. As more sensitive data is considered
for sharing (e.g., communication records and medical records) and used to make
important decisions, there is a growing need for transparency in the way that
the data is processed, while protecting the privacy of individuals and the
integrity of their data. We propose a system, VAMS, which allows individuals to
check accesses to their personal data, and enables auditors to detect
violations of policy. Furthermore, our system protects the privacy of
individuals and organizations, while allowing published statistics to be
publicly verified. We demonstrate the practicality of our system with two
prototypes, based on Hyperledger Fabric and Trillian.
</p>
"
http://arxiv.org/abs/1805.10505,Cookie Synchronization: Everything You Always Wanted to Know But Were Afraid to Ask. (arXiv:1805.10505v2 [cs.IR] UPDATED),"<p>User data is the primary input of digital advertising, fueling the free
Internet as we know it. As a result, web companies invest a lot in elaborate
tracking mechanisms to acquire user data that can sell to data markets and
advertisers. However, with same-origin policy, and cookies as a primary
identification mechanism on the web, each tracker knows the same user with a
different ID. To mitigate this, Cookie Synchronization (CSync) came to the
rescue, facilitating an information sharing channel between third parties that
may or not have direct access to the website the user visits. In the
background, with CSync, they merge user data they own, but also reconstruct a
user's browsing history, bypassing the same origin policy. In this paper, we
perform a first to our knowledge in-depth study of CSync in the wild, using a
year-long weblog from 850 real mobile users. Through our study, we aim to
understand the characteristics of the CSync protocol and the impact it has on
web users' privacy. For this, we design and implement CONRAD, a holistic
mechanism to detect CSync events at real time, and the privacy loss on the user
side, even when the synced IDs are obfuscated. Using CONRAD, we find that 97%
of the regular web users are exposed to CSync: most of them within the first
week of their browsing, and the median userID gets leaked, on average, to 3.5
different domains. Finally, we see that CSync increases the number of domains
that track the user by a factor of 6.75.
</p>
"
http://arxiv.org/abs/1807.00056,Fundamental Limits of Decentralized Data Shuffling. (arXiv:1807.00056v3 [cs.IT] UPDATED),"<p>Data shuffling of training data among different computing nodes (workers) has
been identified as a core element to improve the statistical performance of
modern large scale machine learning algorithms. Data shuffling is often
considered as one of the most significant bottlenecks in such systems due to
the heavy communication load. Under a master-worker architecture (where a
master has access to the entire dataset and only communication between the
master and the workers is allowed) coding has been recently proved to
considerably reduce the communication load. This work considers a different
communication paradigm referred to as decentralized data shuffling, where
workers are allowed to communicate with one another via a shared link. The
decentralized data shuffling problem has two phases: workers communicate with
each other during the data shuffling phase, and then workers update their
stored content during the storage phase. For the case of uncoded storage (i.e.,
each worker directly stores a subset of bits of the dataset), this paper
proposes converse and achievable bounds that are to within a factor of 3/2 of
one another. The proposed schemes are also exactly optimal under the constraint
of uncoded storage for either large memory size or at most four workers in the
system. As a by-product, a novel distributed clique-covering scheme is proposed
for distributed broadcast with side information-a setting that includes as
special cases decentralized data shuffling, distributed index coding, and
device-to-device coded caching.
</p>
"
http://arxiv.org/abs/1807.01417,The Implementation of the Colored Abstract Simplicial Complex and its Application to Mesh Generation. (arXiv:1807.01417v2 [cs.MS] UPDATED),"<p>We introduce CASC: a new, modern, and header-only C++ library which provides
a data structure to represent arbitrary dimension abstract simplicial complexes
(ASC) with user-defined classes stored directly on the simplices at each
dimension. This is accomplished by using the latest C++ language features
including variadic template parameters introduced in C++11 and automatic
function return type deduction from C++14. Effectively CASC decouples the
representation of the topology from the interactions of user data. We present
the innovations and design principles of the data structure and related
algorithms. This includes a metadata aware decimation algorithm which is
general for collapsing simplices of any dimension. We also present an example
application of this library to represent an orientable surface mesh.
</p>
"
http://arxiv.org/abs/1807.07648,"On Chebotar\""ev's nonvanishing minors theorem and the Bir\'o-Meshulam-Tao discrete uncertainty principle. (arXiv:1807.07648v2 [math.CA] UPDATED)","<p>Chebotar\""ev's theorem says that every minor of a discrete Fourier matrix of
prime order is nonzero. We prove a generalization of this result that includes
analogues for discrete cosine and discrete sine matrices as special cases. We
then establish a generalization of the Bir\'o-Meshulam-Tao uncertainty
principle to functions with symmetries that arise from certain group actions,
with some of the simplest examples being even and odd functions. We show that
our result is best possible and in some cases is stronger than that of
Bir\'o-Meshulam-Tao. Some of these results hold in certain circumstances for
non-prime fields; Gauss sums play a central role in such investigations.
</p>
"
http://arxiv.org/abs/1807.07978,Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors. (arXiv:1807.07978v3 [stat.ML] UPDATED),"<p>We study the problem of generating adversarial examples in a black-box
setting in which only loss-oracle access to a model is available. We introduce
a framework that conceptually unifies much of the existing work on black-box
attacks, and we demonstrate that the current state-of-the-art methods are
optimal in a natural sense. Despite this optimality, we show how to improve
black-box attacks by bringing a new element into the problem: gradient priors.
We give a bandit optimization-based algorithm that allows us to seamlessly
integrate any such priors, and we explicitly identify and incorporate two
examples. The resulting methods use two to four times fewer queries and fail
two to five times less often than the current state-of-the-art.
</p>
"
http://arxiv.org/abs/1807.10936,Unsupervised Learning of a Hierarchical Spiking Neural Network for Optical Flow Estimation: From Events to Global Motion Perception. (arXiv:1807.10936v2 [cs.CV] UPDATED),"<p>The combination of spiking neural networks and event-based vision sensors
holds the potential of highly efficient and high-bandwidth optical flow
estimation. This paper presents the first hierarchical spiking architecture in
which motion (direction and speed) selectivity emerges in an unsupervised
fashion from the raw stimuli generated with an event-based camera. A novel
adaptive neuron model and stable spike-timing-dependent plasticity formulation
are at the core of this neural network governing its spike-based processing and
learning, respectively. After convergence, the neural architecture exhibits the
main properties of biological visual motion systems, namely feature extraction
and local and global motion perception. Convolutional layers with input
synapses characterized by single and multiple transmission delays are employed
for feature and local motion perception, respectively; while global motion
selectivity emerges in a final fully-connected layer. The proposed solution is
validated using synthetic and real event sequences. Along with this paper, we
provide the cuSNN library, a framework that enables GPU-accelerated simulations
of large-scale spiking neural networks. Source code and samples are available
at https://github.com/tudelft/cuSNN.
</p>
"
http://arxiv.org/abs/1808.00441,Matrix completion and extrapolation via kernel regression. (arXiv:1808.00441v2 [stat.ML] UPDATED),"<p>Matrix completion and extrapolation (MCEX) are dealt with here over
reproducing kernel Hilbert spaces (RKHSs) in order to account for prior
information present in the available data. Aiming at a faster and
low-complexity solver, the task is formulated as a kernel ridge regression. The
resultant MCEX algorithm can also afford online implementation, while the class
of kernel functions also encompasses several existing approaches to MC with
prior information. Numerical tests on synthetic and real datasets show that the
novel approach performs faster than widespread methods such as alternating
least squares (ALS) or stochastic gradient descent (SGD), and that the recovery
error is reduced, especially when dealing with noisy data.
</p>
"
http://arxiv.org/abs/1808.02871,Random directions stochastic approximation with deterministic perturbations. (arXiv:1808.02871v2 [math.OC] UPDATED),"<p>We introduce deterministic perturbation schemes for the recently proposed
random directions stochastic approximation (RDSA) [17], and propose new
first-order and second-order algorithms. In the latter case, these are the
first second-order algorithms to incorporate deterministic perturbations. We
show that the gradient and/or Hessian estimates in the resulting algorithms
with deterministic perturbations are asymptotically unbiased, so that the
algorithms are provably convergent. Furthermore, we derive convergence rates to
establish the superiority of the first-order and second-order algorithms, for
the special case of a convex and quadratic optimization problem, respectively.
Numerical experiments are used to validate the theoretical results.
</p>
"
http://arxiv.org/abs/1808.06570,Detecting cognitive impairments by agreeing on interpretations of linguistic features. (arXiv:1808.06570v3 [cs.CL] UPDATED),"<p>Linguistic features have shown promising applications for detecting various
cognitive impairments. To improve detection accuracies, increasing the amount
of data or the number of linguistic features have been two applicable
approaches. However, acquiring additional clinical data can be expensive, and
hand-crafting features is burdensome. In this paper, we take a third approach,
proposing Consensus Networks (CNs), a framework to classify after reaching
agreements between modalities. We divide linguistic features into
non-overlapping subsets according to their modalities, and let neural networks
learn low-dimensional representations that agree with each other. These
representations are passed into a classifier network. All neural networks are
optimized iteratively.
</p>
<p>In this paper, we also present two methods that improve the performance of
CNs. We then present ablation studies to illustrate the effectiveness of
modality division. To understand further what happens in CNs, we visualize the
representations during training. Overall, using all of the 413 linguistic
features, our models significantly outperform traditional classifiers, which
are used by the state-of-the-art papers.
</p>
"
http://arxiv.org/abs/1808.08765,Identifiability of Complete Dictionary Learning. (arXiv:1808.08765v2 [stat.ML] UPDATED),"<p>Sparse component analysis (SCA), also known as complete dictionary learning,
is the following problem: Given an input matrix $M$ and an integer $r$, find a
dictionary $D$ with $r$ columns and a matrix $B$ with $k$-sparse columns (that
is, each column of $B$ has at most $k$ non-zero entries) such that $M \approx
DB$. A key issue in SCA is identifiability, that is, characterizing the
conditions under which $D$ and $B$ are essentially unique (that is, they are
unique up to permutation and scaling of the columns of $D$ and rows of $B$).
Although SCA has been vastly investigated in the last two decades, only a few
works have tackled this issue in the deterministic scenario, and no work
provides reasonable bounds in the minimum number of samples (that is, columns
of $M$) that leads to identifiability. In this work, we provide new results in
the deterministic scenario when the data has a low-rank structure, that is,
when $D$ is (under)complete. While previous bounds feature a combinatorial term
$r \choose k$, we exhibit a sufficient condition involving
$\mathcal{O}(r^3/(r-k)^2)$ samples that yields an essentially unique
decomposition, as long as these data points are well spread among the subspaces
spanned by $r-1$ columns of $D$. We also exhibit a necessary lower bound on the
number of samples that contradicts previous results in the literature when $k$
equals $r-1$. Our bounds provide a drastic improvement compared to the state of
the art, and imply for example that for a fixed proportion of zeros (constant
and independent of $r$, e.g., 10\% of zero entries in $B$), one only requires
$\mathcal{O}(r)$ data points to guarantee identifiability.
</p>
"
http://arxiv.org/abs/1809.00156,Optimization of Quantum Discord to Connection with Non-commutivity. (arXiv:1809.00156v9 [quant-ph] UPDATED),"<p>In a pair of correlated quantum systems, a measurement in one corresponds to
a change in the state of the other. In the process, information is lost.
Measurement along which set of projectors would accompany minimum loss in
information content is the optimization problem of quantum discord. This
optimization problem needs to be addressed in any computation of discord and is
also an important aspect of our understanding of any quantum to classical
transition. This asks us to explore the standard zero discord condition to move
to a stronger measure that addresses the correlated observables of the m x n
matrix instead. In such a context one could show that discord minimizes at the
diagonal basis of the reduced density matrices and present an analytical
expression of quantum discord that demonstrates how it arises out of
non-commutivity.
</p>
"
http://arxiv.org/abs/1809.01794,Information-Theoretic Privacy For Distributed Average Consensus: Bounded Integral Inputs. (arXiv:1809.01794v2 [cs.SY] UPDATED),"<p>We propose an asynchronous distributed average consensus algorithm that
guarantees information-theoretic privacy of honest agents' inputs against
colluding passive adversarial agents, as long as the set of colluding passive
adversarial agents is not a vertex cut in the underlying communication network.
This implies that a network with $(t+1)$-connectivity guarantees
information-theoretic privacy of honest agents' inputs against any $t$
colluding agents. The proposed protocol is formed by composing a distributed
privacy mechanism we provide with any (non-private) distributed average
consensus algorithm. The agent' inputs are bounded integers, where the bounds
are apriori known to all the agents.
</p>
"
http://arxiv.org/abs/1809.08514,Fundamental Limits of Invisible Flow Fingerprinting. (arXiv:1809.08514v3 [cs.NI] UPDATED),"<p>Network flow fingerprinting can be used to de-anonymize communications on
anonymity systems such as Tor by linking the ingress and egress segments of
anonymized connections. Assume Alice and Bob have access to the input and the
output links of an anonymous network, respectively, and they wish to
collaboratively reveal the connections between the input and the output links
without being detected by Willie who protects the network. Alice generates a
codebook of fingerprints, where each fingerprint corresponds to a unique
sequence of inter-packet delays and shares it only with Bob. For each input
flow, she selects a fingerprint from the codebook and embeds it in the flow,
i.e., changes the packet timings of the flow to follow the packet timings
suggested by the fingerprint, and Bob extracts the fingerprints from the output
flows. We model the network as parallel $M/M/1$ queues where each queue is
shared by a flow from Alice to Bob and other flows independent of the flow from
Alice to Bob. The timings of the flows are governed by independent Poisson
point processes. Assuming all input flows have equal rates and that Bob
observes only flows with fingerprints, we first present two scenarios: 1) Alice
fingerprints all the flows; 2) Alice fingerprints a subset of the flows,
unknown to Willie. Then, we extend the construction and analysis to the case
where flow rates are arbitrary as well as the case where not all the flows that
Bob observes have a fingerprint. For each scenario, we derive the number of
flows that Alice can fingerprint and Bob can trace by fingerprinting.
</p>
"
http://arxiv.org/abs/1809.10243,Segmentation of Skin Lesions and their Attributes Using Multi-Scale Convolutional Neural Networks and Domain Specific Augmentations. (arXiv:1809.10243v2 [cs.CV] UPDATED),"<p>Computer-aided diagnosis systems for classification of different type of skin
lesions have been an active field of research in recent decades. It has been
shown that introducing lesions and their attributes masks into lesion
classification pipeline can greatly improve the performance. In this paper, we
propose a framework by incorporating transfer learning for segmenting lesions
and their attributes based on the convolutional neural networks. The proposed
framework is inspired by the well-known UNet architecture. It utilizes a
variety of pre-trained networks in the encoding path and generates the
prediction map by combining multi-scale information in decoding path using a
pyramid pooling manner. To circumvent the lack of training data and increase
the proposed model generalization, an extensive set of novel augmentation
routines have been applied during the training of the network. Moreover, for
each task of lesion and attribute segmentation, a specific loss function has
been designed to obviate the training phase difficulties. Finally, the
prediction for each task is generated by ensembling the outputs from different
models. The proposed approach achieves promising results on the
cross-validation experiments on the ISIC2018- Task1 and Task2 data sets.
</p>
"
http://arxiv.org/abs/1810.01898,A Multi-Face Challenging Dataset for Robust Face Recognition. (arXiv:1810.01898v2 [cs.CV] UPDATED),"<p>Face recognition in images is an active area of interest among the computer
vision researchers. However, recognizing human face in an unconstrained
environment, is a relatively less-explored area of research. Multiple face
recognition in unconstrained environment is a challenging task, due to the
variation of view-point, scale, pose, illumination and expression of the face
images. Partial occlusion of faces makes the recognition task even more
challenging. The contribution of this paper is two-folds: introducing a
challenging multiface dataset (i.e., IIITS MFace Dataset) for face recognition
in unconstrained environment and evaluating the performance of state-of-the-art
hand-designed and deep learning based face descriptors on the dataset. The
proposed IIITS MFace dataset contains faces with challenges like pose
variation, occlusion, mask, spectacle, expressions, change of illumination,
etc. We experiment with several state-of-the-art face descriptors, including
recent deep learning based face descriptors like VGGFace, and compare with the
existing benchmark face datasets. Results of the experiments clearly show that
the difficulty level of the proposed dataset is much higher compared to the
benchmark datasets.
</p>
"
http://arxiv.org/abs/1810.05947,Robust Model Predictive Control of Irrigation Systems with Active Uncertainty Learning and Data Analytics. (arXiv:1810.05947v2 [cs.SY] UPDATED),"<p>We develop a novel data-driven robust model predictive control (DDRMPC)
approach for automatic control of irrigation systems. The fundamental idea is
to integrate both mechanistic models, which describe dynamics in soil moisture
variations, and data-driven models, which characterize uncertainty in forecast
errors of evapotranspiration and precipitation, into a holistic systems control
framework. To better capture the support of uncertainty distribution, we take a
new learning-based approach by constructing uncertainty sets from historical
data. For evapotranspiration forecast error, the support vector
clustering-based uncertainty set is adopted, which can be conveniently built
from historical data. As for precipitation forecast errors, we analyze the
dependence of their distribution on forecast values, and further design a
tailored uncertainty set based on the properties of this type of uncertainty.
In this way, the overall uncertainty distribution can be elaborately described,
which finally contributes to rational and efficient control decisions. To
assure the quality of data-driven uncertainty sets, a training-calibration
scheme is used to provide theoretical performance guarantees. A generalized
affine decision rule is adopted to obtain tractable approximations of optimal
control problems, thereby ensuring the practicability of DDRMPC. Case studies
using real data show that, DDRMPC can reliably maintain soil moisture above the
safety level and avoid crop devastation. The proposed DDRMPC approach leads to
a 40\% reduction of total water consumption compared to the fine-tuned
open-loop control strategy. In comparison with the carefully tuned rule-based
control and certainty equivalent model predictive control, the proposed DDRMPC
approach can significantly reduce the total water consumption and improve the
control performance.
</p>
"
http://arxiv.org/abs/1811.03433,Explainable cardiac pathology classification on cine MRI with motion characterization by semi-supervised learning of apparent flow. (arXiv:1811.03433v2 [cs.CV] UPDATED),"<p>We propose a method to classify cardiac pathology based on a novel approach
to extract image derived features to characterize the shape and motion of the
heart. An original semi-supervised learning procedure, which makes efficient
use of a large amount of non-segmented images and a small amount of images
segmented manually by experts, is developed to generate pixel-wise apparent
flow between two time points of a 2D+t cine MRI image sequence. Combining the
apparent flow maps and cardiac segmentation masks, we obtain a local apparent
flow corresponding to the 2D motion of myocardium and ventricular cavities.
This leads to the generation of time series of the radius and thickness of
myocardial segments to represent cardiac motion. These time series of motion
features are reliable and explainable characteristics of pathological cardiac
motion. Furthermore, they are combined with shape-related features to classify
cardiac pathologies. Using only nine feature values as input, we propose an
explainable, simple and flexible model for pathology classification. On ACDC
training set and testing set, the model achieves 95% and 94% respectively as
classification accuracy. Its performance is hence comparable to that of the
state-of-the-art. Comparison with various other models is performed to outline
some advantages of our model.
</p>
"
http://arxiv.org/abs/1811.08383,TSM: Temporal Shift Module for Efficient Video Understanding. (arXiv:1811.08383v2 [cs.CV] UPDATED),"<p>The explosive growth in video streaming gives rise to challenges on
efficiently extracting the spatial-temporal information to perform video
understanding at low computation cost. Conventional 2D CNNs are computationally
cheap but cannot capture temporal relationships; 3D CNN based methods can
achieve good performance but are computationally intensive, making it expensive
to deploy. In this paper, we propose a generic and effective Temporal Shift
Module (TSM) that enjoys both high efficiency and high performance.
Specifically, it can achieve the performance of 3D CNN but maintain 2D CNN's
complexity. TSM shifts part of the channels along the temporal dimension, thus
facilitate information exchanged among neighboring frames. It can be inserted
into 2D CNNs to achieve temporal modeling at zero computation and zero
parameters. We also extended TSM to online video recognition setting, which
enables real-time low-latency online video recognition. On the
Something-Something-V1 dataset which focuses on temporal modeling, we achieved
better results than I3D family and ECO family using 6X and 2.7X fewer FLOPs
respectively. Measured on P100 GPU, our single model achieved 1.8% higher
accuracy at 9.5X lower latency and 12.7X higher throughput compared to I3D. The
code is available here: https://github.com/MIT-HAN-LAB/temporal-shift-module.
</p>
"
http://arxiv.org/abs/1811.12019,Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks. (arXiv:1811.12019v4 [cs.LG] UPDATED),"<p>Large-scale distributed training of deep neural networks suffer from the
generalization gap caused by the increase in the effective mini-batch size.
Previous approaches try to solve this problem by varying the learning rate and
batch size over epochs and layers, or some ad hoc modification of the batch
normalization. We propose an alternative approach using a second-order
optimization method that shows similar generalization capability to first-order
methods, but converges faster and can handle larger mini-batches. To test our
method on a benchmark where highly optimized first-order methods are available
as references, we train ResNet-50 on ImageNet. We converged to 75% Top-1
validation accuracy in 35 epochs for mini-batch sizes under 16,384, and
achieved 75% even with a mini-batch size of 131,072, which took only 978
iterations.
</p>
"
http://arxiv.org/abs/1812.00020,TextureNet: Consistent Local Parametrizations for Learning from High-Resolution Signals on Meshes. (arXiv:1812.00020v2 [cs.CV] UPDATED),"<p>We introduce, TextureNet, a neural network architecture designed to extract
features from high-resolution signals associated with 3D surface meshes (e.g.,
color texture maps). The key idea is to utilize a 4-rotational symmetric
(4-RoSy) field to define a domain for convolution on a surface. Though 4-RoSy
fields have several properties favorable for convolution on surfaces (low
distortion, few singularities, consistent parameterization, etc.), orientations
are ambiguous up to 4-fold rotation at any sample point. So, we introduce a new
convolutional operator invariant to the 4-RoSy ambiguity and use it in a
network to extract features from high-resolution signals on geodesic
neighborhoods of a surface. In comparison to alternatives, such as PointNet
based methods which lack a notion of orientation, the coherent structure given
by these neighborhoods results in significantly stronger features. As an
example application, we demonstrate the benefits of our architecture for 3D
semantic segmentation of textured 3D meshes. The results show that our method
outperforms all existing methods on the basis of mean IoU by a significant
margin in both geometry-only (6.4%) and RGB+Geometry (6.9-8.2%) settings.
</p>
"
http://arxiv.org/abs/1812.00876,Deep Learning based Pedestrian Detection at Distance in Smart Cities. (arXiv:1812.00876v3 [cs.CV] UPDATED),"<p>Generative adversarial networks (GANs) have been promising for many computer
vision problems due to their powerful capabilities to enhance the data for
training and test. In this paper, we leveraged GANs and proposed a new
architecture with a cascaded Single Shot Detector (SSD) for pedestrian
detection at distance, which is yet a challenge due to the varied sizes of
pedestrians in videos at distance. To overcome the low-resolution issues in
pedestrian detection at distance, DCGAN is employed to improve the resolution
first to reconstruct more discriminative features for a SSD to detect objects
in images or videos. A crucial advantage of our method is that it learns a
multi-scale metric to distinguish multiple objects at different distances under
one image, while DCGAN serves as an encoder-decoder platform to generate parts
of an image that contain better discriminative information. To measure the
effectiveness of our proposed method, experiments were carried out on the
Canadian Institute for Advanced Research (CIFAR) dataset, and it was
demonstrated that the proposed new architecture achieved a much better
detection rate, particularly on vehicles and pedestrians at distance, making it
highly suitable for smart cities applications that need to discover key objects
or pedestrians at distance.
</p>
"
http://arxiv.org/abs/1812.02222,Predicting pregnancy using large-scale data from a women's health tracking mobile application. (arXiv:1812.02222v2 [stat.AP] UPDATED),"<p>Predicting pregnancy has been a fundamental problem in women's health for
more than 50 years. Previous datasets have been collected via carefully curated
medical studies, but the recent growth of women's health tracking mobile apps
offers potential for reaching a much broader population. However, the
feasibility of predicting pregnancy from mobile health tracking data is
unclear. Here we develop four models -- a logistic regression model, and 3 LSTM
models -- to predict a woman's probability of becoming pregnant using data from
a women's health tracking app, Clue by BioWink GmbH. Evaluating our models on a
dataset of 79 million logs from 65,276 women with ground truth pregnancy test
data, we show that our predicted pregnancy probabilities meaningfully stratify
women: women in the top 10% of predicted probabilities have a 89% chance of
becoming pregnant over 6 menstrual cycles, as compared to a 27% chance for
women in the bottom 10%. We develop a technique for extracting interpretable
time trends from our deep learning models, and show these trends are consistent
with previous fertility research. Our findings illustrate the potential that
women's health tracking data offers for predicting pregnancy on a broader
population; we conclude by discussing the steps needed to fulfill this
potential.
</p>
"
http://arxiv.org/abs/1812.02347,Counterfactual Critic Multi-Agent Training for Scene Graph Generation. (arXiv:1812.02347v2 [cs.CV] UPDATED),"<p>Scene graphs -- objects as nodes and visual relationships as edges --
describe the whereabouts and interactions of the things and stuff in an image
for comprehensive scene understanding. To generate coherent scene graphs,
almost all existing methods exploit the fruitful visual context by modeling
message passing among objects, fitting the dynamic nature of reasoning with
visual context, eg, ""person"" on ""bike"" can help determine the relationship
""ride"", which in turn contributes to the category confidence of the two
objects. However, we argue that the scene dynamics is not properly learned by
using the prevailing cross-entropy based supervised learning paradigm, which is
not sensitive to graph inconsistency: errors at the hub or non-hub nodes are
unfortunately penalized equally. To this end, we propose a Counterfactual
critic Multi-Agent Training (CMAT) approach to resolve the mismatch. CMAT is a
multi-agent policy gradient method that frames objects as cooperative agents,
and then directly maximizes a graph-level metric as the reward. In particular,
to assign the reward properly to each agent, CMAT uses a counterfactual
baseline that disentangles the agent-specific reward by fixing the dynamics of
other agents. Extensive validations on the challenging Visual Genome benchmark
show that CMAT achieves a state-of-the-art by significant performance gains
under various settings and metrics.
</p>
"
http://arxiv.org/abs/1812.03831,On the Enumeration Complexity of Unions of Conjunctive Queries. (arXiv:1812.03831v3 [cs.DB] UPDATED),"<p>We study the enumeration complexity of Unions of Conjunctive Queries (UCQs).
We aim to identify the UCQs that are tractable in the sense that the answer
tuples can be enumerated with a linear preprocessing phase and a constant delay
between every successive tuples. It has been established that, in the absence
of self joins and under conventional complexity assumptions, the CQs that admit
such an evaluation are precisely the free-connex ones. A union of tractable CQs
is always tractable. We generalize the notion of free-connexity from CQs to
UCQs, thus showing that some unions containing intractable CQs are, in fact,
tractable. Interestingly, some unions consisting of only intractable CQs are
tractable too. The question of a finding a full characterization of the
tractability of UCQs remains open. Nevertheless, we prove that for several
classes of queries, free-connexity fully captures the tractable UCQs.
</p>
"
http://arxiv.org/abs/1812.08252,Towards an Evolvable Cancer Treatment Simulator. (arXiv:1812.08252v2 [cs.NE] UPDATED),"<p>The use of high-fidelity computational simulations promises to enable
high-throughput hypothesis testing and optimisation of cancer therapies.
However, increasing realism comes at the cost of increasing computational
requirements. This article explores the use of surrogate-assisted evolutionary
algorithms to optimise the targeted delivery of a therapeutic compound to
cancerous tumour cells with the multicellular simulator, PhysiCell. The use of
both Gaussian process models and multi-layer perceptron neural network
surrogate models are investigated. We find that evolutionary algorithms are
able to effectively explore the parameter space of biophysical properties
within the agent-based simulations, minimising the resulting number of
cancerous cells after a period of simulated treatment. Both model-assisted
algorithms are found to outperform a standard evolutionary algorithm,
demonstrating their ability to perform a more effective search within the very
small evaluation budget. This represents the first use of efficient
evolutionary algorithms within a high-throughput multicellular computing
approach to find therapeutic design optima that maximise tumour regression.
</p>
"
http://arxiv.org/abs/1812.10085,A Data-driven Adversarial Examples Recognition Framework via Adversarial Feature Genome. (arXiv:1812.10085v2 [cs.CV] UPDATED),"<p>Convolutional neural networks (CNNs) are easily spoofed by adversarial
examples which lead to wrong classification results. Most of the defense
methods focus only on how to improve the robustness of CNNs or to detect
adversarial examples. They are incapable of detecting and correctly classifying
adversarial examples simultaneously. We find that adversarial examples and
original images have diverse representations in the feature space, and this
difference grows as layers go deeper, which we call Adversarial Feature
Separability (AFS). Inspired by AFS, we propose a defense framework based on
Adversarial Feature Genome (AFG), which can detect and correctly classify
adversarial examples into original classes simultaneously. AFG is an innovative
encoding for both image and adversarial example. It consists of group features
and a mixed label. With group features which are visual representations of
adversarial and original images via group visualization method, one can detect
adversarial examples because of ASF of group features. With a mixed label, one
can trace back to the original label of an adversarial example. Then, the
classification of adversarial example is modeled as a multi-label
classification trained on the AFG dataset, which can get the original class of
adversarial example. Experiments show that the proposed framework not only
effectively detects adversarial examples from different attack algorithms, but
also correctly classifies adversarial examples. Our framework potentially gives
a new perspective, i.e., a data-driven way, to improve the robustness of a CNN
model.
</p>
"
http://arxiv.org/abs/1812.11448,Removing Malicious Nodes from Networks. (arXiv:1812.11448v5 [cs.SI] UPDATED),"<p>A fundamental challenge in networked systems is detection and removal of
suspected malicious nodes. In reality, detection is always imperfect, and the
decision about which potentially malicious nodes to remove must trade off false
positives (erroneously removing benign nodes) and false negatives (mistakenly
failing to remove malicious nodes). However, in network settings this
conventional tradeoff must now account for node connectivity. In particular,
malicious nodes may exert malicious influence, so that mistakenly leaving some
of these in the network may cause damage to spread. On the other hand, removing
benign nodes causes direct harm to these, and indirect harm to their benign
neighbors who would wish to communicate with them.
</p>
<p>We formalize the problem of removing potentially malicious nodes from a
network under uncertainty through an objective that takes connectivity into
account. We show that optimally solving the resulting problem is NP-Hard. We
then propose a tractable solution approach based on a convex relaxation of the
objective. Finally, we experimentally demonstrate that our approach
significantly outperforms both a simple baseline that ignores network
structure, as well as a state-of-the-art approach for a related problem, on
both synthetic and real-world datasets.
</p>
"
http://arxiv.org/abs/1901.00434,The capacity of feedforward neural networks. (arXiv:1901.00434v2 [cs.LG] UPDATED),"<p>A long standing open problem in the theory of neural networks is the
development of quantitative methods to estimate and compare the capabilities of
different architectures. Here we define the capacity of an architecture by the
binary logarithm of the number of functions it can compute, as the synaptic
weights are varied. The capacity provides an upper bound on the number of bits
that can be extracted from the training data and stored in the architecture
during learning. We study the capacity of layered, fully-connected,
architectures of linear threshold neurons with $L$ layers of size $n_1,n_2,
\ldots, n_L$ and show that in essence the capacity is given by a cubic
polynomial in the layer sizes: $C(n_1,\ldots, n_L)=\sum_{k=1}^{L-1}
\min(n_1,\ldots,n_k)n_kn_{k+1}$, where layers that are smaller than all
previous layers act as bottlenecks. In proving the main result, we also develop
new techniques (multiplexing, enrichment, and stacking) as well as new bounds
on the capacity of finite sets. We use the main result to identify
architectures with maximal or minimal capacity under a number of natural
constraints. This leads to the notion of structural regularization for deep
architectures. While in general, everything else being equal, shallow networks
compute more functions than deep networks, the functions computed by deep
networks are more regular and ""interesting"".
</p>
"
http://arxiv.org/abs/1901.01660,Deeper and Wider Siamese Networks for Real-Time Visual Tracking. (arXiv:1901.01660v3 [cs.CV] UPDATED),"<p>Siamese networks have drawn great attention in visual tracking because of
their balanced accuracy and speed. However, the backbone networks used in
Siamese trackers are relatively shallow, such as AlexNet [18], which does not
fully take advantage of the capability of modern deep neural networks. In this
paper, we investigate how to leverage deeper and wider convolutional neural
networks to enhance tracking robustness and accuracy. We observe that direct
replacement of backbones with existing powerful architectures, such as ResNet
[14] and Inception [33], does not bring improvements. The main reasons are that
1)large increases in the receptive field of neurons lead to reduced feature
discriminability and localization precision; and 2) the network padding for
convolutions induces a positional bias in learning. To address these issues, we
propose new residual modules to eliminate the negative impact of padding, and
further design new architectures using these modules with controlled receptive
field size and network stride. The designed architectures are lightweight and
guarantee real-time tracking speed when applied to SiamFC [2] and SiamRPN [20].
Experiments show that solely due to the proposed network architectures, our
SiamFC+ and SiamRPN+ obtain up to 9.8%/5.7% (AUC), 23.3%/8.8% (EAO) and
24.4%/25.0% (EAO) relative improvements over the original versions [2, 20] on
the OTB-15, VOT-16 and VOT-17 datasets, respectively.
</p>
"
http://arxiv.org/abs/1901.01894,Resilient Design of 5G Mobile-Edge Computing Over Intermittent mmWave Links. (arXiv:1901.01894v3 [cs.IT] UPDATED),"<p>Two enablers of the 5th Generation (5G) of mobile communication systems are
the high data rates achievable with millimeter-wave radio signals and the
cloudification of the network's mobile edge, made possible also by Multi-access
Edge Computing (MEC). In 5G networks, user devices may exploit the high
capacity of their mobile connection and the computing capabilities of the edge
cloud to offload computational tasks to MEC servers, which run applications on
devices' behalf. This paper investigates new methods to perform power- and
latency-constrained offloading. First, aiming to minimize user devices'
transmit power, the opportunity to exploit concurrent communication links
between the device and the edge cloud is studied. The optimal number of
channels for simultaneous transmission is characterized in a deterministic and
a probabilistic scenario. Subsequently, blocking events that obstruct
millimeter-wave channels making them `intermittent' are considered. Resource
overprovisioning and error-correcting codes against asymmetric block erasures
are proposed to jointly contrast blocking and exploit multi-link
communications' diversity. The asymmetric block-erasure channel is
characterized by a study of its outage probability. The analysis is performed
in a framework that yields closed-form expressions. These, together with
corroborating numerical results, are intended to provide reference points and
bounds to optimal performance in practical applications.
</p>
"
http://arxiv.org/abs/1901.02360,Sum-of-square-of-rational-function based representations of positive semidefinite polynomial matrices. (arXiv:1901.02360v2 [math.OC] UPDATED),"<p>The paper proves sum-of-square-of-rational-function based representations
(shortly, sosrf-based representations) of polynomial matrices that are positive
semidefinite on some special sets: $\mathbb{R}^n;$ $\mathbb{R}$ and its
intervals $[a,b]$, $[0,\infty)$; and the strips $[a,b] \times \mathbb{R}
\subset \mathbb{R}^2.$ A method for numerically computing such representations
is also presented. The methodology is divided into two stages:
</p>
<p>(S1) diagonalizing the initial polynomial matrix based on the Schm\""{u}dgen's
procedure \cite{Schmudgen09};
</p>
<p>(S2) for each diagonal element of the resulting matrix, find its low rank
sosrf-representation satisfying the Artin's theorem solving the Hilbert's 17th
problem.
</p>
<p>Some numerical tests and illustrations with \textsf{OCTAVE} are also
presented for each type of polynomial matrices.
</p>
"
http://arxiv.org/abs/1901.04240,Semi-supervised Learning with Graphs: Covariance Based Superpixels For Hyperspectral Image Classification. (arXiv:1901.04240v3 [cs.CV] UPDATED),"<p>In this paper, we present a graph-based semi-supervised framework for
hyperspectral image classification. We first introduce a novel superpixel
algorithm based on the spectral covariance matrix representation of pixels to
provide a better representation of our data. We then construct a superpixel
graph, based on carefully considered feature vectors, before performing
classification. We demonstrate, through a set of experimental results using two
benchmarking datasets, that our approach outperforms three state-of-the-art
classification frameworks, especially when an extremely small amount of
labelled data is used.
</p>
"
http://arxiv.org/abs/1901.06523,Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks. (arXiv:1901.06523v3 [cs.LG] UPDATED),"<p>We study the training process of Deep Neural Networks (DNNs) from the Fourier
analysis perspective. Our starting point is a Frequency Principle (F-Principle)
--- DNNs initialized with small parameters often fit target functions from low
to high frequencies --- which was first proposed by Xu et al. (2018) and
Rahaman et al. (2018) on synthetic datasets. In this work, we first show the
universality of the F-Principle by demonstrating this phenomenon on
high-dimensional benchmark datasets, such as MNIST and CIFAR10. Then, based on
experiments, we show that the F-Principle provides insight into both the
success and failure of DNNs in different types of problems. Based on the
F-Principle, we further propose that DNN can be adopted to accelerate the
convergence of low frequencies for scientific computing problems, in which most
of the conventional methods (e.g., Jacobi method) exhibit the opposite
convergence behavior --- faster convergence for higher frequencies. Finally, we
prove a theorem for DNNs of one hidden layer as a first step towards a
mathematical explanation of the F-Principle. Our work indicates that the
F-Principle with Fourier analysis is a promising approach to the study of DNNs
because it seems ubiquitous, applicable, and explainable.
</p>
"
http://arxiv.org/abs/1901.06731,Four Deviations Suffice for Rank 1 Matrices. (arXiv:1901.06731v2 [math.CO] UPDATED),"<p>We prove a matrix discrepancy bound that strengthens the famous
Kadison-Singer result of Marcus, Spielman, and Srivastava. Consider any
independent scalar random variables $\xi_1, \ldots, \xi_n$ with finite support,
e.g.
</p>
<p>$\{ \pm 1 \}$ or $\{ 0,1 \}$-valued random variables, or some combination
thereof. Let $u_1, \dots, u_n \in \mathbb{C}^m$ and $$ \sigma^2 = \left\|
\sum_{i=1}^n \text{Var}[ \xi_i ] (u_i u_i^{*})^2 \right\|. $$ Then there exists
a choice of outcomes $\varepsilon_1,\ldots,\varepsilon_n$ in the support of
$\xi_1, \ldots, \xi_n$ s.t. $$ \left \|\sum_{i=1}^n \mathbb{E} [ \xi_i] u_i
u_i^* - \sum_{i=1}^n \varepsilon_i u_i u_i^* \right \| \leq 4 \sigma. $$ A
simple consequence of our result is an improvement of a Lyapunov-type theorem
of Akemann and Weaver.
</p>
"
http://arxiv.org/abs/1901.07298,Online Estimation of Multiple Dynamic Graphs in Pattern Sequences. (arXiv:1901.07298v2 [stat.ML] UPDATED),"<p>Sequences of correlated binary patterns can represent many time-series data
including text, movies, and biological signals. These patterns may be described
by weighted combinations of a few dominant structures that underpin specific
interactions among the binary elements. To extract the dominant correlation
structures and their contributions to generating data in a time-dependent
manner, we model the dynamics of binary patterns using the state-space model of
an Ising-type network that is composed of multiple undirected graphs. We
provide a sequential Bayes algorithm to estimate the dynamics of weights on the
graphs while gaining the graph structures online. This model can uncover
overlapping graphs underlying the data better than a traditional orthogonal
decomposition method, and outperforms an original time-dependent Ising model.
We assess the performance of the method by simulated data, and demonstrate that
spontaneous activity of cultured hippocampal neurons is represented by dynamics
of multiple graphs.
</p>
"
http://arxiv.org/abs/1901.08991,Diffusion Variational Autoencoders. (arXiv:1901.08991v2 [cs.LG] UPDATED),"<p>A standard Variational Autoencoder, with a Euclidean latent space, is
structurally incapable of capturing topological properties of certain datasets.
To remove topological obstructions, we introduce Diffusion Variational
Autoencoders with arbitrary manifolds as a latent space. A Diffusion
Variational Autoencoder uses transition kernels of Brownian motion on the
manifold. In particular, it uses properties of the Brownian motion to implement
the reparametrization trick and fast approximations to the KL divergence. We
show that the Diffusion Variational Autoencoder is capable of capturing
topological properties of synthetic datasets. Additionally, we train MNIST on
spheres, tori, projective spaces, SO(3), and a torus embedded in R3. Although a
natural dataset like MNIST does not have latent variables with a clear-cut
topological structure, training it on a manifold can still highlight
topological and geometrical properties.
</p>
"
http://arxiv.org/abs/1901.09221,Progressive Image Deraining Networks: A Better and Simpler Baseline. (arXiv:1901.09221v2 [cs.CV] UPDATED),"<p>Along with the deraining performance improvement of deep networks, their
structures and learning become more and more complicated and diverse, making it
difficult to analyze the contribution of various network modules when
developing new deraining networks. To handle this issue, this paper provides a
better and simpler baseline deraining network by considering network
architecture, input and output, and loss functions. Specifically, by repeatedly
unfolding a shallow ResNet, progressive ResNet (PRN) is proposed to take
advantage of recursive computation. A recurrent layer is further introduced to
exploit the dependencies of deep features across stages, forming our
progressive recurrent network (PReNet). Furthermore, intra-stage recursive
computation of ResNet can be adopted in PRN and PReNet to notably reduce
network parameters with graceful degradation in deraining performance. For
network input and output, we take both stage-wise result and original rainy
image as input to each ResNet and finally output the prediction of {residual
image}. As for loss functions, single MSE or negative SSIM losses are
sufficient to train PRN and PReNet. Experiments show that PRN and PReNet
perform favorably on both synthetic and real rainy images. Considering its
simplicity, efficiency and effectiveness, our models are expected to serve as a
suitable baseline in future deraining research. The source codes are available
at https://github.com/csdwren/PReNet.
</p>
"
http://arxiv.org/abs/1901.10593,Decentralized Online Learning: Take Benefits from Others' Data without Sharing Your Own to Track Global Trend. (arXiv:1901.10593v3 [cs.LG] UPDATED),"<p>Decentralized Online Learning (online learning in decentralized networks)
attracts more and more attention, since it is believed that Decentralized
Online Learning can help the data providers cooperatively better solve their
online problems without sharing their private data to a third party or other
providers. Typically, the cooperation is achieved by letting the data providers
exchange their models between neighbors, e.g., recommendation model. However,
the best regret bound for a decentralized online learning algorithm is
$\Ocal{n\sqrt{T}}$, where $n$ is the number of nodes (or users) and $T$ is the
number of iterations. This is clearly insignificant since this bound can be
achieved \emph{without} any communication in the networks. This reminds us to
ask a fundamental question: \emph{Can people really get benefit from the
decentralized online learning by exchanging information?} In this paper, we
studied when and why the communication can help the decentralized online
learning to reduce the regret. Specifically, each loss function is
characterized by two components: the adversarial component and the stochastic
component. Under this characterization, we show that decentralized online
gradient (DOG) enjoys a regret bound $\Ocal{n\sqrt{T}G + \sqrt{nT}\sigma}$,
where $G$ measures the magnitude of the adversarial component in the private
data (or equivalently the local loss function) and $\sigma$ measures the
randomness within the private data. This regret suggests that people can get
benefits from the randomness in the private data by exchanging private
information. Another important contribution of this paper is to consider the
dynamic regret -- a more practical regret to track users' interest dynamics.
Empirical studies are also conducted to validate our analysis.
</p>
"
http://arxiv.org/abs/1901.10915,Benchmarking Classic and Learned Navigation in Complex 3D Environments. (arXiv:1901.10915v2 [cs.CV] UPDATED),"<p>Navigation research is attracting renewed interest with the advent of
learning-based methods. However, this new line of work is largely disconnected
from well-established classic navigation approaches. In this paper, we take a
step towards coordinating these two directions of research. We set up classic
and learning-based navigation systems in common simulated environments and
thoroughly evaluate them in indoor spaces of varying complexity, with access to
different sensory modalities. Additionally, we measure human performance in the
same environments. We find that a classic pipeline, when properly tuned, can
perform very well in complex cluttered environments. On the other hand, learned
systems can operate more robustly with a limited sensor suite. Overall, both
approaches are still far from human-level performance.
</p>
"
http://arxiv.org/abs/1902.03025,Parameterized Analysis of Immediate Observation Petri Nets. (arXiv:1902.03025v2 [cs.LO] UPDATED),"<p>We introduce immediate observation Petri nets, a class of interest in the
study of population protocols (a model of distributed computation), and
enzymatic chemical networks. In these areas, relevant analysis questions
translate into parameterized Petri net problems: whether an infinite set of
Petri nets with the same underlying net, but different initial markings,
satisfy a given property. We study the parameterized reachability,
coverability, and liveness problems for immediate observation Petri nets. We
show that all three problems are in PSPACE for infinite sets of initial
markings defined by counting constraints, a class sufficiently rich for the
intended application. This is remarkable, since the problems are already
PSPACE-hard when the set of markings is a singleton, i.e., in the
non-parameterized case. We use these results to prove that the correctness
problem for immediate observation population protocols is PSPACE-complete,
answering a question left open in a previous paper.
</p>
"
http://arxiv.org/abs/1902.08647,Better Algorithms for Stochastic Bandits with Adversarial Corruptions. (arXiv:1902.08647v2 [cs.LG] UPDATED),"<p>We study the stochastic multi-armed bandits problem in the presence of
adversarial corruption. We present a new algorithm for this problem whose
regret is nearly optimal, substantially improving upon previous work. Our
algorithm is agnostic to the level of adversarial contamination and can
tolerate a significant amount of corruption with virtually no degradation in
performance.
</p>
"
http://arxiv.org/abs/1902.10068,Entity Recognition at First Sight: Improving NER with Eye Movement Information. (arXiv:1902.10068v2 [cs.CL] UPDATED),"<p>Previous research shows that eye-tracking data contains information about the
lexical and syntactic properties of text, which can be used to improve natural
language processing models. In this work, we leverage eye movement features
from three corpora with recorded gaze information to augment a state-of-the-art
neural model for named entity recognition (NER) with gaze embeddings. These
corpora were manually annotated with named entity labels. Moreover, we show how
gaze features, generalized on word type level, eliminate the need for recorded
eye-tracking data at test time. The gaze-augmented models for NER using
token-level and type-level features outperform the baselines. We present the
benefits of eye-tracking features by evaluating the NER models on both
individual datasets as well as in cross-domain settings.
</p>
"
http://arxiv.org/abs/1903.01784,Leveraging Shape Completion for 3D Siamese Tracking. (arXiv:1903.01784v2 [cs.CV] UPDATED),"<p>Point clouds are challenging to process due to their sparsity, therefore
autonomous vehicles rely more on appearance attributes than pure geometric
features. However, 3D LIDAR perception can provide crucial information for
urban navigation in challenging light or weather conditions. In this paper, we
investigate the versatility of Shape Completion for 3D Object Tracking in LIDAR
point clouds. We design a Siamese tracker that encodes model and candidate
shapes into a compact latent representation. We regularize the encoding by
enforcing the latent representation to decode into an object model shape. We
observe that 3D object tracking and 3D shape completion complement each other.
Learning a more meaningful latent representation shows better discriminatory
capabilities, leading to improved tracking performance. We test our method on
the KITTI Tracking set using car 3D bounding boxes. Our model reaches a 76.94%
Success rate and 81.38% Precision for 3D Object Tracking, with the shape
completion regularization leading to an improvement of 3% in both metrics.
</p>
"
http://arxiv.org/abs/1903.03404,Accelerating Generalized Linear Models with MLWeaving: A One-Size-Fits-All System for Any-precision Learning (Technical Report). (arXiv:1903.03404v2 [cs.DS] UPDATED),"<p>Learning from the data stored in a database is an important function
increasingly available in relational engines. Methods using lower precision
input data are of special interest given their overall higher efficiency but,
in databases, these methods have a hidden cost: the quantization of the real
value into a smaller number is an expensive step. To address the issue, in this
paper we present MLWeaving, a data structure and hardware acceleration
technique intended to speed up learning of generalized linear models in
databases. ML-Weaving provides a compact, in-memory representation enabling the
retrieval of data at any level of precision. MLWeaving also takes advantage of
the increasing availability of FPGA-based accelerators to provide a highly
efficient implementation of stochastic gradient descent. The solution adopted
in MLWeaving is more efficient than existing designs in terms of space (since
it can process any resolution on the same design) and resources (via the use of
bit-serial multipliers). MLWeaving also enables the runtime tuning of
precision, instead of a fixed precision level during the training. We
illustrate this using a simple, dynamic precision schedule. Experimental
results show MLWeaving achieves up to16 performance improvement over
low-precision CPU implementations of first-order methods.
</p>
"
http://arxiv.org/abs/1903.04448,Pragmatic inference and visual abstraction enable contextual flexibility during visual communication. (arXiv:1903.04448v2 [cs.OH] UPDATED),"<p>Visual modes of communication are ubiquitous in modern life --- from maps to
data plots to political cartoons. Here we investigate drawing, the most basic
form of visual communication. Participants were paired in an online environment
to play a drawing-based reference game. On each trial, both participants were
shown the same four objects, but in different locations. The sketcher's goal
was to draw one of these objects so that the viewer could select it from the
array. On `close' trials, objects belonged to the same basic-level category,
whereas on `far' trials objects belonged to different categories. We found that
people exploited shared information to efficiently communicate about the target
object: on far trials, sketchers achieved high recognition accuracy while
applying fewer strokes, using less ink, and spending less time on their
drawings than on close trials. We hypothesized that humans succeed in this task
by recruiting two core faculties: visual abstraction, the ability to perceive
the correspondence between an object and a drawing of it; and pragmatic
inference, the ability to judge what information would help a viewer
distinguish the target from distractors. To evaluate this hypothesis, we
developed a computational model of the sketcher that embodied both faculties,
instantiated as a deep convolutional neural network nested within a
probabilistic program. We found that this model fit human data well and
outperformed lesioned variants. Together, this work provides the first
algorithmically explicit theory of how visual perception and social cognition
jointly support contextual flexibility in visual communication.
</p>
"
http://arxiv.org/abs/1903.04596,Quality-Gated Convolutional LSTM for Enhancing Compressed Video. (arXiv:1903.04596v2 [cs.CV] UPDATED),"<p>The past decade has witnessed great success in applying deep learning to
enhance the quality of compressed video. However, the existing approaches aim
at quality enhancement on a single frame, or only using fixed neighboring
frames. Thus they fail to take full advantage of the inter-frame correlation in
the video. This paper proposes the Quality-Gated Convolutional Long Short-Term
Memory (QG-ConvLSTM) network with bi-directional recurrent structure to fully
exploit the advantageous information in a large range of frames. More
importantly, due to the obvious quality fluctuation among compressed frames,
higher quality frames can provide more useful information for other frames to
enhance quality. Therefore, we propose learning the ""forget"" and ""input"" gates
in the ConvLSTM cell from quality-related features. As such, the frames with
various quality contribute to the memory in ConvLSTM with different importance,
making the information of each frame reasonably and adequately used. Finally,
the experiments validate the effectiveness of our QG-ConvLSTM approach in
advancing the state-of-the-art quality enhancement of compressed video, and the
ablation study shows that our QG-ConvLSTM approach is learnt to make a
trade-off between quality and correlation when leveraging multi-frame
information.
</p>
"
http://arxiv.org/abs/1903.05726,"A Multi-armed Bandit MCMC, with applications in sampling from doubly intractable posterior. (arXiv:1903.05726v2 [stat.CO] UPDATED)","<p>Markov chain Monte Carlo (MCMC) algorithms are widely used to sample from
complicated distributions, especially to sample from the posterior distribution
in Bayesian inference. However, MCMC is not directly applicable when facing the
doubly intractable problem. In this paper, we discussed and compared two
existing solutions -- Pseudo-marginal Monte Carlo and Exchange Algorithm. This
paper also proposes a novel algorithm: Multi-armed Bandit MCMC (MABMC), which
chooses between two (or more) randomized acceptance ratios in each step. MABMC
could be applied directly to incorporate Pseudo-marginal Monte Carlo and
Exchange algorithm, with higher average acceptance probability.
</p>
"
http://arxiv.org/abs/1903.05759,Consistent Dialogue Generation with Self-supervised Feature Learning. (arXiv:1903.05759v2 [cs.CL] UPDATED),"<p>Generating responses that are consistent with the dialogue context is one of
the central challenges in building engaging conversational agents. In this
paper, we propose a neural conversation model that generates consistent
responses by maintaining certain features related to topics and personas
throughout the conversation. Unlike past work that requires external
supervision such as user identities, which are often unavailable or classified
as sensitive information, our approach trains topic and persona feature
extractors in a self-supervised way by utilizing the natural structure of
dialogue data. Moreover, we adopt a binary feature representation and introduce
a feature disentangling loss which, paired with controllable response
generation techniques, allows us to promote or demote certain learned topics
and personas features. The evaluation result demonstrates the model's
capability of capturing meaningful topics and personas features, and the
incorporation of the learned features brings significant improvement in terms
of the quality of generated responses on two datasets, even comparing with
model which explicit persona information.
</p>
"
http://arxiv.org/abs/1903.06396,COCO: The Large Scale Black-Box Optimization Benchmarking (bbob-largescale) Test Suite. (arXiv:1903.06396v2 [math.OC] UPDATED),"<p>The bbob-largescale test suite, containing 24 single-objective functions in
continuous domain, extends the well-known single-objective noiseless bbob test
suite, which has been used since 2009 in the BBOB workshop series, to large
dimension. The core idea is to make the rotational transformations R, Q in
search space that appear in the bbob test suite computationally cheaper while
retaining some desired properties. This documentation presents an approach that
replaces a full rotational transformation with a combination of a
block-diagonal matrix and two permutation matrices in order to construct test
functions whose computational and memory costs scale linearly in the dimension
of the problem.
</p>
"
http://arxiv.org/abs/1903.06473,DeepHuman: 3D Human Reconstruction from a Single Image. (arXiv:1903.06473v2 [cs.CV] UPDATED),"<p>We propose DeepHuman, an image-guided volume-to-volume translation CNN for 3D
human reconstruction from a single RGB image. To reduce the ambiguities
associated with the surface geometry reconstruction, even for the
reconstruction of invisible areas, we propose and leverage a dense semantic
representation generated from SMPL model as an additional input. One key
feature of our network is that it fuses different scales of image features into
the 3D space through volumetric feature transformation, which helps to recover
accurate surface geometry. The visible surface details are further refined
through a normal refinement network, which can be concatenated with the volume
generation network using our proposed volumetric normal projection layer. We
also contribute THuman, a 3D real-world human model dataset containing about
7000 models. The network is trained using training data generated from the
dataset. Overall, due to the specific design of our network and the diversity
in our dataset, our method enables 3D human model estimation given only a
single image and outperforms state-of-the-art approaches.
</p>
"
http://arxiv.org/abs/1903.06494,Content Differences in Syntactic and Semantic Representations. (arXiv:1903.06494v3 [cs.CL] UPDATED),"<p>Syntactic analysis plays an important role in semantic parsing, but the
nature of this role remains a topic of ongoing debate. The debate has been
constrained by the scarcity of empirical comparative studies between syntactic
and semantic schemes, which hinders the development of parsing methods informed
by the details of target schemes and constructions. We target this gap, and
take Universal Dependencies (UD) and UCCA as a test case. After abstracting
away from differences of convention or formalism, we find that most content
divergences can be ascribed to: (1) UCCA's distinction between a Scene and a
non-Scene; (2) UCCA's distinction between primary relations, secondary ones and
participants; (3) different treatment of multi-word expressions, and (4)
different treatment of inter-clause linkage. We further discuss the long tail
of cases where the two schemes take markedly different approaches. Finally, we
show that the proposed comparison methodology can be used for fine-grained
evaluation of UCCA parsing, highlighting both challenges and potential sources
for improvement. The substantial differences between the schemes suggest that
semantic parsers are likely to benefit downstream text understanding
applications beyond their syntactic counterparts.
</p>
"
http://arxiv.org/abs/1903.06888,Rethinking Uplink Hybrid Processing: When is Pure Analog Processing Suggested?. (arXiv:1903.06888v2 [cs.IT] UPDATED),"<p>In this correspondence, we analytically characterize the benefit of digital
processing in uplink massive multiple-input multiple-output (MIMO) with
sub-connected hybrid architecture. By assuming that the number of radio
frequency (RF) chains is equal to that of users, we characterize achievable
rates of both pure analog detection and hybrid detection under the i.i.d.
Rayleigh fading channel model. From the derived expressions, we discover that
the analog processing can outperform the hybrid processing using the maximal
ratio combining (MRC) or zero-forcing (ZF) criterion in cases under some
engineering assumptions. Performance comparison of the schemes are presented
under tests with various numbers of users and numbers of antennas at the base
station.
</p>
"
http://arxiv.org/abs/1903.06961,Entropy modulo a prime. (arXiv:1903.06961v2 [math.NT] UPDATED),"<p>Building on work of Kontsevich, we introduce a definition of the entropy of a
finite probability distribution in which the ""probabilities"" are integers
modulo a prime p. The entropy, too, is an integer mod p. Entropy mod p is shown
to be uniquely characterized by a functional equation identical to the one that
characterizes ordinary Shannon entropy. We also establish a sense in which
certain real entropies have residues mod p, connecting the concepts of entropy
over R and over Z/pZ. Finally, entropy mod p is expressed as a polynomial which
is shown to satisfy several identities, linking into work of Cathelineau,
Elbaz-Vincent and Gangl on polylogarithms.
</p>
"
http://arxiv.org/abs/1903.07009,Multi-Authority Attribute-Based Access Control with Smart Contract. (arXiv:1903.07009v3 [cs.CR] UPDATED),"<p>Attribute-based access control makes access control decisions based on the
assigned attributes of subjects and the access policies to protect objects by
mediating operations from the subjects. Authority, which validates attributes
of subjects, is one key component to facilitate attribute-based access control.
In an increasingly decentralized society, multiple attributes possessed by
subjects may need to be validated by multiple different authorities. This paper
proposes a multi-authority attribute-based access control scheme by using
Ethereum's smart contracts. In the proposed scheme, Ethereum smart contracts
are created to define the interactions between data owner, data user, and
multiple attribute authorities. A data user presents its attributes to
different attribute authorities, and after successful validation of attributes,
obtains attribute tokens from respective attribute authorities. After
collecting enough attribute tokens, a smart contract will be executed to issue
secret key to the data user to access the requested object. The smart contracts
for multi-authority attribute-based access control have been prototyped in
Solidity, and their performance has been evaluated on the Rinkeby Ethereum
Testnet.
</p>
"
http://arxiv.org/abs/1903.07807,Distributed Kalman-filtering: Distributed optimization viewpoint. (arXiv:1903.07807v2 [math.OC] UPDATED),"<p>We consider the Kalman-filtering problem with multiple sensors which are
connected through a communication network. If all measurements are delivered to
one place called fusion center and processed together, we call the process
centralized Kalman-filtering (CKF). When there is no fusion center, each sensor
can also solve the problem by using local measurements and exchanging
information with its neighboring sensors, which is called distributed
Kalman-filtering (DKF). Noting that CKF problem is a maximum likelihood
estimation problem, which is a quadratic optimization problem, we reformulate
DKF problem as a consensus optimization problem, resulting in that DKF problem
can be solved by many existing distributed optimization algorithms. A new DKF
algorithm employing the distributed dual ascent method is provided and its
performance is evaluated through numerical experiments.
</p>
"
http://arxiv.org/abs/1903.07864,Class-incremental Learning via Deep Model Consolidation. (arXiv:1903.07864v2 [cs.CV] UPDATED),"<p>Deep neural networks (DNNs) often suffer from ""catastrophic forgetting""
during incremental learning (IL) --- an abrupt degradation of performance on
the original set of classes when the training objective is adapted to a newly
added set of classes. Existing IL approaches tend to produce a model that is
biased towards either the old classes or new classes, unless with the help of
exemplars of the old data. To address this issue, we propose a
class-incremental learning paradigm called Deep Model Consolidation (DMC),
which works well even when the original training data is not available. The
idea is to first train a separate model only for the new classes, and then
combine the two individual models trained on data of two distinct set of
classes (old classes and new classes) via a novel dual distillation training
objective. The two existing models are consolidated by exploiting publicly
available unlabeled auxiliary data. This overcomes the potential difficulties
due to unavailability of original training data. Compared to the
state-of-the-art techniques, DMC demonstrates significantly better performance
in CIFAR-100 image classification and PASCAL VOC 2007 object detection
benchmarks in the single-headed IL setting.
</p>
"
http://arxiv.org/abs/1903.08510,Topological Data Analysis in Information Space. (arXiv:1903.08510v2 [cs.CG] UPDATED),"<p>Various kinds of data are routinely represented as discrete probability
distributions. Examples include text documents summarized by histograms of word
occurrences and images represented as histograms of oriented gradients. Viewing
a discrete probability distribution as a point in the standard simplex of the
appropriate dimension, we can understand collections of such objects in
geometric and topological terms. Importantly, instead of using the standard
Euclidean distance, we look into dissimilarity measures with
information-theoretic justification, and we develop the theory needed for
applying topological data analysis in this setting. In doing so, we emphasize
constructions that enable usage of existing computational topology software in
this context.
</p>
"
http://arxiv.org/abs/1903.09465,Managing Recurrent Virtual Network Updates in Multi-Tenant Datacenters: A System Perspective. (arXiv:1903.09465v2 [cs.CR] UPDATED),"<p>With the advent of software-defined networking, network configuration through
programmable interfaces becomes practical, leading to various on-demand
opportunities for network routing update in multi-tenant datacenters, where
tenants have diverse requirements on network routings such as short latency,
low path inflation, large bandwidth, high reliability, etc. Conventional
solutions that rely on topology search coupled with an objective function to
find desired routings have at least two shortcomings: (i) they run into
scalability issues when handling consistent and frequent routing updates and
(ii) they restrict the flexibility and capability to satisfy various routing
requirements. To address these issues, this paper proposes a novel search and
optimization decoupled design, which not only saves considerable topology
search costs via search result reuse, but also avoids possible sub-optimality
in greedy routing search algorithms by making decisions based on the global
view of all possible routings. We implement a prototype of our proposed system,
OpReduce, and perform extensive evaluations to validate its design goals.
</p>
"
http://arxiv.org/abs/1903.09798,Spatially-weighted Anomaly Detection with Regression Model. (arXiv:1903.09798v2 [cs.CV] UPDATED),"<p>Visual anomaly detection is common in several applications including medical
screening and production quality check. Although a definition of the anomaly is
an unknown trend in data, in many cases some hints or samples of the anomaly
class can be given in advance. Conventional methods cannot use the available
anomaly data, and also do not have a robustness of noise. In this paper, we
propose a novel spatially-weighted reconstruction-loss-based anomaly detection
with a likelihood value from a regression model trained by all known data. The
spatial weights are calculated by a region of interest generated from employing
visualization of the regression model. We introduce some ways to combine with
various strategies to propose a state-of-the-art method. Comparing with other
methods on three different datasets, we empirically verify the proposed method
performs better than the others.
</p>
"
http://arxiv.org/abs/1903.10081,Determining satisfiability of 3-SAT in polynomial time. (arXiv:1903.10081v3 [cs.DS] UPDATED),"<p>In this paper, we provide a polynomial time (and space), algorithm that
determines satisfiability of 3-SAT. The complexity analysis for the algorithm
takes into account no efficiency and yet provides a low enough bound, that
efficient versions are practical with respect to today's hardware. We accompany
this paper with a serial version of the algorithm without non-trivial
efficiencies.
</p>
"
http://arxiv.org/abs/1903.10153,DenseBody: Directly Regressing Dense 3D Human Pose and Shape From a Single Color Image. (arXiv:1903.10153v3 [cs.CV] UPDATED),"<p>Recovering 3D human body shape and pose from 2D images is a challenging task
due to high complexity and flexibility of human body, and relatively less 3D
labeled data. Previous methods addressing these issues typically rely on
predicting intermediate results such as body part segmentation, 2D/3D joints,
silhouette mask to decompose the problem into multiple sub-tasks in order to
utilize more 2D labels. Most previous works incorporated parametric body shape
model in their methods and predict parameters in low-dimensional space to
represent human body. In this paper, we propose to directly regress the 3D
human mesh from a single color image using Convolutional Neural Network(CNN).
We use an efficient representation of 3D human shape and pose which can be
predicted through an encoder-decoder neural network. The proposed method
achieves state-of-the-art performance on several 3D human body datasets
including Human3.6M, SURREAL and UP-3D with even faster running speed.
</p>
"
http://arxiv.org/abs/1903.10175,A Novel Method for the Absolute Pose Problem with Pairwise Constraints. (arXiv:1903.10175v2 [cs.CV] UPDATED),"<p>Absolute pose estimation is a fundamental problem in computer vision, and it
is a typical parameter estimation problem, meaning that efforts to solve it
will always suffer from outlier-contaminated data. Conventionally, for a fixed
dimensionality d and the number of measurements N, a robust estimation problem
cannot be solved faster than O(N^d). Furthermore, it is almost impossible to
remove d from the exponent of the runtime of a globally optimal algorithm.
However, absolute pose estimation is a geometric parameter estimation problem,
and thus has special constraints. In this paper, we consider pairwise
constraints and propose a globally optimal algorithm for solving the absolute
pose estimation problem. The proposed algorithm has a linear complexity in the
number of correspondences at a given outlier ratio. Concretely, we first
decouple the rotation and the translation subproblems by utilizing the pairwise
constraints, and then we solve the rotation subproblem using the
branch-and-bound algorithm. Lastly, we estimate the translation based on the
known rotation by using another branch-and-bound algorithm. The advantages of
our method are demonstrated via thorough testing on both synthetic and
real-world data
</p>
"
http://arxiv.org/abs/1903.10831,Attention Based Glaucoma Detection: A Large-scale Database and CNN Model. (arXiv:1903.10831v2 [cs.CV] UPDATED),"<p>Recently, the attention mechanism has been successfully applied in
convolutional neural networks (CNNs), significantly boosting the performance of
many computer vision tasks. Unfortunately, few medical image recognition
approaches incorporate the attention mechanism in the CNNs. In particular,
there exists high redundancy in fundus images for glaucoma detection, such that
the attention mechanism has potential in improving the performance of CNN-based
glaucoma detection. This paper proposes an attention-based CNN for glaucoma
detection (AG-CNN). Specifically, we first establish a large-scale attention
based glaucoma (LAG) database, which includes 5,824 fundus images labeled with
either positive glaucoma (2,392) or negative glaucoma (3,432). The attention
maps of the ophthalmologists are also collected in LAG database through a
simulated eye-tracking experiment. Then, a new structure of AG-CNN is designed,
including an attention prediction subnet, a pathological area localization
subnet and a glaucoma classification subnet. Different from other
attention-based CNN methods, the features are also visualized as the localized
pathological area, which can advance the performance of glaucoma detection.
Finally, the experiment results show that the proposed AG-CNN approach
significantly advances state-of-the-art glaucoma detection.
</p>
"
http://arxiv.org/abs/1903.11114,SUSI: Supervised Self-Organizing Maps for Regression and Classification in Python. (arXiv:1903.11114v2 [cs.LG] UPDATED),"<p>In many research fields, the sizes of the existing datasets vary widely.
Hence, there is a need for machine learning techniques which are well-suited
for these different datasets. One possible technique is the self-organizing map
(SOM), a type of artificial neural network which is, so far, weakly represented
in the field of machine learning. The SOM's unique characteristic is the
neighborhood relationship of the output neurons. This relationship improves the
ability of generalization on small datasets. SOMs are mostly applied in
unsupervised learning and few studies focus on using SOMs as supervised
learning approach. Furthermore, no appropriate SOM package is available with
respect to machine learning standards and in the widely used programming
language Python. In this paper, we introduce the freely available SUpervised
Self-organIzing maps (SUSI) Python package which performs supervised regression
and classification. The implementation of SUSI is described with respect to the
underlying mathematics. Then, we present first evaluations of the SOM for
regression and classification datasets from two different domains of geospatial
image analysis. Despite the early stage of its development, the SUSI framework
performs well and is characterized by only small performance differences
between the training and the test datasets. A comparison of the SUSI framework
with existing Python and R packages demonstrates the importance of the SUSI
framework. In future work, the SUSI framework will be extended, optimized and
upgraded e.g. with tools to better understand and visualize the input data as
well as the handling of missing and incomplete data.
</p>
"
http://arxiv.org/abs/1903.11210,Colorectal cancer diagnosis from histology images: A comparative study. (arXiv:1903.11210v2 [cs.CV] UPDATED),"<p>Computer-aided diagnosis (CAD) based on histopathological imaging has
progressed rapidly in recent years with the rise of machine learning based
methodologies. Traditional approaches consist of training a classification
model using features extracted from the images, based on textures or
morphological properties. Recently, deep-learning based methods have been
applied directly to the raw (unprocessed) data. However, their usability is
impacted by the paucity of annotated data in the biomedical sector. In order to
leverage the learning capabilities of deep Convolutional Neural Nets (CNNs)
within the confines of limited labelled data, in this study we shall
investigate the transfer learning approaches that aim to apply the knowledge
gained from solving a source (e.g., non-medical) problem, to learn better
predictive models for the target (e.g., biomedical) task. As an alternative, we
shall further propose a new adaptive and compact CNN based architecture that
can be trained from scratch even on scarce and low-resolution data. Moreover,
we conduct quantitative comparative evaluations among the traditional methods,
transfer learning-based methods and the proposed adaptive approach for the
particular task of cancer detection and identification from scarce and
low-resolution histology images. Over the largest benchmark dataset formed for
this purpose, the proposed adaptive approach achieved a higher cancer detection
accuracy with a significant gap, whereas the deep CNNs with transfer learning
achieved a superior cancer identification.
</p>
"
http://arxiv.org/abs/1903.11367,Does My Rebuttal Matter? Insights from a Major NLP Conference. (arXiv:1903.11367v2 [cs.CL] UPDATED),"<p>Peer review is a core element of the scientific process, particularly in
conference-centered fields such as ML and NLP. However, only few studies have
evaluated its properties empirically. Aiming to fill this gap, we present a
corpus that contains over 4k reviews and 1.2k author responses from ACL-2018.
We quantitatively and qualitatively assess the corpus. This includes a pilot
study on paper weaknesses given by reviewers and on quality of author
responses. We then focus on the role of the rebuttal phase, and propose a novel
task to predict after-rebuttal (i.e., final) scores from initial reviews and
author responses. Although author responses do have a marginal (and
statistically significant) influence on the final scores, especially for
borderline papers, our results suggest that a reviewer's final score is largely
determined by her initial score and the distance to the other reviewers'
initial scores. In this context, we discuss the conformity bias inherent to
peer reviewing, a bias that has largely been overlooked in previous research.
We hope our analyses will help better assess the usefulness of the rebuttal
phase in NLP conferences.
</p>
"
http://arxiv.org/abs/1903.11397,Lost in translation: Exposing hidden compiler optimization opportunities. (arXiv:1903.11397v2 [cs.PL] UPDATED),"<p>To increase productivity, today's compilers offer a two-fold abstraction:
they hide hardware complexity from the software developer, and they support
many architectures and programming languages. At the same time, due to fierce
market competition, most processor vendors do not disclose many of their
implementation details. These factors force software developers to treat both
compilers and architectures as black boxes. In practice, this leads to a
suboptimal compiler behavior where the maximum potential of improving an
application's resource usage, such as execution time, is often not realized.
This paper exposes missed optimization opportunities and is of interest to all
three communities, compiler engineers, software developers and hardware
architects. By exploiting the behavior of the standard optimization levels,
such as the -O3, of the LLVM v6.0 compiler, we show how to reveal hidden
cross-architecture and architecture-dependent potential optimizations on two
popular processors: the Intel i5-6300U, widely used in portable PCs, and the
ARM Cortex-A53-based Broadcom BCM2837 used in the Raspberry Pi 3B+. The classic
nightly regression testing can then be extended to use the resource usage and
compilation information collected while exploiting subsequences of the standard
optimization levels. This provides a systematic means of detecting and tracking
missed optimization opportunities. The enhanced nightly regression system is
capable of driving the improvement and tuning of the compiler's common
optimizer
</p>
"
http://arxiv.org/abs/1903.11538,RF-Assisted Free-Space Optics for 5G Vehicle-to-Vehicle Communications. (arXiv:1903.11538v2 [cs.IT] UPDATED),"<p>Vehicle-to-Vehicle (V2V) communications are being proposed, tested and
deployed to improve road safety and traffic efficiency. However, the automotive
industry poses strict requirements for safety-critical applications, that call
for reliable, low latency and high data rate communications. In this context,
it is widely agreed that both Radio-Frequency (RF) technologies at mmWaves and
Free-Space Optics (FSO) represent promising solutions, although their
performances are severely degraded by transmitter-receiver misalignment due to
the challenging high-mobility conditions. By combining RF and FSO technologies,
this paper proposes a FSO-based V2V communication system where the pointing
coordinates of laser sources are based on vehicle's information exchanged over
a reliable low-rate RF link. Numerical simulations demonstrate that such
compensation mechanism is mandatory to counteract the unavoidable misalignments
induced by vehicle dynamics, and thus to enable FSO technology for V2V
communications even in high mobility scenarios.
</p>
"
http://arxiv.org/abs/1902.02771,Impact of Fully Connected Layers on Performance of Convolutional Neural Networks for Image Classification. (arXiv:1902.02771v2 [cs.CV] CROSS LISTED),"<p>The Convolutional Neural Networks (CNNs), in domains like computer vision,
mostly reduced the need for handcrafted features due to its ability to learn
the problem-specific features from the raw input data. However, the selection
of dataset-specific CNN architecture, which mostly performed by either
experience or expertise is a time-consuming and error-prone process. To
automate the process of learning a CNN architecture, this letter attempts at
finding the relationship between Fully Connected (FC) layers with some of the
characteristics of the datasets. The CNN architectures, and recently data sets
also, are categorized as deep, shallow, wide, etc. This letter tries to
formalize these terms along with answering the following questions. (i) What is
the impact of deeper/shallow architectures on the performance of the CNN w.r.t
FC layers?, (ii) How the deeper/wider datasets influence the performance of CNN
w.r.t FC layers?, and (iii) Which kind of architecture (deeper/ shallower) is
better suitable for which kind of (deeper/ wider) datasets. To address these
findings, we have performed experiments with three CNN architectures having
different depths. The experiments are conducted by varying the number of FC
layers. We used four widely used datasets including CIFAR-10, CIFAR-100, Tiny
ImageNet, and CRCHistoPhenotypes to justify our findings in the context of the
image classification problem. The source code of this research is available at
https://github.com/shabbeersh/Impact-of-FC-layers.
</p>
"
http://arxiv.org/abs/1903.09354,A Model Counter's Guide to Probabilistic Systems. (arXiv:1903.09354v1 [cs.LO] CROSS LISTED),"<p>In this paper, we systematize the modeling of probabilistic systems for the
purpose of analyzing them with model counting techniques. Starting from
unbiased coin flips, we show how to model biased coins, correlated coins, and
distributions over finite sets. From there, we continue with modeling
sequential systems, such as Markov chains, and revisit the relationship between
weighted and unweighted model counting. Thereby, this work provides a
conceptual framework for deriving #SAT encodings for probabilistic inference.
</p>
"
http://arxiv.org/abs/1903.11359,Scaling up the randomized gradient-free adversarial attack reveals overestimation of robustness using established attacks. (arXiv:1903.11359v1 [cs.LG] CROSS LISTED),"<p>Modern neural networks are highly non-robust against adversarial
manipulation. A significant amount of work has been invested in techniques to
compute lower bounds on robustness through formal guarantees and to build
provably robust model. However it is still difficult to apply them to larger
networks or in order to get robustness against larger perturbations. Thus
attack strategies are needed to provide tight upper bounds on the actual
robustness. We significantly improve the randomized gradient-free attack for
ReLU networks [9], in particular by scaling it up to large networks. We show
that our attack achieves similar or significantly smaller robust accuracy than
state-of-the-art attacks like PGD or the one of Carlini and Wagner, thus
revealing an overestimation of the robustness by these state-of-the-art
methods. Our attack is not based on a gradient descent scheme and in this sense
gradient-free, which makes it less sensitive to the choice of hyperparameters
as no careful selection of the stepsize is required.
</p>
"
http://arxiv.org/abs/1903.11521,Yet Another Tensor Toolbox for discontinuous Galerkin methods and other applications. (arXiv:1903.11521v1 [cs.MS] CROSS LISTED),"<p>The numerical solution of partial differential equations is at the heart of
many grand challenges in supercomputing. Solvers based on high-order
discontinuous Galerkin (DG) discretisation have been shown to scale on large
supercomputers with excellent performance and efficiency, if the implementation
exploits all levels of parallelism and is tailored to the specific
architecture. However, every year new supercomputers emerge and the list of
hardware-specific considerations grows, simultaneously with the list of desired
features in a DG code. Thus we believe that a sustainable DG code needs an
abstraction layer to implement the numerical scheme in a suitable language. We
explore the possibility to abstract the numerical scheme as small tensor
operations, describe them in a domain-specific language (DSL) resembling the
Einstein notation, and to map them to existing code generators which generate
small matrix matrix multiplication routines. The compiler for our DSL
implements classic optimisations that are used for large tensor contractions,
and we present novel optimisation techniques such as equivalent sparsity
patterns and optimal index permutations for temporary tensors. Our application
examples, which include the earthquake simulation software SeisSol, show that
the generated kernels achieve over 50 % peak performance while the DSL
considerably simplifies the implementation.
</p>
"
